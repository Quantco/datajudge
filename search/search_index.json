{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"datajudge","text":"<p><code>datajudge</code> allows for assessing whether data from database complies with reference information.</p> <p>While meant to be as agnostic to concrete database management systems as possible, <code>datajudge</code> currently explicitly supports:</p> <ul> <li>Postgres</li> <li>MSSQL</li> <li>Snowflake</li> <li>BigQuery</li> <li>DuckDB</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#1130-20260121","title":"1.13.0 - 2026.01.21","text":"<ul> <li>Deprecate <code>sqlalchemy</code> &lt;2.0.0.</li> </ul>"},{"location":"CHANGELOG/#1120-20260112","title":"1.12.0 - 2026.01.12","text":"<ul> <li>Drop support impala as a backend.</li> <li>Add support for DuckDB as a backend.</li> </ul>"},{"location":"CHANGELOG/#1110-20251217","title":"1.11.0 - 2025.12.17","text":"<ul> <li>Drop support for Python 3.8 and Python 3.9.</li> </ul>"},{"location":"CHANGELOG/#1100-20250213","title":"1.10.0 - 2025.02.13","text":"<ul> <li>Address deprecation warnings from <code>sqlalchemy</code>.</li> <li>Provide more type hints.</li> </ul>"},{"location":"CHANGELOG/#193-20250113","title":"1.9.3 - 2025.01.13","text":"<p>Bug fixes</p> <ul> <li>Fix a bug in   <code>datajudge.WithinRequirement.add_date_no_overlap_constraint</code> and   <code>datajudge.WithinRequirement.add_date_no_overlap_2d_constraint</code> and   <code>datajudge.WithinRequirement.add_numeric_no_overlap_constraint</code> in   which some overlaps were not detected due to equality of their   leftmost bounds.</li> </ul>"},{"location":"CHANGELOG/#192-20240905","title":"1.9.2 - 2024.09.05","text":"<p>Bug fixes</p> <ul> <li>Fix a bug in <code>datajudge.constraints.numeric.NumericPercentile</code> which   could lead to off-by-one errors in retrieving a percentile value.</li> </ul>"},{"location":"CHANGELOG/#190-20240625","title":"1.9.0 - 2024.06.25","text":"<p>New features</p> <ul> <li>Add styling for assertion messages. See <code>assertion-message-styling</code>   for more information.</li> <li>Add <code>output_processors</code> and <code>filter_func</code> parameters to   <code>datajudge.WithinRequiremen.tadd_uniques_equality_constraint</code>,   <code>datajudge.WithinRequirement.add_uniques_superset_constraint</code>   and   <code>datajudge.WithinRequirement.add_uniques_subset_constraint</code>.</li> <li>Add <code>output_processors</code>, <code>filter_func</code> and <code>compare_distinct</code>   parameters to   <code>datajudge.BetweenRequirement.add_uniques_equality_constraint</code>,   <code>datajudge.BetweenRequirement.add_uniques_superset_constraint</code>   and   <code>datajudge.BetweenRequirement.add_uniques_subset_constraint</code>.</li> <li>Add <code>output_processors</code> parameter to   <code>datajudge.WithinRequirement.add_functional_dependency_constraint</code>.</li> </ul> <p>Other changes</p> <ul> <li>Provide a <code>py.typed</code> file.</li> <li>Remove usage of <code>pkg_resources</code>.</li> </ul>"},{"location":"CHANGELOG/#180-20230616","title":"1.8.0 - 2023.06.16","text":"<p>New features</p> <ul> <li>Implement   <code>datajudge.WithinRequirement.add_functional_dependency_constraint</code>.</li> </ul> <p>Other changes</p> <ul> <li>Improve error message when a <code>DataReference</code> is constructed   with a single column name instead of specifying a list of columns.</li> </ul>"},{"location":"CHANGELOG/#170-20230511","title":"1.7.0 - 2023.05.11","text":"<p>New features</p> <ul> <li>Implement   <code>datajudge.WithinRequirement.add_categorical_bound_constraint</code>.</li> <li>Extended <code>datajudge.WithinRequirement.add_column_type_constraint</code> to   support column type specification using string format,   backend-specific SQLAlchemy types, and SQLAlchemy's generic types.</li> <li>Implement <code>datajudge.WithinRequirement.add_numeric_no_gap_constraint</code>,   <code>datajudge.WithinRequirement.add_numeric_no_overlap_constraint</code>,</li> </ul>"},{"location":"CHANGELOG/#160-20230412","title":"1.6.0 - 2023.04.12","text":"<p>Other changes</p> <ul> <li>Ensure compatibility with <code>sqlalchemy</code> &gt;= 2.0.</li> </ul>"},{"location":"CHANGELOG/#150-20230314","title":"1.5.0 - 2023.03.14","text":"<p>New features</p> <ul> <li>Implement   <code>datajudge.BetweenRequirement.add_max_null_fraction_constraint</code> and   <code>datajudge.WithinRequirement.add_max_null_fraction_constraint</code>.</li> <li>Implement   <code>datajudge.BetweenRequirement.add_numeric_percentile_constraint</code> and   <code>datajudge.WithinRequirement.add_numeric_percentile_constraint</code>.</li> </ul>"},{"location":"CHANGELOG/#140-20230224","title":"1.4.0 - 2023.02.24","text":"<p>New features</p> <ul> <li>Add partial and experimental support for db2 as a backend.</li> </ul>"},{"location":"CHANGELOG/#130-20230117","title":"1.3.0 - 2023.01.17","text":"<p>New features</p> <ul> <li>Implement <code>datajudge.BetweenRequirement.add_column_type_constraint</code>.   Previously, only the <code>WithinRequirement</code> method existed.</li> <li>Implemented an option <code>infer_pk</code> to automatically retrieve and primary   key definition as part of   <code>datajudge.WithinRequirement.add_uniqueness_constraint</code>.</li> <li>Added a <code>name</code> parameter to all <code>add_x_constraint</code> methods of   <code>WithinRequirement</code> and <code>BetweenRequirement</code>. This will give pytest   test a custom name.</li> <li>Added preliminary support for Impala.</li> </ul> <p>Other changes</p> <ul> <li>Improve assertion error for   <code>datajudge.BetweenRequirement.add_row_matching_equality_constraint</code>.</li> </ul>"},{"location":"CHANGELOG/#120-20221021","title":"1.2.0 - 2022.10.21","text":"<p>New features</p> <ul> <li>Implemented specification of number of counterexamples in   <code>datajudge.WithinRequirement.add_varchar_regex_constraint</code>.</li> <li>Implemented in-database regex matching for some dialects via   <code>computation_in_db</code> parameter in   <code>datajudge.WithinRequirement.add_varchar_regex_constraint</code>.</li> <li>Added support for BigQuery backends.</li> </ul> <p>Bug fix</p> <ul> <li>Snowflake-sqlalchemy version 1.4.0 introduced an unexpected change in   behaviour. This problem is resolved by pinning it to the previous   version, 1.3.4.</li> </ul>"},{"location":"CHANGELOG/#111-20220630","title":"1.1.1 - 2022.06.30","text":"<p>New: SQL implementation for KS-test</p> <ul> <li>The Kolgomorov Smirnov test is now implemented in pure SQL, shifting   the computation to the database engine, improving performance   tremendously.</li> </ul>"},{"location":"CHANGELOG/#110-20220601","title":"1.1.0 - 2022.06.01","text":"<p>New feature: Statistical Tests</p> <ul> <li>Implemented a new constraint   <code>datajudge.constraints.stats.KolmogorovSmirnov2Sample</code> for   <code>datajudge.BetweenRequirement</code> that performs a Kolmogorov Smirnov   Test   between two data sources.</li> </ul>"},{"location":"CHANGELOG/#101-20220524","title":"1.0.1 - 2022.05.24","text":"<p>Bug fix:</p> <ul> <li>The method <code>is_deprecated</code> of <code>datajudge.Condition</code> was called   despite not existing.</li> </ul>"},{"location":"api-documentation/","title":"API Documentation","text":"<p>datajudge allows to assess  whether data from database complies with referenceinformation.</p>"},{"location":"api-documentation/#datajudge.BetweenRequirement","title":"BetweenRequirement","text":"<pre><code>BetweenRequirement(data_source: DataSource, data_source2: DataSource, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>               Bases: <code>Requirement</code></p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(\n    self,\n    data_source: DataSource,\n    data_source2: DataSource,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    self._data_source = data_source\n    self._data_source2 = data_source2\n    self._ref = DataReference(self._data_source)\n    self._ref2 = DataReference(self._data_source2)\n    self._date_column = date_column\n    self._date_column2 = date_column2\n    super().__init__()\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_column_subset_constraint","title":"add_column_subset_constraint","text":"<pre><code>add_column_subset_constraint(name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Columns of first table are subset of second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_subset_constraint(\n    self, name: str | None = None, cache_size=None\n) -&gt; None:\n    \"\"\"Columns of first table are subset of second table.\"\"\"\n    self._constraints.append(\n        column_constraints.ColumnSubset(\n            self._ref, ref2=self._ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_column_superset_constraint","title":"add_column_superset_constraint","text":"<pre><code>add_column_superset_constraint(name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Columns of first table are superset of columns of second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_superset_constraint(\n    self, name: str | None = None, cache_size=None\n) -&gt; None:\n    \"\"\"Columns of first table are superset of columns of second table.\"\"\"\n    self._constraints.append(\n        column_constraints.ColumnSuperset(\n            self._ref, ref2=self._ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_column_type_constraint","title":"add_column_type_constraint","text":"<pre><code>add_column_type_constraint(column1: str, column2: str, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check that the columns have the same type.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_type_constraint(\n    self,\n    column1: str,\n    column2: str,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check that the columns have the same type.\"\"\"\n    ref1 = DataReference(self._data_source, [column1])\n    ref2 = DataReference(self._data_source2, [column2])\n    self._constraints.append(\n        column_constraints.ColumnType(\n            ref1, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_date_max_constraint","title":"add_date_max_constraint","text":"<pre><code>add_date_max_constraint(column1: str, column2: str, use_upper_bound_reference: bool = True, column_type: str = 'date', condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Compare date max of first table to date max of second table.</p> <p>The used columns of both tables need to be of the same type.</p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_upper_bound_reference</code>, the max of the first table has to be smaller or equal to the max of the second table. If not <code>use_upper_bound_reference</code>, the max of the first table has to be greater or equal to the max of the second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_max_constraint(\n    self,\n    column1: str,\n    column2: str,\n    use_upper_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Compare date max of first table to date max of second table.\n\n    The used columns of both tables need to be of the same type.\n\n    For more information on ``column_type`` values, see ``add_column_type_constraint``.\n\n    If ``use_upper_bound_reference``, the max of the first table has to be\n    smaller or equal to the max of the second table.\n    If not ``use_upper_bound_reference``, the max of the first table has to\n    be greater or equal to the max of the second table.\n    \"\"\"\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        date_constraints.DateMax(\n            ref,\n            ref2=ref2,\n            use_upper_bound_reference=use_upper_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_date_min_constraint","title":"add_date_min_constraint","text":"<pre><code>add_date_min_constraint(column1: str, column2: str, use_lower_bound_reference: bool = True, column_type: str = 'date', condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Ensure date min of first table is greater or equal date min of second table.</p> <p>The used columns of both tables need to be of the same type.</p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_lower_bound_reference</code>, the min of the first table has to be greater or equal to the min of the second table. If not <code>use_upper_bound_reference</code>, the min of the first table has to be smaller or equal to the min of the second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_min_constraint(\n    self,\n    column1: str,\n    column2: str,\n    use_lower_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Ensure date min of first table is greater or equal date min of second table.\n\n    The used columns of both tables need to be of the same type.\n\n    For more information on ``column_type`` values, see ``add_column_type_constraint``.\n\n    If ``use_lower_bound_reference``, the min of the first table has to be\n    greater or equal to the min of the second table.\n    If not ``use_upper_bound_reference``, the min of the first table has to\n    be smaller or equal to the min of the second table.\n    \"\"\"\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        date_constraints.DateMin(\n            ref,\n            ref2=ref2,\n            use_lower_bound_reference=use_lower_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_ks_2sample_constraint","title":"add_ks_2sample_constraint","text":"<pre><code>add_ks_2sample_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, significance_level: float = 0.05, cache_size=None) -&gt; None\n</code></pre> <p>Apply the so-called two-sample Kolmogorov-Smirnov test to the distributions of the two given columns.</p> <p>The constraint is fulfilled, when the resulting p-value of the test is higher than the significance level (default is 0.05, i.e., 5%). The significance_level must be a value between 0.0 and 1.0.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_ks_2sample_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    significance_level: float = 0.05,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"\n    Apply the so-called two-sample Kolmogorov-Smirnov test to the distributions of the two given columns.\n\n    The constraint is fulfilled, when the resulting p-value of the test is higher than the significance level\n    (default is 0.05, i.e., 5%).\n    The significance_level must be a value between 0.0 and 1.0.\n    \"\"\"\n    if not column1 or not column2:\n        raise ValueError(\n            \"Column names have to be given for this test's functionality.\"\n        )\n\n    if significance_level &lt;= 0.0 or significance_level &gt; 1.0:\n        raise ValueError(\n            \"The requested significance level has to be in ``(0.0, 1.0]``. Default is 0.05.\"\n        )\n\n    ref = DataReference(self._data_source, [column1], condition=condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition=condition2)\n    self._constraints.append(\n        stats_constraints.KolmogorovSmirnov2Sample(\n            ref,\n            ref2,\n            significance_level,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_max_null_fraction_constraint","title":"add_max_null_fraction_constraint","text":"<pre><code>add_max_null_fraction_constraint(column1: str, column2: str, max_relative_deviation: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the fraction of <code>NULL</code> values of one is at most that of the other.</p> <p>Given that <code>column2</code>'s underlying data has a fraction <code>q</code> of <code>NULL</code> values, the <code>max_relative_deviation</code> parameter allows <code>column1</code>'s underlying data to have a fraction <code>(1 + max_relative_deviation) * q</code> of <code>NULL</code> values.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_max_null_fraction_constraint(\n    self,\n    column1: str,\n    column2: str,\n    max_relative_deviation: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the fraction of ``NULL`` values of one is at most that of the other.\n\n    Given that ``column2``'s underlying data has a fraction ``q`` of ``NULL`` values, the\n    ``max_relative_deviation`` parameter allows ``column1``'s underlying data to have a\n    fraction ``(1 + max_relative_deviation) * q`` of ``NULL`` values.\n    \"\"\"  # noqa: D301\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        miscs_constraints.MaxNullFraction(\n            ref,\n            ref2=ref2,\n            max_relative_deviation=max_relative_deviation,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_rows_equality_constraint","title":"add_n_rows_equality_constraint","text":"<pre><code>add_n_rows_equality_constraint(condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_equality_constraint(\n    self,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsEquality(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_rows_max_gain_constraint","title":"add_n_rows_max_gain_constraint","text":"<pre><code>add_n_rows_max_gain_constraint(constant_max_relative_gain: float | None = None, date_range_gain_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of rows hasn't grown by more than expected.</p> <p>In particular, assert that</p> \\[n^{rows}_1 \\leq n^{rows}_2 \\cdot (1 + \\text{cmrg})\\] <p>See readme for more information on <code>constant_max_relative_gain</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_max_gain_constraint(\n    self,\n    constant_max_relative_gain: float | None = None,\n    date_range_gain_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of rows hasn't grown by more than expected.\n\n    In particular, assert that\n\n    $$n^{rows}_1 \\leq n^{rows}_2 \\cdot (1 + \\text{cmrg})$$\n\n    See readme for more information on ``constant_max_relative_gain``.\n    \"\"\"\n    max_relative_gain_getter = self._get_deviation_getter(\n        constant_max_relative_gain, date_range_gain_deviation\n    )\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsMaxGain(\n            ref,\n            ref2,\n            max_relative_gain_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_rows_max_loss_constraint","title":"add_n_rows_max_loss_constraint","text":"<pre><code>add_n_rows_max_loss_constraint(constant_max_relative_loss: float | None = None, date_range_loss_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of rows hasn't decreased too much.</p> <p>In particular, assert that</p> \\[n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 - \\text{cmrl})\\] <p>See readme for more information on <code>constant_max_relative_loss</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_max_loss_constraint(\n    self,\n    constant_max_relative_loss: float | None = None,\n    date_range_loss_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of rows hasn't decreased too much.\n\n    In particular, assert that\n\n    $$n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 - \\text{cmrl})$$\n\n    See readme for more information on ``constant_max_relative_loss``.\n    \"\"\"\n    max_relative_loss_getter = self._get_deviation_getter(\n        constant_max_relative_loss, date_range_loss_deviation\n    )\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsMaxLoss(\n            ref,\n            ref2,\n            max_relative_loss_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_rows_min_gain_constraint","title":"add_n_rows_min_gain_constraint","text":"<pre><code>add_n_rows_min_gain_constraint(constant_min_relative_gain: float | None = None, date_range_gain_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of rows hasn't grown less than expected.</p> <p>In particular, assert that</p> \\[n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 + \\text{cmrg})\\] <p>See readme for more information on <code>constant_min_relative_gain</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_min_gain_constraint(\n    self,\n    constant_min_relative_gain: float | None = None,\n    date_range_gain_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of rows hasn't grown less than expected.\n\n    In particular, assert that\n\n    $$n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 + \\text{cmrg})$$\n\n    See readme for more information on ``constant_min_relative_gain``.\n    \"\"\"\n    min_relative_gain_getter = self._get_deviation_getter(\n        constant_min_relative_gain, date_range_gain_deviation\n    )\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsMinGain(\n            ref,\n            ref2,\n            min_relative_gain_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_uniques_equality_constraint","title":"add_n_uniques_equality_constraint","text":"<pre><code>add_n_uniques_equality_constraint(columns1: list[str] | None, columns2: list[str] | None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_equality_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.NUniquesEquality(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_uniques_max_gain_constraint","title":"add_n_uniques_max_gain_constraint","text":"<pre><code>add_n_uniques_max_gain_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_relative_gain: float | None = None, date_range_gain_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of uniques hasn't grown by too much.</p> <p>In particular, assert that</p> \\[n^{uniques}_1 \\leq n^{uniques}_2 \\cdot (1 - \\text{cmrg})\\] <p>The number of uniques in first table are defined based on <code>columns1</code>, the number of uniques in second table are defined based on <code>columns2</code>.</p> <p>See readme for more information on <code>constant_max_relative_gain</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_max_gain_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_relative_gain: float | None = None,\n    date_range_gain_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of uniques hasn't grown by too much.\n\n    In particular, assert that\n\n    $$n^{uniques}_1 \\leq n^{uniques}_2 \\cdot (1 - \\text{cmrg})$$\n\n    The number of uniques in first table are defined based on ``columns1``, the\n    number of uniques in second table are defined based on ``columns2``.\n\n    See readme for more information on ``constant_max_relative_gain``.\n    \"\"\"\n    max_relative_gain_getter = self._get_deviation_getter(\n        constant_max_relative_gain, date_range_gain_deviation\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.NUniquesMaxGain(\n            ref,\n            ref2,\n            max_relative_gain_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_n_uniques_max_loss_constraint","title":"add_n_uniques_max_loss_constraint","text":"<pre><code>add_n_uniques_max_loss_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_relative_loss: float | None = None, date_range_loss_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of uniques hasn't decreased too much.</p> <p>In particular, assert that</p> \\[n^{uniques}_1 \\geq n^{uniques}_2 \\cdot (1 - \\text{cmrl})\\] <p>The number of uniques in first table are defined based on <code>columns1</code>, the number of uniques in second table are defined based on <code>columns2</code>.</p> <p>See readme for more information on <code>constant_max_relative_loss</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_max_loss_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_relative_loss: float | None = None,\n    date_range_loss_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of uniques hasn't decreased too much.\n\n    In particular, assert that\n\n    $$n^{uniques}_1 \\geq n^{uniques}_2 \\cdot (1 - \\text{cmrl})$$\n\n    The number of uniques in first table are defined based on ``columns1``, the\n    number of uniques in second table are defined based on ``columns2``.\n\n    See readme for more information on ``constant_max_relative_loss``.\n    \"\"\"\n    max_relative_loss_getter = self._get_deviation_getter(\n        constant_max_relative_loss, date_range_loss_deviation\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.NUniquesMaxLoss(\n            ref,\n            ref2,\n            max_relative_loss_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_numeric_max_constraint","title":"add_numeric_max_constraint","text":"<pre><code>add_numeric_max_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_max_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericMax(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_numeric_mean_constraint","title":"add_numeric_mean_constraint","text":"<pre><code>add_numeric_mean_constraint(column1: str, column2: str, max_absolute_deviation: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_mean_constraint(\n    self,\n    column1: str,\n    column2: str,\n    max_absolute_deviation: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericMean(\n            ref,\n            max_absolute_deviation,\n            ref2=ref2,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_numeric_min_constraint","title":"add_numeric_min_constraint","text":"<pre><code>add_numeric_min_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_min_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericMin(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_numeric_percentile_constraint","title":"add_numeric_percentile_constraint","text":"<pre><code>add_numeric_percentile_constraint(column1: str, column2: str, percentage: float, max_absolute_deviation: float | None = None, max_relative_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the <code>percentage</code>-th percentile is approximately equal.</p> <p>The percentile is defined as the smallest value present in <code>column1</code> / <code>column2</code> for which <code>percentage</code> % of the values in <code>column1</code> / <code>column2</code> are less or equal. <code>NULL</code> values are ignored.</p> <p>Hence, if <code>percentage</code> is less than the inverse of the number of non-<code>NULL</code> rows, <code>None</code> is received as the <code>percentage</code>-th percentile.</p> <p><code>percentage</code> is expected to be provided in percent. The median, for example, would correspond to <code>percentage=50</code>.</p> <p>At least one of <code>max_absolute_deviation</code> and <code>max_relative_deviation</code> must be provided.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_percentile_constraint(\n    self,\n    column1: str,\n    column2: str,\n    percentage: float,\n    max_absolute_deviation: float | None = None,\n    max_relative_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the ``percentage``-th percentile is approximately equal.\n\n    The percentile is defined as the smallest value present in ``column1`` / ``column2``\n    for which ``percentage`` % of the values in ``column1`` / ``column2`` are\n    less or equal. ``NULL`` values are ignored.\n\n    Hence, if ``percentage`` is less than the inverse of the number of non-``NULL``\n    rows, ``None`` is received as the ``percentage``-th percentile.\n\n    ``percentage`` is expected to be provided in percent. The median, for example,\n    would correspond to ``percentage=50``.\n\n    At least one of ``max_absolute_deviation`` and ``max_relative_deviation`` must\n    be provided.\n    \"\"\"\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericPercentile(\n            ref,\n            percentage=percentage,\n            max_absolute_deviation=max_absolute_deviation,\n            max_relative_deviation=max_relative_deviation,\n            ref2=ref2,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_row_equality_constraint","title":"add_row_equality_constraint","text":"<pre><code>add_row_equality_constraint(columns1: list[str] | None, columns2: list[str] | None, max_missing_fraction: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>At most <code>max_missing_fraction</code> of rows in T1 and T2 are absent in either.</p> <p>In other words</p> \\[\\frac{|T1 - T2| + |T2 - T1|}{|T1 \\cup T2|} \\leq \\text{mmf}\\] <p>Rows from T1 are indexed in <code>columns1</code>, rows from T2 are indexed in <code>columns2</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_equality_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    max_missing_fraction: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"At most ``max_missing_fraction`` of rows in T1 and T2 are absent in either.\n\n    In other words\n\n    $$\\\\frac{|T1 - T2| + |T2 - T1|}{|T1 \\\\cup T2|} \\\\leq \\\\text{mmf}$$\n\n    Rows from T1 are indexed in ``columns1``, rows from T2 are indexed in ``columns2``.\n    \"\"\"  # noqa: D301\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        row_constraints.RowEquality(\n            ref,\n            ref2,\n            lambda engine: max_missing_fraction,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_row_matching_equality_constraint","title":"add_row_matching_equality_constraint","text":"<pre><code>add_row_matching_equality_constraint(matching_columns1: list[str], matching_columns2: list[str], comparison_columns1: list[str], comparison_columns2: list[str], max_missing_fraction: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Match tables in matching_columns, compare for equality in comparison_columns.</p> <p>This constraint is similar to the nature of the <code>RowEquality</code> constraint. Just as the latter, this constraint divides the cardinality of an intersection by the cardinality of a union. The difference lies in how the set are created. While <code>RowEquality</code> considers all rows of both tables, indexed in columns, <code>RowMatchingEquality</code> considers only rows in both tables having values in <code>matching_columns</code> present in both tables. At most <code>max_missing_fraction</code> of such rows can be missing in the intersection.</p> <p>Alternatively, this can be thought of as counting mismatches in <code>comparison_columns</code> after performing an inner join on <code>matching_columns</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_matching_equality_constraint(\n    self,\n    matching_columns1: list[str],\n    matching_columns2: list[str],\n    comparison_columns1: list[str],\n    comparison_columns2: list[str],\n    max_missing_fraction: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Match tables in matching_columns, compare for equality in comparison_columns.\n\n    This constraint is similar to the nature of the ``RowEquality``\n    constraint. Just as the latter, this constraint divides the\n    cardinality of an intersection by the cardinality of a union.\n    The difference lies in how the set are created. While ``RowEquality``\n    considers all rows of both tables, indexed in columns,\n    ``RowMatchingEquality`` considers only rows in both tables having values\n    in ``matching_columns`` present in both tables. At most ``max_missing_fraction``\n    of such rows can be missing in the intersection.\n\n    Alternatively, this can be thought of as counting mismatches in\n    ``comparison_columns`` after performing an inner join on ``matching_columns``.\n    \"\"\"\n    ref = DataReference(\n        self._data_source, matching_columns1 + comparison_columns1, condition1\n    )\n    ref2 = DataReference(\n        self._data_source2, matching_columns2 + comparison_columns2, condition2\n    )\n    self._constraints.append(\n        row_constraints.RowMatchingEquality(\n            ref,\n            ref2,\n            matching_columns1,\n            matching_columns2,\n            comparison_columns1,\n            comparison_columns2,\n            lambda engine: max_missing_fraction,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_row_subset_constraint","title":"add_row_subset_constraint","text":"<pre><code>add_row_subset_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_missing_fraction: float | None, date_range_loss_fraction: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>At most <code>max_missing_fraction</code> of rows in T1 are not in T2.</p> <p>In other words, :math:<code>\\frac{|T1-T2|}{|T1|} \\leq</code> <code>max_missing_fraction</code>. Rows from T1 are indexed in columns1, rows from T2 are indexed in <code>columns2</code>.</p> <p>In particular, the operation <code>|T1-T2|</code> relies on a sql <code>EXCEPT</code> statement. In contrast to <code>EXCEPT ALL</code>, this should lead to a set subtraction instead of a multiset subtraction. In other words, duplicates in T1 are treated as single occurrences.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_subset_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_missing_fraction: float | None,\n    date_range_loss_fraction: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"At most ``max_missing_fraction`` of rows in T1 are not in T2.\n\n    In other words,\n    :math:`\\\\frac{|T1-T2|}{|T1|} \\\\leq` ``max_missing_fraction``.\n    Rows from T1 are indexed in columns1, rows from T2 are indexed in ``columns2``.\n\n    In particular, the operation ``|T1-T2|`` relies on a sql ``EXCEPT`` statement. In\n    contrast to ``EXCEPT ALL``, this should lead to a set subtraction instead of\n    a multiset subtraction. In other words, duplicates in T1 are treated as\n    single occurrences.\n    \"\"\"  # noqa: D301\n    max_missing_fraction_getter = self._get_deviation_getter(\n        constant_max_missing_fraction, date_range_loss_fraction\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        row_constraints.RowSubset(\n            ref,\n            ref2,\n            max_missing_fraction_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_row_superset_constraint","title":"add_row_superset_constraint","text":"<pre><code>add_row_superset_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_missing_fraction: float, date_range_loss_fraction: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>At most <code>max_missing_fraction</code> of rows in T2 are not in T1.</p> <p>In other words, \\(\\frac{|T2-T1|}{|T2|} \\leq\\) <code>max_missing_fraction</code>. Rows from T1 are indexed in <code>columns1</code>, rows from T2 are indexed in <code>columns2</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_superset_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_missing_fraction: float,\n    date_range_loss_fraction: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"At most ``max_missing_fraction`` of rows in T2 are not in T1.\n\n    In other words, $\\\\frac{|T2-T1|}{|T2|} \\\\leq$ ``max_missing_fraction``.\n    Rows from T1 are indexed in ``columns1``, rows from T2 are indexed in\n    ``columns2``.\n    \"\"\"  # noqa: D301\n    max_missing_fraction_getter = self._get_deviation_getter(\n        constant_max_missing_fraction, date_range_loss_fraction\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        row_constraints.RowSuperset(\n            ref,\n            ref2,\n            max_missing_fraction_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_uniques_equality_constraint","title":"add_uniques_equality_constraint","text":"<pre><code>add_uniques_equality_constraint(columns1: list[str], columns2: list[str], filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check if the data's unique values in given columns are equal.</p> <p>The <code>UniquesEquality</code> constraint asserts if the values contained in a column of a <code>DataSource</code>'s columns, are strictly the ones of another <code>DataSource</code>'s columns.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly..</p> <p>See <code>Uniques</code> for further parameter details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_equality_constraint(\n    self,\n    columns1: list[str],\n    columns2: list[str],\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if the data's unique values in given columns are equal.\n\n    The ``UniquesEquality`` constraint asserts if the values contained in a column\n    of a ``DataSource``'s columns, are strictly the ones of another ``DataSource``'s\n    columns.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly..\n\n    See [`Uniques`][datajudge.constraints.uniques.Uniques] for further parameter details on ``map_func``,\n    ``reduce_func``, and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.UniquesEquality(\n            ref,\n            ref2=ref2,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_uniques_subset_constraint","title":"add_uniques_subset_constraint","text":"<pre><code>add_uniques_subset_constraint(columns1: list[str], columns2: list[str], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, compare_distinct: bool = False, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None) -&gt; None\n</code></pre> <p>Check if the given columns's unique values in are contained in reference data.</p> <p>The <code>UniquesSubset</code> constraint asserts if the values contained in given column of a <code>DataSource</code> are part of the unique values of given columns of another <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly. <code>max_relative_violations</code> indicates what fraction of rows of the given table may have values not included in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>By default, the number of occurrences affects the computed fraction of violations. To disable this weighting, set <code>compare_distinct=True</code>. This argument does not have an effect on the test results for other <code>Uniques</code> constraints, or if <code>max_relative_violations</code> is 0.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_subset_constraint(\n    self,\n    columns1: list[str],\n    columns2: list[str],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    compare_distinct: bool = False,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if the given columns's unique values in are contained in reference data.\n\n    The ``UniquesSubset`` constraint asserts if the values contained in given column of\n    a ``DataSource`` are part of the unique values of given columns of another\n    ``DataSource``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly.\n    ``max_relative_violations`` indicates what fraction of rows of the given table\n    may have values not included in the reference set of unique values. Please note\n    that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    By default, the number of occurrences affects the computed fraction of violations.\n    To disable this weighting, set ``compare_distinct=True``.\n    This argument does not have an effect on the test results for other [`Uniques`][datajudge.constraints.uniques.Uniques] constraints,\n    or if ``max_relative_violations`` is 0.\n\n    See [`Uniques`][datajudge.constraints.uniques.Uniques] for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.UniquesSubset(\n            ref,\n            ref2=ref2,\n            max_relative_violations=max_relative_violations,\n            compare_distinct=compare_distinct,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_uniques_superset_constraint","title":"add_uniques_superset_constraint","text":"<pre><code>add_uniques_superset_constraint(columns1: list[str], columns2: list[str], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None) -&gt; None\n</code></pre> <p>Check if unique values of columns are contained in the reference data.</p> <p>The <code>UniquesSuperset</code> constraint asserts that reference set of expected values, derived from the unique values in given columns of the reference <code>DataSource</code>, is contained in given columns of a <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly..</p> <p><code>max_relative_violations</code> indicates what fraction of unique values of the given <code>DataSource</code> are not represented in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>One use of this constraint is to test for consistency in columns with expected categorical values.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_superset_constraint(\n    self,\n    columns1: list[str],\n    columns2: list[str],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if unique values of columns are contained in the reference data.\n\n    The ``UniquesSuperset`` constraint asserts that reference set of expected values,\n    derived from the unique values in given columns of the reference ``DataSource``,\n    is contained in given columns of a ``DataSource``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly..\n\n    ``max_relative_violations`` indicates what fraction of unique values of the given\n    ``DataSource`` are not represented in the reference set of unique values. Please\n    note that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    One use of this constraint is to test for consistency in columns with expected\n    categorical values.\n\n    See [`Uniques`][datajudge.constraints.uniques.Uniques] for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.UniquesSuperset(\n            ref,\n            ref2=ref2,\n            max_relative_violations=max_relative_violations,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_varchar_max_length_constraint","title":"add_varchar_max_length_constraint","text":"<pre><code>add_varchar_max_length_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_max_length_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        varchar_constraints.VarCharMaxLength(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.add_varchar_min_length_constraint","title":"add_varchar_min_length_constraint","text":"<pre><code>add_varchar_min_length_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_min_length_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        varchar_constraints.VarCharMinLength(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.from_expressions","title":"from_expressions  <code>classmethod</code>","text":"<pre><code>from_expressions(expression1, expression2, name1: str, name2: str, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>Create a <code>BetweenTableRequirement</code> based on sqlalchemy expressions.</p> <p>Any sqlalchemy object implementing the <code>alias</code> method can be passed as an argument for the <code>expression1</code> and <code>expression2</code> parameters. This could, e.g. be a <code>sqlalchemy.Table</code> object or the result of a <code>sqlalchemy.select</code> invocation.</p> <p><code>name1</code> and <code>name2</code> will be used to represent the expressions in error messages, respectively.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_expressions(\n    cls,\n    expression1,\n    expression2,\n    name1: str,\n    name2: str,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    \"\"\"Create a ``BetweenTableRequirement`` based on sqlalchemy expressions.\n\n    Any sqlalchemy object implementing the ``alias`` method can be passed as an\n    argument for the ``expression1`` and ``expression2`` parameters. This could,\n    e.g. be a ``sqlalchemy.Table`` object or the result of a ``sqlalchemy.select``\n    invocation.\n\n    ``name1`` and ``name2`` will be used to represent the expressions in error messages,\n    respectively.\n    \"\"\"\n    return cls(\n        data_source=ExpressionDataSource(expression1, name1),\n        data_source2=ExpressionDataSource(expression2, name2),\n        date_column=date_column,\n        date_column2=date_column2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.from_raw_queries","title":"from_raw_queries  <code>classmethod</code>","text":"<pre><code>from_raw_queries(query1: str, query2: str, name1: str, name2: str, columns1: list[str] | None = None, columns2: list[str] | None = None, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>Create a <code>BetweenRequirement</code> based on raw query strings.</p> <p>The <code>query1</code> and <code>query2</code> parameters can be passed any query string returning rows, e.g. <code>\"SELECT * FROM myschema.mytable LIMIT 1337\"</code> or <code>\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"</code>.</p> <p><code>name1</code> and <code>name2</code> will be used to represent the queries in error messages, respectively.</p> <p>If constraints rely on specific columns, these should be provided here via <code>columns1</code> and <code>columns2</code> respectively.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_raw_queries(\n    cls,\n    query1: str,\n    query2: str,\n    name1: str,\n    name2: str,\n    columns1: list[str] | None = None,\n    columns2: list[str] | None = None,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    \"\"\"Create a ``BetweenRequirement`` based on raw query strings.\n\n    The ``query1`` and ``query2`` parameters can be passed any query string returning\n    rows, e.g. ``\"SELECT * FROM myschema.mytable LIMIT 1337\"`` or\n    ``\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"``.\n\n    ``name1`` and ``name2`` will be used to represent the queries in error messages,\n    respectively.\n\n    If constraints rely on specific columns, these should be provided here via\n    ``columns1`` and ``columns2`` respectively.\n    \"\"\"\n    return cls(\n        data_source=RawQueryDataSource(query1, name1, columns=columns1),\n        data_source2=RawQueryDataSource(query2, name2, columns=columns2),\n        date_column=date_column,\n        date_column2=date_column2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.BetweenRequirement.from_tables","title":"from_tables  <code>classmethod</code>","text":"<pre><code>from_tables(db_name1: str, schema_name1: str, table_name1: str, db_name2: str, schema_name2: str, table_name2: str, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>Create a <code>BetweenRequirement</code> based on a table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_tables(\n    cls,\n    db_name1: str,\n    schema_name1: str,\n    table_name1: str,\n    db_name2: str,\n    schema_name2: str,\n    table_name2: str,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    \"\"\"Create a ``BetweenRequirement`` based on a table.\"\"\"\n    return cls(\n        data_source=TableDataSource(\n            db_name=db_name1,\n            schema_name=schema_name1,\n            table_name=table_name1,\n        ),\n        data_source2=TableDataSource(\n            db_name=db_name2,\n            schema_name=schema_name2,\n            table_name=table_name2,\n        ),\n        date_column=date_column,\n        date_column2=date_column2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.Condition","title":"Condition  <code>dataclass</code>","text":"<pre><code>Condition(raw_string: str | None = None, conditions: Sequence[Condition] | None = None, reduction_operator: str | None = None)\n</code></pre> <p>Condition allows for further narrowing down of a DataSource in a Constraint.</p> <p>A <code>Condition</code> can be thought of as a filter, the content of a sql 'where' clause or a condition as known from probability theory.</p> <p>While a <code>DataSource</code> is expressed more generally, one might be interested in testing properties of a specific part of said <code>DataSource</code> in light of a particular constraint. Hence using <code>Condition</code> allows for the reusage of a <code>DataSource</code>, in lieu of creating a new custom <code>DataSource</code> with the <code>Condition</code> implicitly built in.</p> <p>A <code>Condition</code> can either be 'atomic', i.e. not further reducible to sub-conditions or 'composite', i.e. combining multiple subconditions. In the former case, it can be instantiated with help of the <code>raw_string</code> parameter, e.g. <code>\"col1 &gt; 0\"</code>. In the latter case, it can be instantiated with help of the <code>conditions</code> and <code>reduction_operator</code> parameters. <code>reduction_operator</code> allows for two values: <code>\"and\"</code> (logical conjunction) and <code>\"or\"</code> (logical disjunction). Note that composition of <code>Condition</code> supports arbitrary degrees of nesting.</p>"},{"location":"api-documentation/#datajudge.Condition.conditions","title":"conditions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>conditions: Sequence[Condition] | None = None\n</code></pre>"},{"location":"api-documentation/#datajudge.Condition.raw_string","title":"raw_string  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_string: str | None = None\n</code></pre>"},{"location":"api-documentation/#datajudge.Condition.reduction_operator","title":"reduction_operator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reduction_operator: str | None = None\n</code></pre>"},{"location":"api-documentation/#datajudge.Constraint","title":"Constraint","text":"<pre><code>Constraint(ref: DataReference, *, ref2: DataReference | None = None, ref_value: Any = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Express a DataReference constraint against either another DataReference or a reference value.</p> <p>Constraints against other DataReferences are typically referred to as 'between' constraints. Please use the the <code>ref2</code> argument to instantiate such a constraint. Constraints against a fixed reference value are typically referred to as 'within' constraints. Please use the <code>ref_value</code> argument to instantiate such a constraint.</p> <p>A constraint typically relies on the comparison of factual and target values. The former represent the key quantity of interest as seen in the database, the latter the key quantity of interest as expected a priori. Such a comparison is meant to be carried out in the <code>test</code> method.</p> <p>In order to obtain such values, the <code>retrieve</code> method defines a mapping from DataReference, be it the DataReference of primary interest, <code>ref</code>, or a baseline DataReference, <code>ref2</code>, to value. If <code>ref_value</code> is already provided, usually no further mapping needs to be taken care of.</p> <p>By default, retrieved arguments are cached indefinitely <code>@lru_cache(maxsize=None)</code>. This can be controlled by setting the <code>cache_size</code> argument to a different value. <code>0</code> disables caching.</p> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    ref_value: Any = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    self._check_if_valid_between_or_within(ref2, ref_value)\n    self._ref = ref\n    self._ref2 = ref2\n    self._ref_value = ref_value\n    self.name = name\n    self._factual_selections: _OptionalSelections = None\n    self._target_selections: _OptionalSelections = None\n    self._factual_queries: list[str] | None = None\n    self._target_queries: list[str] | None = None\n\n    if (output_processors is not None) and (\n        not isinstance(output_processors, list)\n    ):\n        output_processors = [output_processors]\n\n    self._output_processors: list[OutputProcessor] | None = output_processors\n\n    self._cache_size = cache_size\n    self._setup_caching()\n</code></pre>"},{"location":"api-documentation/#datajudge.Constraint.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api-documentation/#datajudge.Constraint.get_description","title":"get_description","text":"<pre><code>get_description() -&gt; str\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def get_description(self) -&gt; str:\n    if self.name is not None:\n        return self.name\n    if self._ref2 is None:\n        data_source_string = str(self._ref.data_source)\n    else:\n        data_source1_string = str(self._ref.data_source)\n        data_source2_string = str(self._ref2.data_source)\n\n        data_source1_substring, data_source2_substring = _uncommon_substrings(\n            data_source1_string, data_source2_string\n        )\n        data_source_string = f\"{data_source1_substring} | {data_source2_substring}\"\n    return self.__class__.__name__ + \"::\" + data_source_string\n</code></pre>"},{"location":"api-documentation/#datajudge.Constraint.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    # ty can't figure out that this is a method and that self is passed\n    # as the first argument.\n    value_factual = self._get_factual_value(engine=engine)  # type: ignore[missing-argument]\n    # ty can't figure out that this is a method and that self is passed\n    # as the first argument.\n    value_target = self._get_target_value(engine=engine)  # type: ignore[missing-argument]\n    is_success, assertion_message = self._compare(value_factual, value_target)\n\n    if is_success:\n        return TestResult.success()\n\n    factual_queries = None\n    if self._factual_selections:\n        factual_queries = [\n            str(\n                factual_selection.compile(\n                    engine, compile_kwargs={\"literal_binds\": True}\n                )\n            )\n            for factual_selection in self._factual_selections\n        ]\n    target_queries = None\n    if self._target_selections:\n        target_queries = [\n            str(\n                target_selection.compile(\n                    engine, compile_kwargs={\"literal_binds\": True}\n                )\n            )\n            for target_selection in self._target_selections\n        ]\n    return TestResult.failure(\n        assertion_message,\n        self.get_description(),\n        factual_queries,\n        target_queries,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.DataSource","title":"DataSource","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api-documentation/#datajudge.Requirement","title":"Requirement","text":"<pre><code>Requirement()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>MutableSequence</code></p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(self):\n    self._constraints: list[Constraint] = []\n    self._data_source: DataSource\n</code></pre>"},{"location":"api-documentation/#datajudge.Requirement.insert","title":"insert","text":"<pre><code>insert(index: int, value: Constraint) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def insert(self, index: int, value: Constraint) -&gt; None:\n    self._constraints.insert(index, value)\n</code></pre>"},{"location":"api-documentation/#datajudge.Requirement.test","title":"test","text":"<pre><code>test(engine) -&gt; list[TestResult]\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def test(self, engine) -&gt; list[TestResult]:\n    return [constraint.test(engine) for constraint in self]\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement","title":"WithinRequirement","text":"<pre><code>WithinRequirement(data_source: DataSource)\n</code></pre> <p>               Bases: <code>Requirement</code></p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(self, data_source: DataSource):\n    self._data_source = data_source\n    super().__init__()\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_categorical_bound_constraint","title":"add_categorical_bound_constraint","text":"<pre><code>add_categorical_bound_constraint(columns: list[str], distribution: dict[_T, tuple[float, float]], default_bounds: tuple[float, float] = (0, 0), max_relative_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check if the distribution of unique values in columns falls within the specified minimum and maximum bounds.</p> <p>The <code>CategoricalBoundConstraint</code> is added to ensure the distribution of unique values in the specified columns of a <code>DataSource</code> falls within the given minimum and maximum bounds defined in the <code>distribution</code> parameter.</p> PARAMETER DESCRIPTION <code>columns</code> <p>A list of column names from the <code>DataSource</code> to apply the constraint on.</p> <p> TYPE: <code>list[str]</code> </p> <code>distribution</code> <p>A dictionary where keys represent unique values and the corresponding tuple values represent the minimum and maximum allowed proportions of the respective unique value in the columns.</p> <p> TYPE: <code>dict[_T, tuple[float, float]]</code> </p> <code>default_bounds</code> <p>A tuple specifying the minimum and maximum allowed proportions for all elements not mentioned in the distribution. By default, it's set to (0, 0), which means all elements not present in <code>distribution</code> will cause a constraint failure.</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0, 0)</code> </p> <code>max_relative_violations</code> <p>A tolerance threshold (0 to 1) for the proportion of elements in the data that can violate the bound constraints without triggering the constraint violation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>condition</code> <p>An optional parameter to specify a <code>Condition</code> object to filter the data before applying the constraint.</p> <p> TYPE: <code>Condition | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>An optional parameter to provide a custom name for the constraint.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cache_size</code> <p>TODO</p> <p> DEFAULT: <code>None</code> </p> Example: <p>This method can be used to test for consistency in columns with expected categorical values or ensure that the distribution of values in a column adheres to a certain criterion.</p> <p>Usage:</p> <pre><code>requirement = WithinRequirement(data_source)\nrequirement.add_categorical_bound_constraint(\n    columns=['column_name'],\n    distribution={'A': (0.2, 0.3), 'B': (0.4, 0.6), 'C': (0.1, 0.2)},\n    max_relative_violations=0.05,\n    name='custom_name'\n)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_categorical_bound_constraint(\n    self,\n    columns: list[str],\n    distribution: dict[_T, tuple[float, float]],\n    default_bounds: tuple[float, float] = (0, 0),\n    max_relative_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if the distribution of unique values in columns falls within the specified minimum and maximum bounds.\n\n    The ``CategoricalBoundConstraint`` is added to ensure the distribution of unique values\n    in the specified columns of a ``DataSource`` falls within the given minimum and maximum\n    bounds defined in the ``distribution`` parameter.\n\n    Parameters\n    ----------\n    columns:\n        A list of column names from the `DataSource` to apply the constraint on.\n    distribution:\n        A dictionary where keys represent unique values and the corresponding\n        tuple values represent the minimum and maximum allowed proportions of the respective\n        unique value in the columns.\n    default_bounds:\n        A tuple specifying the minimum and maximum allowed proportions for all\n        elements not mentioned in the distribution. By default, it's set to (0, 0), which means\n        all elements not present in `distribution` will cause a constraint failure.\n    max_relative_violations:\n        A tolerance threshold (0 to 1) for the proportion of elements in the data that can violate the\n        bound constraints without triggering the constraint violation.\n    condition:\n        An optional parameter to specify a `Condition` object to filter the data\n        before applying the constraint.\n    name:\n        An optional parameter to provide a custom name for the constraint.\n    cache_size:\n        TODO\n\n    Example:\n    -------\n    This method can be used to test for consistency in columns with expected categorical\n    values or ensure that the distribution of values in a column adheres to a certain\n    criterion.\n\n    Usage:\n\n    ```\n    requirement = WithinRequirement(data_source)\n    requirement.add_categorical_bound_constraint(\n        columns=['column_name'],\n        distribution={'A': (0.2, 0.3), 'B': (0.4, 0.6), 'C': (0.1, 0.2)},\n        max_relative_violations=0.05,\n        name='custom_name'\n    )\n    ```\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.CategoricalBoundConstraint(\n            ref,\n            distribution=distribution,\n            default_bounds=default_bounds,\n            max_relative_violations=max_relative_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_column_existence_constraint","title":"add_column_existence_constraint","text":"<pre><code>add_column_existence_constraint(columns: list[str], name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_existence_constraint(\n    self, columns: list[str], name: str | None = None, cache_size=None\n) -&gt; None:\n    # Note that columns are not meant to be part of the reference.\n    ref = DataReference(self._data_source)\n    self._constraints.append(\n        column_constraints.ColumnExistence(ref, columns, cache_size=cache_size)\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_column_type_constraint","title":"add_column_type_constraint","text":"<pre><code>add_column_type_constraint(column: str, column_type: str | TypeEngine, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check if a column type matches the expected column_type.</p> <p>The <code>column_type</code> can be provided as a string (backend-specific type name), a backend-specific SQLAlchemy type, or a SQLAlchemy's generic type.</p> <p>If SQLAlchemy's generic types are used, the check is performed using <code>isinstance</code>, which means that the actual type can also be a subclass of the target type. For more information on SQLAlchemy's generic types, see https://docs.sqlalchemy.org/en/20/core/type_basics.html</p> PARAMETER DESCRIPTION <code>column</code> <p>The name of the column to which the constraint will be applied.</p> <p> TYPE: <code>str</code> </p> <code>column_type</code> <p>The expected type of the column. This can be a string, a backend-specific SQLAlchemy type, or a generic SQLAlchemy type.</p> <p> TYPE: <code>str | TypeEngine</code> </p> <code>name</code> <p>An optional name for the constraint. If not provided, a name will be generated automatically.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_type_constraint(\n    self,\n    column: str,\n    column_type: str | sa.types.TypeEngine,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"\n    Check if a column type matches the expected column_type.\n\n    The ``column_type`` can be provided as a string (backend-specific type name), a backend-specific SQLAlchemy type, or a SQLAlchemy's generic type.\n\n    If SQLAlchemy's generic types are used, the check is performed using ``isinstance``, which means that the actual type can also be a subclass of the target type.\n    For more information on SQLAlchemy's generic types, see https://docs.sqlalchemy.org/en/20/core/type_basics.html\n\n    Parameters\n    ----------\n    column : str\n        The name of the column to which the constraint will be applied.\n\n    column_type : str | sa.types.TypeEngine\n        The expected type of the column. This can be a string, a backend-specific SQLAlchemy type, or a generic SQLAlchemy type.\n\n    name : str | None\n        An optional name for the constraint. If not provided, a name will be generated automatically.\n    \"\"\"\n    ref = DataReference(self._data_source, [column])\n    self._constraints.append(\n        column_constraints.ColumnType(\n            ref,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_date_between_constraint","title":"add_date_between_constraint","text":"<pre><code>add_date_between_constraint(column: str, lower_bound: str, upper_bound: str, min_fraction: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Use string format: <code>lower_bound=\"'20121230'\"</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_between_constraint(\n    self,\n    column: str,\n    lower_bound: str,\n    upper_bound: str,\n    min_fraction: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Use string format: ``lower_bound=\"'20121230'\"``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        date_constraints.DateBetween(\n            ref,\n            min_fraction,\n            lower_bound,\n            upper_bound,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_date_max_constraint","title":"add_date_max_constraint","text":"<pre><code>add_date_max_constraint(column: str, max_value: str, use_upper_bound_reference: bool = True, column_type: str = 'date', condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Ensure all dates to be superior than <code>max_value</code>.</p> <p>Use string format: <code>max_value=\"'20121230'\"</code></p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_upper_bound_reference</code> is <code>True</code>, the maximum date in <code>column</code> has to be smaller or equal to <code>max_value</code>. Otherwise the maximum date in <code>column</code> has to be greater or equal to <code>max_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_max_constraint(\n    self,\n    column: str,\n    max_value: str,\n    use_upper_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Ensure all dates to be superior than ``max_value``.\n\n    Use string format: ``max_value=\"'20121230'\"``\n\n    For more information on ``column_type`` values, see [`add_column_type_constraint`][datajudge.requirements.WithinRequirement.add_column_type_constraint].\n\n    If ``use_upper_bound_reference`` is ``True``, the maximum date in ``column`` has to be smaller or\n    equal to ``max_value``. Otherwise the maximum date in ``column`` has to be greater or equal\n    to ``max_value``.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        date_constraints.DateMax(\n            ref,\n            max_value=max_value,\n            use_upper_bound_reference=use_upper_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_date_min_constraint","title":"add_date_min_constraint","text":"<pre><code>add_date_min_constraint(column: str, min_value: str, use_lower_bound_reference: bool = True, column_type: str = 'date', condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Ensure all dates to be superior than <code>min_value</code>.</p> <p>Use string format: <code>min_value=\"'20121230'\"</code>.</p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_lower_bound_reference</code>, the min of the first table has to be greater or equal to <code>min_value</code>. If not <code>use_upper_bound_reference</code>, the min of the first table has to be smaller or equal to <code>min_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_min_constraint(\n    self,\n    column: str,\n    min_value: str,\n    use_lower_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Ensure all dates to be superior than ``min_value``.\n\n    Use string format: ``min_value=\"'20121230'\"``.\n\n    For more information on ``column_type`` values, see ``add_column_type_constraint``.\n\n    If ``use_lower_bound_reference``, the min of the first table has to be\n    greater or equal to ``min_value``.\n    If not ``use_upper_bound_reference``, the min of the first table has to\n    be smaller or equal to ``min_value``.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        date_constraints.DateMin(\n            ref,\n            min_value=min_value,\n            use_lower_bound_reference=use_lower_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_date_no_gap_constraint","title":"add_date_no_gap_constraint","text":"<pre><code>add_date_no_gap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Express that date range rows have no gap in-between them.</p> <p>The table under inspection must consist of at least one but up to many key columns, identifying an entity. Additionally, a <code>start_column</code> and an <code>end_column</code>, indicating start and end dates, should be provided.</p> <p>Neither of those columns should contain <code>NULL</code> values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p>Note that the value of <code>start_column</code> is expected to be included in each date range. By default, the value of <code>end_column</code> is expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several date ranges.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this gap property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key_values, at least one gap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_no_gap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Express that date range rows have no gap in-between them.\n\n    The table under inspection must consist of at least one but up to many key columns,\n    identifying an entity. Additionally, a ``start_column`` and an ``end_column``,\n    indicating start and end dates, should be provided.\n\n    Neither of those columns should contain ``NULL`` values. Also, it should hold that\n    for a given row, the value of ``end_column`` is strictly greater than the value of\n    ``start_column``.\n\n    Note that the value of ``start_column`` is expected to be included in each date range.\n    By default, the value of ``end_column`` is expected to be included as well - this can\n    however be changed by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often has\n    several rows. Thereby, a key will often come with several date ranges.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be\n    considered as composing the key.\n\n    In order to express a tolerance for some violations of this gap property, use the\n    ``max_relative_n_violations`` parameter. The latter expresses for what fraction\n    of all key_values, at least one gap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = (\n        ([start_column, end_column] + key_columns) if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        date_constraints.DateNoGap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            max_relative_n_violations=max_relative_n_violations,\n            legitimate_gap_size=1 if end_included else 0,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_date_no_overlap_2d_constraint","title":"add_date_no_overlap_2d_constraint","text":"<pre><code>add_date_no_overlap_2d_constraint(start_column1: str, end_column1: str, start_column2: str, end_column2: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Express that several date range rows do not overlap in two date dimensions.</p> <p>The table under inspection must consist of at least one but up to many key columns, identifying an entity. Per date dimension, a start column (<code>start_column1</code>, <code>start_column2</code>) and end (<code>end_column1</code>, <code>end_column2</code>) column should be provided in order to define date ranges.</p> <p>Date ranges in different date dimensions are expected to represent different kinds of dates. For example, let's say that a row in a table indicates an averag temperature forecast. <code>start_column1</code> and <code>end_column1</code> could the date span that was forecasted, e.g. the weather from next Saturday to next Sunday. <code>end_column1</code> and <code>end_column2</code> might indicate the timespan when this forceast was used, e.g. from the previous Monday to Wednesday.</p> <p>Neither of those columns should contain <code>NULL</code> values. Also, the value of <code>end_column_k</code> should be strictly greater than the value of <code>start_column_k</code>.</p> <p>Note that the values of <code>start_column1</code> and <code>start_column2</code> are expected to be included in each date range. By default, the values of <code>end_column1</code> and <code>end_column2</code> are expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in key_columns and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several date ranges.</p> <p>Often, you might want the date ranges for a given key not to overlap.</p> <p>If key_columns is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this non-overlapping property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key_values, at least one overlap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_no_overlap_2d_constraint(\n    self,\n    start_column1: str,\n    end_column1: str,\n    start_column2: str,\n    end_column2: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Express that several date range rows do not overlap in two date dimensions.\n\n    The table under inspection must consist of at least one but up to many key columns,\n    identifying an entity. Per date dimension, a start column (``start_column1``, ``start_column2``)\n    and end (``end_column1``, ``end_column2``) column should be provided in order to define\n    date ranges.\n\n    Date ranges in different date dimensions are expected to represent different kinds\n    of dates. For example, let's say that a row in a table indicates an averag temperature\n    forecast. ``start_column1`` and ``end_column1`` could the date span that was forecasted,\n    e.g. the weather from next Saturday to next Sunday. ``end_column1`` and ``end_column2``\n    might indicate the timespan when this forceast was used, e.g. from the\n    previous Monday to Wednesday.\n\n    Neither of those columns should contain ``NULL`` values. Also, the value of ``end_column_k``\n    should be strictly greater than the value of ``start_column_k``.\n\n    Note that the values of ``start_column1`` and ``start_column2`` are expected to be\n    included in each date range. By default, the values of ``end_column1`` and\n    ``end_column2`` are expected to be included as well - this can however be changed\n    by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in key_columns and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often has\n    several rows. Thereby, a key will often come with several date ranges.\n\n    Often, you might want the date ranges for a given key not to overlap.\n\n    If key_columns is ``None`` or ``[]``, all columns of the table will be considered as\n    composing the key.\n\n    In order to express a tolerance for some violations of this non-overlapping property,\n    use the ``max_relative_n_violations`` parameter. The latter expresses for what fraction\n    of all key_values, at least one overlap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = (\n        [start_column1, end_column1, start_column2, end_column2] + key_columns\n        if key_columns\n        else []\n    )\n    ref = DataReference(\n        self._data_source,\n        relevant_columns,\n        condition,\n    )\n    self._constraints.append(\n        date_constraints.DateNoOverlap2d(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column1, start_column2],\n            end_columns=[end_column1, end_column2],\n            end_included=end_included,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_date_no_overlap_constraint","title":"add_date_no_overlap_constraint","text":"<pre><code>add_date_no_overlap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Constraint expressing that several date range rows may not overlap.</p> <p>The <code>DataSource</code> under inspection must consist of at least one but up to many <code>key_columns</code>, identifying an entity, a <code>start_column</code> and an <code>end_column</code>.</p> <p>For a given row in this <code>DataSource</code>, <code>start_column</code> and <code>end_column</code> indicate a date range. Neither of those columns should contain NULL values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p>Note that the value of <code>start_column</code> is expected to be included in each date range. By default, the value of <code>end_column</code> is expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several date ranges.</p> <p>Often, you might want the date ranges for a given key not to overlap.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this non-overlapping property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key values, at least one overlap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_no_overlap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Constraint expressing that several date range rows may not overlap.\n\n    The [`DataSource`][datajudge.DataSource] under inspection must consist of at least one but up\n    to many ``key_columns``, identifying an entity, a ``start_column`` and an\n    ``end_column``.\n\n    For a given row in this [`DataSource`][datajudge.DataSource], ``start_column`` and ``end_column`` indicate a\n    date range. Neither of those columns should contain NULL values. Also, it\n    should hold that for a given row, the value of ``end_column`` is strictly greater\n    than the value of ``start_column``.\n\n    Note that the value of ``start_column`` is expected to be included in each date\n    range. By default, the value of ``end_column`` is expected to be included as well -\n    this can however be changed by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often\n    has several rows. Thereby, a key will often come with several date ranges.\n\n    Often, you might want the date ranges for a given key not to overlap.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be considered\n    as composing the key.\n\n    In order to express a tolerance for some violations of this non-overlapping\n    property, use the ``max_relative_n_violations`` parameter. The latter expresses for\n    what fraction of all key values, at least one overlap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = [start_column, end_column] + (\n        key_columns if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        date_constraints.DateNoOverlap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            end_included=end_included,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_functional_dependency_constraint","title":"add_functional_dependency_constraint","text":"<pre><code>add_functional_dependency_constraint(key_columns: list[str], value_columns: list[str], condition: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>Expresses a functional dependency, a constraint where the <code>value_columns</code> are uniquely determined by the <code>key_columns</code>.</p> <p>This means that for each unique combination of values in the <code>key_columns</code>, there is exactly one corresponding combination of values in the <code>value_columns</code>.</p> <p>The <code>add_unique_constraint</code> constraint is a special case of this constraint, where the <code>key_columns</code> are a primary key, and all other columns are included <code>value_columns</code>. This constraint allows for a more general definition of functional dependencies, where the <code>key_columns</code> are not necessarily a primary key.</p> <p>An additional configuration option (for details see the analogous parameter in for <code>Uniques</code>-constraints) on how the output is sorted and how many counterexamples are shown is available as <code>output_processors</code>.</p> <p>An additional configuration option (for details see the analogous parameter in for <code>Uniques</code>-constraints) on how the output is sorted and how many counterexamples are shown is available as <code>output_processors</code>.</p> <p>For more information on functional dependencies, see https://en.wikipedia.org/wiki/Functional_dependency.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_functional_dependency_constraint(\n    self,\n    key_columns: list[str],\n    value_columns: list[str],\n    condition: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    \"\"\"Expresses a functional dependency, a constraint where the `value_columns` are uniquely determined by the `key_columns`.\n\n    This means that for each unique combination of values in the `key_columns`, there is exactly one corresponding combination of values in the `value_columns`.\n\n    The ``add_unique_constraint`` constraint is a special case of this constraint, where the ``key_columns`` are a primary key,\n    and all other columns are included ``value_columns``.\n    This constraint allows for a more general definition of functional dependencies, where the ``key_columns`` are not necessarily a primary key.\n\n    An additional configuration option (for details see the analogous parameter in for ``Uniques``-constraints)\n    on how the output is sorted and how many counterexamples are shown is available as ``output_processors``.\n\n    An additional configuration option (for details see the analogous parameter in for ``Uniques``-constraints)\n    on how the output is sorted and how many counterexamples are shown is available as ``output_processors``.\n\n    For more information on functional dependencies, see https://en.wikipedia.org/wiki/Functional_dependency.\n    \"\"\"\n    relevant_columns = key_columns + value_columns\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        miscs_constraints.FunctionalDependency(\n            ref,\n            key_columns=key_columns,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_groupby_aggregation_constraint","title":"add_groupby_aggregation_constraint","text":"<pre><code>add_groupby_aggregation_constraint(columns: Sequence[str], aggregation_column: str, start_value: int, tolerance: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Check whether array aggregate corresponds to an integer range.</p> <p>The <code>DataSource</code> is grouped by <code>columns</code>. Sql's <code>array_agg</code> function is then applied to the <code>aggregate_column</code>.</p> <p>Since we expect <code>aggregate_column</code> to be a numeric column, this leads to a multiset of aggregated values. These values should correspond to the integers ranging from <code>start_value</code> to the cardinality of the multiset.</p> <p>In order to allow for slight deviations from this pattern, <code>tolerance</code> expresses the fraction of all grouped-by rows, which may be incomplete ranges.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_groupby_aggregation_constraint(\n    self,\n    columns: Sequence[str],\n    aggregation_column: str,\n    start_value: int,\n    tolerance: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Check whether array aggregate corresponds to an integer range.\n\n    The ``DataSource`` is grouped by ``columns``. Sql's ``array_agg`` function is then\n    applied to the ``aggregate_column``.\n\n    Since we expect ``aggregate_column`` to be a numeric column, this leads to\n    a multiset of aggregated values. These values should correspond to the integers\n    ranging from ``start_value`` to the cardinality of the multiset.\n\n    In order to allow for slight deviations from this pattern, ``tolerance`` expresses\n    the fraction of all grouped-by rows, which may be incomplete ranges.\n    \"\"\"\n    ref = DataReference(self._data_source, list(columns), condition)\n    self._constraints.append(\n        groupby_constraints.AggregateNumericRangeEquality(\n            ref,\n            aggregation_column=aggregation_column,\n            tolerance=tolerance,\n            start_value=start_value,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_max_null_fraction_constraint","title":"add_max_null_fraction_constraint","text":"<pre><code>add_max_null_fraction_constraint(column: str, max_null_fraction: float, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Assert that <code>column</code> has less than a certain fraction of <code>NULL</code> values.</p> <p><code>max_null_fraction</code> is expected to lie within [0, 1].</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_max_null_fraction_constraint(\n    self,\n    column: str,\n    max_null_fraction: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Assert that ``column`` has less than a certain fraction of ``NULL`` values.\n\n    ``max_null_fraction`` is expected to lie within [0, 1].\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        miscs_constraints.MaxNullFraction(\n            ref,\n            max_null_fraction=max_null_fraction,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_n_rows_equality_constraint","title":"add_n_rows_equality_constraint","text":"<pre><code>add_n_rows_equality_constraint(n_rows: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_equality_constraint(\n    self,\n    n_rows: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, None, condition)\n    self._constraints.append(\n        nrows_constraints.NRowsEquality(\n            ref, n_rows=n_rows, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_n_rows_max_constraint","title":"add_n_rows_max_constraint","text":"<pre><code>add_n_rows_max_constraint(n_rows_max: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_max_constraint(\n    self,\n    n_rows_max: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, None, condition)\n    self._constraints.append(\n        nrows_constraints.NRowsMax(\n            ref, n_rows=n_rows_max, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_n_rows_min_constraint","title":"add_n_rows_min_constraint","text":"<pre><code>add_n_rows_min_constraint(n_rows_min: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_min_constraint(\n    self,\n    n_rows_min: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, None, condition)\n    self._constraints.append(\n        nrows_constraints.NRowsMin(\n            ref, n_rows=n_rows_min, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_n_uniques_equality_constraint","title":"add_n_uniques_equality_constraint","text":"<pre><code>add_n_uniques_equality_constraint(columns: list[str] | None, n_uniques: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_equality_constraint(\n    self,\n    columns: list[str] | None,\n    n_uniques: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.NUniquesEquality(\n            ref, n_uniques=n_uniques, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_null_absence_constraint","title":"add_null_absence_constraint","text":"<pre><code>add_null_absence_constraint(column: str, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_null_absence_constraint(\n    self,\n    column: str,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        miscs_constraints.MaxNullFraction(\n            ref, max_null_fraction=0, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_between_constraint","title":"add_numeric_between_constraint","text":"<pre><code>add_numeric_between_constraint(column: str, lower_bound: float, upper_bound: float, min_fraction: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the column's values lie between <code>lower_bound</code> and <code>upper_bound</code>.</p> <p>Note that both bounds are inclusive.</p> <p>Unless specified otherwise via the usage of a <code>condition</code>, <code>NULL</code> values will be considered in the denominator of <code>min_fraction</code>. <code>NULL</code> values will never be considered to lie in the interval [<code>lower_bound</code>, <code>upper_bound</code>].</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_between_constraint(\n    self,\n    column: str,\n    lower_bound: float,\n    upper_bound: float,\n    min_fraction: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the column's values lie between ``lower_bound`` and ``upper_bound``.\n\n    Note that both bounds are inclusive.\n\n    Unless specified otherwise via the usage of a ``condition``, ``NULL`` values will\n    be considered in the denominator of ``min_fraction``. ``NULL`` values will never be\n    considered to lie in the interval [``lower_bound``, ``upper_bound``].\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericBetween(\n            ref,\n            min_fraction,\n            lower_bound,\n            upper_bound,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_max_constraint","title":"add_numeric_max_constraint","text":"<pre><code>add_numeric_max_constraint(column: str, max_value: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>All values in <code>column</code> are less or equal <code>max_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_max_constraint(\n    self,\n    column: str,\n    max_value: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"All values in ``column`` are less or equal ``max_value``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericMax(\n            ref, max_value=max_value, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_mean_constraint","title":"add_numeric_mean_constraint","text":"<pre><code>add_numeric_mean_constraint(column: str, mean_value: float, max_absolute_deviation: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert the mean of the column <code>column</code> deviates at most <code>max_deviation</code> from <code>mean_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_mean_constraint(\n    self,\n    column: str,\n    mean_value: float,\n    max_absolute_deviation: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert the mean of the column ``column`` deviates at most ``max_deviation`` from ``mean_value``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericMean(\n            ref,\n            max_absolute_deviation,\n            mean_value=mean_value,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_min_constraint","title":"add_numeric_min_constraint","text":"<pre><code>add_numeric_min_constraint(column: str, min_value: float, condition: Condition | None = None, cache_size=None) -&gt; None\n</code></pre> <p>All values in <code>column</code> are greater or equal <code>min_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_min_constraint(\n    self,\n    column: str,\n    min_value: float,\n    condition: Condition | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"All values in ``column`` are greater or equal ``min_value``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericMin(\n            ref, min_value=min_value, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_no_gap_constraint","title":"add_numeric_no_gap_constraint","text":"<pre><code>add_numeric_no_gap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, legitimate_gap_size: float = 0, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Express that numeric interval rows have no gaps larger than some max value in-between them.</p> <p>The table under inspection must consist of at least one but up to many key columns, identifying an entity. Additionally, a <code>start_column</code> and an <code>end_column</code>, indicating interval start and end values, should be provided.</p> <p>Neither of those columns should contain <code>NULL</code> values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p><code>legitimate_gap_size</code> is the maximum tollerated gap size between two intervals.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several intervals.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this gap property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key_values, at least one gap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_no_gap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    legitimate_gap_size: float = 0,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Express that numeric interval rows have no gaps larger than some max value in-between them.\n\n    The table under inspection must consist of at least one but up to many key columns,\n    identifying an entity. Additionally, a ``start_column`` and an ``end_column``,\n    indicating interval start and end values, should be provided.\n\n    Neither of those columns should contain ``NULL`` values. Also, it should hold that\n    for a given row, the value of ``end_column`` is strictly greater than the value of\n    ``start_column``.\n\n    ``legitimate_gap_size`` is the maximum tollerated gap size between two intervals.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often has\n    several rows. Thereby, a key will often come with several intervals.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be\n    considered as composing the key.\n\n    In order to express a tolerance for some violations of this gap property, use the\n    ``max_relative_n_violations`` parameter. The latter expresses for what fraction\n    of all key_values, at least one gap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = (\n        ([start_column, end_column] + key_columns) if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        numeric_constraints.NumericNoGap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            legitimate_gap_size=legitimate_gap_size,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_no_overlap_constraint","title":"add_numeric_no_overlap_constraint","text":"<pre><code>add_numeric_no_overlap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Constraint expressing that several numeric interval rows may not overlap.</p> <p>The <code>DataSource</code> under inspection must consist of at least one but up to many <code>key_columns</code>, identifying an entity, a <code>start_column</code> and an <code>end_column</code>.</p> <p>For a given row in this <code>DataSource</code>, <code>start_column</code> and <code>end_column</code> indicate a numeric interval. Neither of those columns should contain NULL values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p>Note that the value of <code>start_column</code> is expected to be included in each interval. By default, the value of <code>end_column</code> is expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several intervals.</p> <p>Often, you might want the intervals for a given key not to overlap.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this non-overlapping property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key values, at least one overlap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_no_overlap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Constraint expressing that several numeric interval rows may not overlap.\n\n    The ``DataSource`` under inspection must consist of at least one but up\n    to many ``key_columns``, identifying an entity, a ``start_column`` and an\n    ``end_column``.\n\n    For a given row in this ``DataSource``, ``start_column`` and ``end_column`` indicate a\n    numeric interval. Neither of those columns should contain NULL values. Also, it\n    should hold that for a given row, the value of ``end_column`` is strictly greater\n    than the value of ``start_column``.\n\n    Note that the value of ``start_column`` is expected to be included in each interval.\n    By default, the value of ``end_column`` is expected to be included as well -\n    this can however be changed by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often\n    has several rows. Thereby, a key will often come with several intervals.\n\n    Often, you might want the intervals for a given key not to overlap.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be considered\n    as composing the key.\n\n    In order to express a tolerance for some violations of this non-overlapping\n    property, use the ``max_relative_n_violations`` parameter. The latter expresses for\n    what fraction of all key values, at least one overlap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = [start_column, end_column] + (\n        key_columns if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        numeric_constraints.NumericNoOverlap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            end_included=end_included,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_numeric_percentile_constraint","title":"add_numeric_percentile_constraint","text":"<pre><code>add_numeric_percentile_constraint(column: str, percentage: float, expected_percentile: float, max_absolute_deviation: float | None = None, max_relative_deviation: float | None = None, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the <code>percentage</code>-th percentile is approximately <code>expected_percentile</code>.</p> <p>The percentile is defined as the smallest value present in <code>column</code> for which <code>percentage</code> % of the values in <code>column</code> are less or equal. <code>NULL</code> values are ignored.</p> <p>Hence, if <code>percentage</code> is less than the inverse of the number of non-<code>NULL</code> rows, <code>None</code> is received as the <code>percentage</code> -th percentile.</p> <p><code>percentage</code> is expected to be provided in percent. The median, for example, would correspond to <code>percentage=50</code>.</p> <p>At least one of <code>max_absolute_deviation</code> and <code>max_relative_deviation</code> must be provided.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_percentile_constraint(\n    self,\n    column: str,\n    percentage: float,\n    expected_percentile: float,\n    max_absolute_deviation: float | None = None,\n    max_relative_deviation: float | None = None,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the ``percentage``-th percentile is approximately ``expected_percentile``.\n\n    The percentile is defined as the smallest value present in ``column`` for which\n    ``percentage`` % of the values in ``column`` are less or equal. ``NULL`` values\n    are ignored.\n\n    Hence, if ``percentage`` is less than the inverse of the number of non-``NULL`` rows,\n    ``None`` is received as the ``percentage`` -th percentile.\n\n    ``percentage`` is expected to be provided in percent. The median, for example, would\n    correspond to ``percentage=50``.\n\n    At least one of ``max_absolute_deviation`` and ``max_relative_deviation`` must\n    be provided.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericPercentile(\n            ref,\n            percentage=percentage,\n            expected_percentile=expected_percentile,\n            max_absolute_deviation=max_absolute_deviation,\n            max_relative_deviation=max_relative_deviation,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_primary_key_definition_constraint","title":"add_primary_key_definition_constraint","text":"<pre><code>add_primary_key_definition_constraint(primary_keys: list[str], name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check that the primary key constraints in the database are exactly equal to the given column names.</p> <p>Note that this doesn't actually check that the primary key values are unique across the table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_primary_key_definition_constraint(\n    self,\n    primary_keys: list[str],\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check that the primary key constraints in the database are exactly equal to the given column names.\n\n    Note that this doesn't actually check that the primary key values are unique across the table.\n    \"\"\"\n    ref = DataReference(self._data_source)\n    self._constraints.append(\n        miscs_constraints.PrimaryKeyDefinition(\n            ref, primary_keys, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_uniqueness_constraint","title":"add_uniqueness_constraint","text":"<pre><code>add_uniqueness_constraint(columns: list[str] | None = None, max_duplicate_fraction: float = 0, condition: Condition | None = None, max_absolute_n_duplicates: int = 0, infer_pk_columns: bool = False, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Columns should uniquely identify row.</p> <p>Given a list of columns <code>columns</code>, validate the condition of a primary key, i.e. uniqueness of tuples in said columns. This constraint has a tolerance for inconsistencies, expressed via <code>max_duplicate_fraction</code>. The latter suggests that the number of uniques from said columns is larger or equal to <code>1 - max_duplicate_fraction</code> times the number of rows.</p> <p>If <code>infer_pk_columns</code> is <code>True</code>, <code>columns</code> will be retrieved from the primary keys. If <code>columns</code> is <code>None</code> and <code>infer_pk_column</code> is <code>False</code>, the fallback is validating that all rows in a table are unique.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniqueness_constraint(\n    self,\n    columns: list[str] | None = None,\n    max_duplicate_fraction: float = 0,\n    condition: Condition | None = None,\n    max_absolute_n_duplicates: int = 0,\n    infer_pk_columns: bool = False,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Columns should uniquely identify row.\n\n    Given a list of columns ``columns``, validate the condition of a primary key, i.e.\n    uniqueness of tuples in said columns. This constraint has a tolerance\n    for inconsistencies, expressed via ``max_duplicate_fraction``. The latter\n    suggests that the number of uniques from said columns is larger or equal\n    to ``1 - max_duplicate_fraction`` times the number of rows.\n\n    If ``infer_pk_columns`` is ``True``, ``columns`` will be retrieved from the primary keys.\n    If ``columns`` is ``None`` and ``infer_pk_column`` is ``False``, the fallback is\n    validating that all rows in a table are unique.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        miscs_constraints.Uniqueness(\n            ref,\n            max_duplicate_fraction=max_duplicate_fraction,\n            max_absolute_n_duplicates=max_absolute_n_duplicates,\n            infer_pk_columns=infer_pk_columns,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_uniques_equality_constraint","title":"add_uniques_equality_constraint","text":"<pre><code>add_uniques_equality_constraint(columns: list[str], uniques: Collection[_T], filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Check if the data's unique values are equal to a given set of values.</p> <p>The <code>UniquesEquality</code> constraint asserts if the values contained in a column of a <code>DataSource</code> are strictly the ones of a reference set of expected values, specified via the <code>uniques</code> parameter.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>.</p> <p>By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p>See the <code>Uniques</code> class for further parameter details on <code>map_func</code> and <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_equality_constraint(\n    self,\n    columns: list[str],\n    uniques: Collection[_T],\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Check if the data's unique values are equal to a given set of values.\n\n    The `UniquesEquality` constraint asserts if the values contained in a column\n    of a `DataSource` are strictly the ones of a reference set of expected values,\n    specified via the `uniques` parameter.\n\n    Null values in the columns `columns` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for `WithinRequirement`.\n\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom `filter_func` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing `None` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set `filter_func` explicitly.\n\n    See the `Uniques` class for further parameter details on `map_func` and\n    `reduce_func`, and `output_processors`.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.UniquesEquality(\n            ref,\n            uniques=uniques,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_uniques_subset_constraint","title":"add_uniques_subset_constraint","text":"<pre><code>add_uniques_subset_constraint(columns: list[str], uniques: Collection[_T], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, compare_distinct: bool = False, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>Check if the data's unique values are contained in a given set of values.</p> <p>The <code>UniquesSubset</code> constraint asserts if the values contained in a column of a <code>DataSource</code> are part of a reference set of expected values, specified via <code>uniques</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p><code>max_relative_violations</code> indicates what fraction of rows of the given table may have values not included in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>By default, the number of occurrences affects the computed fraction of violations. To disable this weighting, set <code>compare_distinct=True</code>. This argument does not have an effect on the test results for other <code>Uniques</code> constraints, or if <code>max_relative_violations</code> is 0.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_subset_constraint(\n    self,\n    columns: list[str],\n    uniques: Collection[_T],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    compare_distinct: bool = False,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    \"\"\"Check if the data's unique values are contained in a given set of values.\n\n    The ``UniquesSubset`` constraint asserts if the values contained in a column of\n    a ``DataSource`` are part of a reference set of expected values, specified via\n    ``uniques``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly.\n\n\n    ``max_relative_violations`` indicates what fraction of rows of the given table\n    may have values not included in the reference set of unique values. Please note\n    that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    By default, the number of occurrences affects the computed fraction of violations.\n    To disable this weighting, set `compare_distinct=True`.\n    This argument does not have an effect on the test results for other `Uniques` constraints,\n    or if `max_relative_violations` is 0.\n\n    See ``Uniques`` for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.UniquesSubset(\n            ref,\n            uniques=uniques,\n            max_relative_violations=max_relative_violations,\n            filter_func=filter_func,\n            compare_distinct=compare_distinct,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_uniques_superset_constraint","title":"add_uniques_superset_constraint","text":"<pre><code>add_uniques_superset_constraint(columns: list[str], uniques: Collection[_T], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>Check if unique values of columns are contained in the reference data.</p> <p>The <code>UniquesSuperset</code> constraint asserts that reference set of expected values, specified via <code>uniques</code>, is contained in given columns of a <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>.</p> <p>By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> will cause (possibly often unintended) changes in behavior when the user adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p><code>max_relative_violations</code> indicates what fraction of unique values of the given <code>DataSource</code> are not represented in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>One use of this constraint is to test for consistency in columns with expected categorical values.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_superset_constraint(\n    self,\n    columns: list[str],\n    uniques: Collection[_T],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    \"\"\"Check if unique values of columns are contained in the reference data.\n\n    The ``UniquesSuperset`` constraint asserts that reference set of expected values,\n    specified via ``uniques``, is contained in given columns of a ``DataSource``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    will cause (possibly often unintended) changes in behavior when the user adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly.\n\n    ``max_relative_violations`` indicates what fraction of unique values of the given\n    ``DataSource`` are not represented in the reference set of unique values. Please\n    note that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    One use of this constraint is to test for consistency in columns with expected\n    categorical values.\n\n    See ``Uniques`` for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.UniquesSuperset(\n            ref,\n            uniques=uniques,\n            max_relative_violations=max_relative_violations,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_varchar_max_length_constraint","title":"add_varchar_max_length_constraint","text":"<pre><code>add_varchar_max_length_constraint(column: str, max_length: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_max_length_constraint(\n    self,\n    column: str,\n    max_length: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharMaxLength(\n            ref,\n            max_length=max_length,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_varchar_min_length_constraint","title":"add_varchar_min_length_constraint","text":"<pre><code>add_varchar_min_length_constraint(column: str, min_length: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_min_length_constraint(\n    self,\n    column: str,\n    min_length: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharMinLength(\n            ref,\n            min_length=min_length,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_varchar_regex_constraint","title":"add_varchar_regex_constraint","text":"<pre><code>add_varchar_regex_constraint(column: str, regex: str, condition: Condition | None = None, name: str | None = None, allow_none: bool = False, relative_tolerance: float = 0.0, aggregated: bool = True, n_counterexamples: int = 5, cache_size=None)\n</code></pre> <p>Assesses whether the values in a column match a given regular expression pattern.</p> <p>The option <code>allow_none</code> can be used in cases where the column is defined as nullable and contains null values.</p> <p>How the tolerance factor is calculated can be controlled with the <code>aggregated</code> flag. When <code>True</code>, the tolerance is calculated using unique values. If not, the tolerance is calculated using all the instances of the data.</p> <p><code>n_counterexamples</code> defines how many counterexamples are displayed in an assertion text. If all counterexamples are meant to be shown, provide <code>-1</code> as an argument.</p> <p>When using this method, the regex matching will take place in memory. If instead, you would like the matching to take place in database which is typically faster and substantially more memory-saving, please consider using <code>add_varchar_regex_constraint_db</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_regex_constraint(\n    self,\n    column: str,\n    regex: str,\n    condition: Condition | None = None,\n    name: str | None = None,\n    allow_none: bool = False,\n    relative_tolerance: float = 0.0,\n    aggregated: bool = True,\n    n_counterexamples: int = 5,\n    cache_size=None,\n):\n    \"\"\"Assesses whether the values in a column match a given regular expression pattern.\n\n    The option ``allow_none`` can be used in cases where the column is defined as\n    nullable and contains null values.\n\n    How the tolerance factor is calculated can be controlled with the ``aggregated``\n    flag. When ``True``, the tolerance is calculated using unique values. If not, the\n    tolerance is calculated using all the instances of the data.\n\n    ``n_counterexamples`` defines how many counterexamples are displayed in an\n    assertion text. If all counterexamples are meant to be shown, provide ``-1`` as\n    an argument.\n\n    When using this method, the regex matching will take place in memory. If instead,\n    you would like the matching to take place in database which is typically faster and\n    substantially more memory-saving, please consider using\n    ``add_varchar_regex_constraint_db``.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharRegex(\n            ref,\n            regex,\n            allow_none=allow_none,\n            relative_tolerance=relative_tolerance,\n            aggregated=aggregated,\n            n_counterexamples=n_counterexamples,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.add_varchar_regex_constraint_db","title":"add_varchar_regex_constraint_db","text":"<pre><code>add_varchar_regex_constraint_db(column: str, regex: str, condition: Condition | None = None, name: str | None = None, relative_tolerance: float = 0.0, aggregated: bool = True, n_counterexamples: int = 5, cache_size=None)\n</code></pre> <p>Assesses whether the values in a column match a given regular expression pattern.</p> <p>How the tolerance factor is calculated can be controlled with the <code>aggregated</code> flag. When <code>True</code>, the tolerance is calculated using unique values. If not, the tolerance is calculated using all the instances of the data.</p> <p><code>n_counterexamples</code> defines how many counterexamples are displayed in an assertion text. If all counterexamples are meant to be shown, provide <code>-1</code> as an argument.</p> <p>When using this method, the regex matching will take place in database, which is only supported for Postgres, Sqllite and Snowflake. Note that for this feature is only for Snowflake when using sqlalchemy-snowflake &gt;= 1.4.0. As an alternative, <code>add_varchar_regex_constraint</code> performs the regex matching in memory. This is typically slower and more expensive in terms of memory but available on all supported database mamangement systems.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_regex_constraint_db(\n    self,\n    column: str,\n    regex: str,\n    condition: Condition | None = None,\n    name: str | None = None,\n    relative_tolerance: float = 0.0,\n    aggregated: bool = True,\n    n_counterexamples: int = 5,\n    cache_size=None,\n):\n    \"\"\"Assesses whether the values in a column match a given regular expression pattern.\n\n    How the tolerance factor is calculated can be controlled with the ``aggregated``\n    flag. When ``True``, the tolerance is calculated using unique values. If not, the\n    tolerance is calculated using all the instances of the data.\n\n    ``n_counterexamples`` defines how many counterexamples are displayed in an\n    assertion text. If all counterexamples are meant to be shown, provide ``-1`` as\n    an argument.\n\n    When using this method, the regex matching will take place in database, which is\n    only supported for Postgres, Sqllite and Snowflake. Note that for this\n    feature is only for Snowflake when using sqlalchemy-snowflake &gt;= 1.4.0. As an\n    alternative, ``add_varchar_regex_constraint`` performs the regex matching in memory.\n    This is typically slower and more expensive in terms of memory but available\n    on all supported database mamangement systems.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharRegexDb(\n            ref,\n            regex=regex,\n            relative_tolerance=relative_tolerance,\n            aggregated=aggregated,\n            n_counterexamples=n_counterexamples,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.from_expression","title":"from_expression  <code>classmethod</code>","text":"<pre><code>from_expression(expression: FromClause, name: str)\n</code></pre> <p>Create a <code>WithinRequirement</code> based on a sqlalchemy expression.</p> <p>Any sqlalchemy object implementing the <code>alias</code> method can be passed as an argument for the <code>expression</code> parameter. This could, e.g. be an <code>sqlalchemy.Table</code> object or the result of a <code>sqlalchemy.select</code> call.</p> <p>The <code>name</code> will be used to represent this expression in error messages.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_expression(cls, expression: selectable.FromClause, name: str):\n    \"\"\"Create a ``WithinRequirement`` based on a sqlalchemy expression.\n\n    Any sqlalchemy object implementing the ``alias`` method can be passed as an\n    argument for the ``expression`` parameter. This could, e.g. be an\n    ``sqlalchemy.Table`` object or the result of a ``sqlalchemy.select`` call.\n\n    The ``name`` will be used to represent this expression in error messages.\n    \"\"\"\n    return cls(data_source=ExpressionDataSource(expression, name))\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.from_raw_query","title":"from_raw_query  <code>classmethod</code>","text":"<pre><code>from_raw_query(query: str, name: str, columns: list[str] | None = None)\n</code></pre> <p>Create a <code>WithinRequirement</code> based on a raw query string.</p> <p>The <code>query</code> parameter can be passed any query string returning rows, e.g. <code>\"SELECT * FROM myschema.mytable LIMIT 1337\"</code> or <code>\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"</code>.</p> <p>The <code>name</code> will be used to represent this query in error messages.</p> <p>If constraints rely on specific columns, these should be provided here via <code>columns</code>, e.g. <code>[\"id\", \"name\"]</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_raw_query(cls, query: str, name: str, columns: list[str] | None = None):\n    \"\"\"Create a ``WithinRequirement`` based on a raw query string.\n\n    The ``query`` parameter can be passed any query string returning rows, e.g.\n    ``\"SELECT * FROM myschema.mytable LIMIT 1337\"`` or\n    ``\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"``.\n\n    The ``name`` will be used to represent this query in error messages.\n\n    If constraints rely on specific columns, these should be provided here via\n    ``columns``, e.g. ``[\"id\", \"name\"]``.\n    \"\"\"\n    return cls(data_source=RawQueryDataSource(query, name, columns=columns))\n</code></pre>"},{"location":"api-documentation/#datajudge.WithinRequirement.from_table","title":"from_table  <code>classmethod</code>","text":"<pre><code>from_table(db_name: str, schema_name: str, table_name: str)\n</code></pre> <p>Create a <code>WithinRequirement</code> based on a table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_table(cls, db_name: str, schema_name: str, table_name: str):\n    \"\"\"Create a `WithinRequirement` based on a table.\"\"\"\n    return cls(\n        data_source=TableDataSource(\n            db_name=db_name, schema_name=schema_name, table_name=table_name\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.condition","title":"condition","text":""},{"location":"api-documentation/#datajudge.condition.Condition","title":"Condition  <code>dataclass</code>","text":"<pre><code>Condition(raw_string: str | None = None, conditions: Sequence[Condition] | None = None, reduction_operator: str | None = None)\n</code></pre> <p>Condition allows for further narrowing down of a DataSource in a Constraint.</p> <p>A <code>Condition</code> can be thought of as a filter, the content of a sql 'where' clause or a condition as known from probability theory.</p> <p>While a <code>DataSource</code> is expressed more generally, one might be interested in testing properties of a specific part of said <code>DataSource</code> in light of a particular constraint. Hence using <code>Condition</code> allows for the reusage of a <code>DataSource</code>, in lieu of creating a new custom <code>DataSource</code> with the <code>Condition</code> implicitly built in.</p> <p>A <code>Condition</code> can either be 'atomic', i.e. not further reducible to sub-conditions or 'composite', i.e. combining multiple subconditions. In the former case, it can be instantiated with help of the <code>raw_string</code> parameter, e.g. <code>\"col1 &gt; 0\"</code>. In the latter case, it can be instantiated with help of the <code>conditions</code> and <code>reduction_operator</code> parameters. <code>reduction_operator</code> allows for two values: <code>\"and\"</code> (logical conjunction) and <code>\"or\"</code> (logical disjunction). Note that composition of <code>Condition</code> supports arbitrary degrees of nesting.</p>"},{"location":"api-documentation/#datajudge.condition.Condition.conditions","title":"conditions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>conditions: Sequence[Condition] | None = None\n</code></pre>"},{"location":"api-documentation/#datajudge.condition.Condition.raw_string","title":"raw_string  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_string: str | None = None\n</code></pre>"},{"location":"api-documentation/#datajudge.condition.Condition.reduction_operator","title":"reduction_operator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reduction_operator: str | None = None\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints","title":"constraints","text":""},{"location":"api-documentation/#datajudge.constraints.base","title":"base","text":""},{"location":"api-documentation/#datajudge.constraints.base.Constraint","title":"Constraint","text":"<pre><code>Constraint(ref: DataReference, *, ref2: DataReference | None = None, ref_value: Any = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Express a DataReference constraint against either another DataReference or a reference value.</p> <p>Constraints against other DataReferences are typically referred to as 'between' constraints. Please use the the <code>ref2</code> argument to instantiate such a constraint. Constraints against a fixed reference value are typically referred to as 'within' constraints. Please use the <code>ref_value</code> argument to instantiate such a constraint.</p> <p>A constraint typically relies on the comparison of factual and target values. The former represent the key quantity of interest as seen in the database, the latter the key quantity of interest as expected a priori. Such a comparison is meant to be carried out in the <code>test</code> method.</p> <p>In order to obtain such values, the <code>retrieve</code> method defines a mapping from DataReference, be it the DataReference of primary interest, <code>ref</code>, or a baseline DataReference, <code>ref2</code>, to value. If <code>ref_value</code> is already provided, usually no further mapping needs to be taken care of.</p> <p>By default, retrieved arguments are cached indefinitely <code>@lru_cache(maxsize=None)</code>. This can be controlled by setting the <code>cache_size</code> argument to a different value. <code>0</code> disables caching.</p> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    ref_value: Any = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    self._check_if_valid_between_or_within(ref2, ref_value)\n    self._ref = ref\n    self._ref2 = ref2\n    self._ref_value = ref_value\n    self.name = name\n    self._factual_selections: _OptionalSelections = None\n    self._target_selections: _OptionalSelections = None\n    self._factual_queries: list[str] | None = None\n    self._target_queries: list[str] | None = None\n\n    if (output_processors is not None) and (\n        not isinstance(output_processors, list)\n    ):\n        output_processors = [output_processors]\n\n    self._output_processors: list[OutputProcessor] | None = output_processors\n\n    self._cache_size = cache_size\n    self._setup_caching()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.Constraint.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.Constraint.get_description","title":"get_description","text":"<pre><code>get_description() -&gt; str\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def get_description(self) -&gt; str:\n    if self.name is not None:\n        return self.name\n    if self._ref2 is None:\n        data_source_string = str(self._ref.data_source)\n    else:\n        data_source1_string = str(self._ref.data_source)\n        data_source2_string = str(self._ref2.data_source)\n\n        data_source1_substring, data_source2_substring = _uncommon_substrings(\n            data_source1_string, data_source2_string\n        )\n        data_source_string = f\"{data_source1_substring} | {data_source2_substring}\"\n    return self.__class__.__name__ + \"::\" + data_source_string\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.Constraint.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    # ty can't figure out that this is a method and that self is passed\n    # as the first argument.\n    value_factual = self._get_factual_value(engine=engine)  # type: ignore[missing-argument]\n    # ty can't figure out that this is a method and that self is passed\n    # as the first argument.\n    value_target = self._get_target_value(engine=engine)  # type: ignore[missing-argument]\n    is_success, assertion_message = self._compare(value_factual, value_target)\n\n    if is_success:\n        return TestResult.success()\n\n    factual_queries = None\n    if self._factual_selections:\n        factual_queries = [\n            str(\n                factual_selection.compile(\n                    engine, compile_kwargs={\"literal_binds\": True}\n                )\n            )\n            for factual_selection in self._factual_selections\n        ]\n    target_queries = None\n    if self._target_selections:\n        target_queries = [\n            str(\n                target_selection.compile(\n                    engine, compile_kwargs={\"literal_binds\": True}\n                )\n            )\n            for target_selection in self._target_selections\n        ]\n    return TestResult.failure(\n        assertion_message,\n        self.get_description(),\n        factual_queries,\n        target_queries,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult","title":"TestResult  <code>dataclass</code>","text":"<pre><code>TestResult(outcome: bool, _failure_message: str | None = None, _constraint_description: str | None = None, _factual_queries: str | None = None, _target_queries: str | None = None)\n</code></pre> <p>The result of the execution of a Constraint.</p>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.constraint_description","title":"constraint_description  <code>property</code>","text":"<pre><code>constraint_description: str | None\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.failure_message","title":"failure_message  <code>property</code>","text":"<pre><code>failure_message: str | None\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.logging_message","title":"logging_message  <code>property</code>","text":"<pre><code>logging_message\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.outcome","title":"outcome  <code>instance-attribute</code>","text":"<pre><code>outcome: bool\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.failure","title":"failure  <code>classmethod</code>","text":"<pre><code>failure(*args, **kwargs)\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>@classmethod\ndef failure(cls, *args, **kwargs):\n    return cls(False, *args, **kwargs)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.formatted_constraint_description","title":"formatted_constraint_description","text":"<pre><code>formatted_constraint_description(formatter: Formatter) -&gt; str | None\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def formatted_constraint_description(self, formatter: Formatter) -&gt; str | None:\n    return (\n        formatter.fmt_str(self._constraint_description)\n        if self._constraint_description\n        else None\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.formatted_failure_message","title":"formatted_failure_message","text":"<pre><code>formatted_failure_message(formatter: Formatter) -&gt; str | None\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def formatted_failure_message(self, formatter: Formatter) -&gt; str | None:\n    return (\n        formatter.fmt_str(self._failure_message) if self._failure_message else None\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.base.TestResult.success","title":"success  <code>classmethod</code>","text":"<pre><code>success()\n</code></pre> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>@classmethod\ndef success(cls):\n    return cls(True)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.column","title":"column","text":""},{"location":"api-documentation/#datajudge.constraints.column.Column","title":"Column","text":"<pre><code>Column(ref: DataReference, *, ref2: DataReference | None = None, ref_value: Any = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code>, <code>ABC</code></p> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    ref_value: Any = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    self._check_if_valid_between_or_within(ref2, ref_value)\n    self._ref = ref\n    self._ref2 = ref2\n    self._ref_value = ref_value\n    self.name = name\n    self._factual_selections: _OptionalSelections = None\n    self._target_selections: _OptionalSelections = None\n    self._factual_queries: list[str] | None = None\n    self._target_queries: list[str] | None = None\n\n    if (output_processors is not None) and (\n        not isinstance(output_processors, list)\n    ):\n        output_processors = [output_processors]\n\n    self._output_processors: list[OutputProcessor] | None = output_processors\n\n    self._cache_size = cache_size\n    self._setup_caching()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.column.ColumnExistence","title":"ColumnExistence","text":"<pre><code>ColumnExistence(ref: DataReference, columns: list[str], name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Column</code></p> Source code in <code>src/datajudge/constraints/column.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    columns: list[str],\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref_value=columns, name=name, cache_size=cache_size)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.column.ColumnSubset","title":"ColumnSubset","text":"<pre><code>ColumnSubset(ref: DataReference, *, ref2: DataReference | None = None, ref_value: Any = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>               Bases: <code>Column</code></p> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    ref_value: Any = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    self._check_if_valid_between_or_within(ref2, ref_value)\n    self._ref = ref\n    self._ref2 = ref2\n    self._ref_value = ref_value\n    self.name = name\n    self._factual_selections: _OptionalSelections = None\n    self._target_selections: _OptionalSelections = None\n    self._factual_queries: list[str] | None = None\n    self._target_queries: list[str] | None = None\n\n    if (output_processors is not None) and (\n        not isinstance(output_processors, list)\n    ):\n        output_processors = [output_processors]\n\n    self._output_processors: list[OutputProcessor] | None = output_processors\n\n    self._cache_size = cache_size\n    self._setup_caching()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.column.ColumnSuperset","title":"ColumnSuperset","text":"<pre><code>ColumnSuperset(ref: DataReference, *, ref2: DataReference | None = None, ref_value: Any = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>               Bases: <code>Column</code></p> Source code in <code>src/datajudge/constraints/base.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    ref_value: Any = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    self._check_if_valid_between_or_within(ref2, ref_value)\n    self._ref = ref\n    self._ref2 = ref2\n    self._ref_value = ref_value\n    self.name = name\n    self._factual_selections: _OptionalSelections = None\n    self._target_selections: _OptionalSelections = None\n    self._factual_queries: list[str] | None = None\n    self._target_queries: list[str] | None = None\n\n    if (output_processors is not None) and (\n        not isinstance(output_processors, list)\n    ):\n        output_processors = [output_processors]\n\n    self._output_processors: list[OutputProcessor] | None = output_processors\n\n    self._cache_size = cache_size\n    self._setup_caching()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.column.ColumnType","title":"ColumnType","text":"<pre><code>ColumnType(ref: DataReference, *, ref2: DataReference | None = None, column_type: str | TypeEngine | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> <p>A class used to represent a ColumnType constraint.</p> <p>This class enables flexible specification of column types either in string format or using SQLAlchemy's type hierarchy. It checks whether a column's type matches the specified type, allowing for checks against backend-specific types, SQLAlchemy's generic types, or string representations of backend-specific types.</p> <p>When using SQLAlchemy's generic types, the comparison is done using <code>isinstance</code>, which means that the actual type can also be a subclass of the target type. For more information, see https://docs.sqlalchemy.org/en/20/core/type_basics.html</p> Source code in <code>src/datajudge/constraints/column.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    column_type: str | sa.types.TypeEngine | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=column_type,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date","title":"date","text":""},{"location":"api-documentation/#datajudge.constraints.date.Date","title":"Date  <code>module-attribute</code>","text":"<pre><code>Date = str | date | datetime\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date.DateBetween","title":"DateBetween","text":"<pre><code>DateBetween(ref: DataReference, min_fraction: float, lower_bound: str, upper_bound: str, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/date.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    min_fraction: float,\n    lower_bound: str,\n    upper_bound: str,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref_value=min_fraction, name=name, cache_size=cache_size)\n    self._lower_bound = lower_bound\n    self._upper_bound = upper_bound\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date.DateMax","title":"DateMax","text":"<pre><code>DateMax(ref: DataReference, use_upper_bound_reference: bool, column_type: str, name: str | None = None, cache_size=None, *, ref2: DataReference | None = None, max_value: str | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/date.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    use_upper_bound_reference: bool,\n    column_type: str,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    ref2: DataReference | None = None,\n    max_value: str | None = None,\n):\n    self._format = _get_format_from_column_type(column_type)\n    self._use_upper_bound_reference = use_upper_bound_reference\n    max_date: dt.date | None = None\n    if max_value is not None:\n        max_date = dt.datetime.strptime(max_value, _INPUT_DATE_FORMAT).date()\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=max_date,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date.DateMin","title":"DateMin","text":"<pre><code>DateMin(ref: DataReference, use_lower_bound_reference: bool, column_type: str, name: str | None = None, cache_size=None, *, ref2: DataReference | None = None, min_value: str | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/date.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    use_lower_bound_reference: bool,\n    column_type: str,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    ref2: DataReference | None = None,\n    min_value: str | None = None,\n):\n    self._format = _get_format_from_column_type(column_type)\n    self._use_lower_bound_reference = use_lower_bound_reference\n    min_date: dt.date | None = None\n    if min_value is not None:\n        min_date = dt.datetime.strptime(min_value, _INPUT_DATE_FORMAT).date()\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=min_date,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date.DateNoGap","title":"DateNoGap","text":"<pre><code>DateNoGap(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, legitimate_gap_size: float, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NoGapConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    legitimate_gap_size: float,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._legitimate_gap_size = legitimate_gap_size\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date.DateNoOverlap","title":"DateNoOverlap","text":"<pre><code>DateNoOverlap(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, end_included: bool, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NoOverlapConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    end_included: bool,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._end_included = end_included\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.date.DateNoOverlap2d","title":"DateNoOverlap2d","text":"<pre><code>DateNoOverlap2d(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, end_included: bool, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NoOverlapConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    end_included: bool,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._end_included = end_included\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.groupby","title":"groupby","text":""},{"location":"api-documentation/#datajudge.constraints.groupby.AggregateNumericRangeEquality","title":"AggregateNumericRangeEquality","text":"<pre><code>AggregateNumericRangeEquality(ref: DataReference, aggregation_column: str, start_value: int = 0, name: str | None = None, cache_size=None, *, tolerance: float = 0, ref2: DataReference | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/groupby.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    aggregation_column: str,\n    start_value: int = 0,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    tolerance: float = 0,\n    ref2: DataReference | None = None,\n):\n    super().__init__(ref, ref2=ref2, ref_value=object(), name=name)\n    self._aggregation_column = aggregation_column\n    self._tolerance = tolerance\n    self._start_value = start_value\n    self._selection = None\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.interval","title":"interval","text":""},{"location":"api-documentation/#datajudge.constraints.interval.IntervalConstraint","title":"IntervalConstraint","text":"<pre><code>IntervalConstraint(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref_value=object(), name=name)\n    self._key_columns = key_columns\n    self._start_columns = start_columns\n    self._end_columns = end_columns\n    self._max_relative_n_violations = max_relative_n_violations\n    self._validate_dimensions()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.interval.NoGapConstraint","title":"NoGapConstraint","text":"<pre><code>NoGapConstraint(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, legitimate_gap_size: float, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>IntervalConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    legitimate_gap_size: float,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._legitimate_gap_size = legitimate_gap_size\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.interval.NoOverlapConstraint","title":"NoOverlapConstraint","text":"<pre><code>NoOverlapConstraint(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, end_included: bool, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>IntervalConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    end_included: bool,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._end_included = end_included\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs","title":"miscs","text":""},{"location":"api-documentation/#datajudge.constraints.miscs.FunctionalDependency","title":"FunctionalDependency","text":"<pre><code>FunctionalDependency(ref: DataReference, key_columns: list[str], **kwargs)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/miscs.py</code> <pre><code>def __init__(self, ref: DataReference, key_columns: list[str], **kwargs):\n    super().__init__(ref, ref_value=object(), **kwargs)\n    self.key_columns = key_columns\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.FunctionalDependency.key_columns","title":"key_columns  <code>instance-attribute</code>","text":"<pre><code>key_columns = key_columns\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.FunctionalDependency.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/miscs.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    violations, _ = db_access.get_functional_dependency_violations(\n        engine, self._ref, self.key_columns\n    )\n    if not violations:\n        return TestResult.success()\n\n    assertion_text = (\n        f\"{self._ref} has violations of functional dependence (in total {len(violations)} rows):\\n\"\n        + \"\\n\".join(\n            [\n                f\"{violation}\"\n                for violation in self._apply_output_formatting(\n                    [tuple(elem) for elem in violations]\n                )\n            ]\n        )\n    )\n    return TestResult.failure(assertion_text)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.MaxNullFraction","title":"MaxNullFraction","text":"<pre><code>MaxNullFraction(ref, *, ref2: DataReference | None = None, max_null_fraction: float | None = None, max_relative_deviation: float = 0, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/miscs.py</code> <pre><code>def __init__(\n    self,\n    ref,\n    *,\n    ref2: DataReference | None = None,\n    max_null_fraction: float | None = None,\n    max_relative_deviation: float = 0,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=max_null_fraction,\n        name=name,\n        cache_size=cache_size,\n    )\n    if max_null_fraction is not None and not (0 &lt;= max_null_fraction &lt;= 1):\n        raise ValueError(\n            f\"max_null_fraction was expected to lie within [0, 1] but is \"\n            f\"{max_null_fraction}.\"\n        )\n    if max_relative_deviation &lt; 0:\n        raise ValueError(\n            f\"{max_relative_deviation} is negative even though it needs to be positive.\"\n        )\n    self.max_relative_deviation = max_relative_deviation\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.MaxNullFraction.max_relative_deviation","title":"max_relative_deviation  <code>instance-attribute</code>","text":"<pre><code>max_relative_deviation = max_relative_deviation\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.PrimaryKeyDefinition","title":"PrimaryKeyDefinition","text":"<pre><code>PrimaryKeyDefinition(ref, primary_keys: list[str], name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/miscs.py</code> <pre><code>def __init__(\n    self,\n    ref,\n    primary_keys: list[str],\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref_value=set(primary_keys), name=name)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.Uniqueness","title":"Uniqueness","text":"<pre><code>Uniqueness(ref: DataReference, max_duplicate_fraction: float = 0, max_absolute_n_duplicates: int = 0, infer_pk_columns: bool = False, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/miscs.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    max_duplicate_fraction: float = 0,\n    max_absolute_n_duplicates: int = 0,\n    infer_pk_columns: bool = False,\n    name: str | None = None,\n    cache_size=None,\n):\n    if max_duplicate_fraction != 0 and max_absolute_n_duplicates != 0:\n        raise ValueError(\n            \"\"\"Uniqueness constraint was attempted to be constructed\n            with both a relative and an absolute tolerance. Only use one\n            of both at a time.\"\"\"\n        )\n    if max_duplicate_fraction != 0:\n        ref_value = (\"relative\", max_duplicate_fraction)\n    elif max_absolute_n_duplicates != 0:\n        ref_value = (\"absolute\", max_absolute_n_duplicates)\n    else:\n        ref_value = (\"relative\", 0)\n\n    self.infer_pk_columns = infer_pk_columns\n    super().__init__(ref, ref_value=ref_value, name=name, cache_size=cache_size)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.Uniqueness.infer_pk_columns","title":"infer_pk_columns  <code>instance-attribute</code>","text":"<pre><code>infer_pk_columns = infer_pk_columns\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.miscs.Uniqueness.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/miscs.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    if self.infer_pk_columns and db_access.is_bigquery(engine):\n        raise NotImplementedError(\"No primary key concept in BigQuery\")\n\n    # only check for primary keys when actually defined\n    # otherwise default back to searching the whole table\n    if self.infer_pk_columns and (\n        pk_columns := db_access.get_primary_keys(engine, self._ref)[0]\n    ):\n        self._ref.columns = pk_columns\n        if not pk_columns:  # there were no primary keys found\n            warnings.warn(\n                f\"\"\"No primary keys found in {self._ref}.\n                Uniqueness will be tested for all columns.\"\"\"\n            )\n\n    unique_count, unique_selections = db_access.get_unique_count(engine, self._ref)\n    row_count, row_selections = db_access.get_row_count(engine, self._ref)\n    self.factual_selections = row_selections\n    self.target_selections = unique_selections\n    if row_count == 0:\n        return TestResult(True, \"No occurrences.\")\n\n    tolerance_kind, tolerance_value = self._ref_value\n\n    if tolerance_kind == \"relative\":\n        result = unique_count &gt;= row_count * (1 - tolerance_value)\n    elif tolerance_kind == \"absolute\":\n        result = unique_count &gt;= row_count - tolerance_value\n    else:\n        raise ValueError(\n            \"Given tolerance is neither relative nor absolute: {tolerance_kind}.\"\n        )\n    if result:\n        return TestResult.success()\n    sample, _ = db_access.get_duplicate_sample(engine, self._ref)\n    sample_string = _format_sample(sample, self._ref)\n    assertion_text = (\n        f\"{self._ref} has {row_count} rows &gt; {unique_count} \"\n        f\"uniques. This surpasses the max_duplicate_fraction of \"\n        f\"{self._ref_value}. An example tuple breaking the \"\n        f\"uniqueness condition is: {sample_string}.\"\n    )\n    return TestResult.failure(assertion_text)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows","title":"nrows","text":""},{"location":"api-documentation/#datajudge.constraints.nrows.NRows","title":"NRows","text":"<pre><code>NRows(ref: DataReference, *, ref2: DataReference | None = None, n_rows: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code>, <code>ABC</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    n_rows: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=n_rows,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsEquality","title":"NRowsEquality","text":"<pre><code>NRowsEquality(ref: DataReference, *, ref2: DataReference | None = None, n_rows: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NRows</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    n_rows: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=n_rows,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMax","title":"NRowsMax","text":"<pre><code>NRowsMax(ref: DataReference, *, ref2: DataReference | None = None, n_rows: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NRows</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    n_rows: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=n_rows,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMaxGain","title":"NRowsMaxGain","text":"<pre><code>NRowsMaxGain(ref: DataReference, ref2: DataReference, max_relative_gain_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NRows</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_relative_gain_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._max_relative_gain_getter = max_relative_gain_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMaxGain.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    self._max_relative_gain = self._max_relative_gain_getter(engine)\n    return super().test(engine)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMaxLoss","title":"NRowsMaxLoss","text":"<pre><code>NRowsMaxLoss(ref: DataReference, ref2: DataReference, max_relative_loss_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NRows</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_relative_loss_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self.max_relative_loss_getter = max_relative_loss_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMaxLoss.max_relative_loss_getter","title":"max_relative_loss_getter  <code>instance-attribute</code>","text":"<pre><code>max_relative_loss_getter = max_relative_loss_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMaxLoss.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    self.max_relative_loss = self.max_relative_loss_getter(engine)\n    return super().test(engine)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMin","title":"NRowsMin","text":"<pre><code>NRowsMin(ref: DataReference, *, ref2: DataReference | None = None, n_rows: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NRows</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    n_rows: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=n_rows,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMinGain","title":"NRowsMinGain","text":"<pre><code>NRowsMinGain(ref: DataReference, ref2: DataReference, min_relative_gain_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NRows</code></p> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    min_relative_gain_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._min_relative_gain_getter = min_relative_gain_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.nrows.NRowsMinGain.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/nrows.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    self._min_relative_gain = self._min_relative_gain_getter(engine)\n    return super().test(engine)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric","title":"numeric","text":""},{"location":"api-documentation/#datajudge.constraints.numeric.NumericBetween","title":"NumericBetween","text":"<pre><code>NumericBetween(ref: DataReference, min_fraction: float, lower_bound: float, upper_bound: float, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/numeric.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    min_fraction: float,\n    lower_bound: float,\n    upper_bound: float,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref_value=min_fraction, name=name, cache_size=cache_size)\n    self._lower_bound = lower_bound\n    self._upper_bound = upper_bound\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericMax","title":"NumericMax","text":"<pre><code>NumericMax(ref: DataReference, name: str | None = None, cache_size=None, *, ref2: DataReference | None = None, max_value: float | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/numeric.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    ref2: DataReference | None = None,\n    max_value: float | None = None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=max_value,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericMean","title":"NumericMean","text":"<pre><code>NumericMean(ref: DataReference, max_absolute_deviation: float, name: str | None = None, cache_size=None, *, ref2: DataReference | None = None, mean_value: float | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/numeric.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    max_absolute_deviation: float,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    ref2: DataReference | None = None,\n    mean_value: float | None = None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=mean_value,\n        name=name,\n        cache_size=cache_size,\n    )\n    self._max_absolute_deviation = max_absolute_deviation\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericMean.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/numeric.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    # ty can't figure out that this is a method and that self is passed\n    # as the first argument.\n    mean_factual = self._get_factual_value(engine=engine)  # type: ignore[missing-argument]\n    # ty can't figure out that this is a method and that self is passed\n    # as the first argument.\n    mean_target = self._get_target_value(engine=engine)  # type: ignore[missing-argument]\n\n    if mean_factual is None or mean_target is None:\n        return TestResult(\n            mean_factual is None and mean_target is None,\n            \"Mean over empty set.\",\n        )\n    deviation = abs(mean_factual - mean_target)\n    assertion_text = (\n        f\"{self._ref} \"\n        f\"has mean {mean_factual}, deviating more than \"\n        f\"{self._max_absolute_deviation} from \"\n        f\"{self._target_prefix} {mean_target}. \"\n        f\"{self._condition_string}\"\n    )\n    result = deviation &lt;= self._max_absolute_deviation\n    return TestResult(result, assertion_text)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericMin","title":"NumericMin","text":"<pre><code>NumericMin(ref: DataReference, name: str | None = None, cache_size=None, *, ref2: DataReference | None = None, min_value: float | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/numeric.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    ref2: DataReference | None = None,\n    min_value: float | None = None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=min_value,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericNoGap","title":"NumericNoGap","text":"<pre><code>NumericNoGap(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, legitimate_gap_size: float, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NoGapConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    legitimate_gap_size: float,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._legitimate_gap_size = legitimate_gap_size\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericNoOverlap","title":"NumericNoOverlap","text":"<pre><code>NumericNoOverlap(ref: DataReference, key_columns: list[str] | None, start_columns: list[str], end_columns: list[str], max_relative_n_violations: float, end_included: bool, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NoOverlapConstraint</code></p> Source code in <code>src/datajudge/constraints/interval.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    key_columns: list[str] | None,\n    start_columns: list[str],\n    end_columns: list[str],\n    max_relative_n_violations: float,\n    end_included: bool,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._end_included = end_included\n    super().__init__(\n        ref,\n        key_columns,\n        start_columns,\n        end_columns,\n        max_relative_n_violations,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericPercentile","title":"NumericPercentile","text":"<pre><code>NumericPercentile(ref: DataReference, percentage: float, max_absolute_deviation: float | None = None, max_relative_deviation: float | None = None, name: str | None = None, cache_size=None, *, ref2: DataReference | None = None, expected_percentile: float | None = None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/numeric.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    percentage: float,\n    max_absolute_deviation: float | None = None,\n    max_relative_deviation: float | None = None,\n    name: str | None = None,\n    cache_size=None,\n    *,\n    ref2: DataReference | None = None,\n    expected_percentile: float | None = None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=expected_percentile,\n        name=name,\n        cache_size=cache_size,\n    )\n    if not (0 &lt;= percentage &lt;= 100):\n        raise ValueError(\n            f\"Expected percentage to be a value between 0 and 100, got {percentage}.\"\n        )\n    self.percentage = percentage\n    if max_absolute_deviation is None and max_relative_deviation is None:\n        raise ValueError(\n            \"At least one of 'max_absolute_deviation' and 'max_relative_deviation' \"\n            \"must be given.\"\n        )\n    if max_absolute_deviation is not None and max_absolute_deviation &lt; 0:\n        raise ValueError(\n            f\"max_absolute_deviation must be at least 0 but is {max_absolute_deviation}.\"\n        )\n    if max_relative_deviation is not None and max_relative_deviation &lt; 0:\n        raise ValueError(\n            f\"max_relative_deviation must be at least 0 but is {max_relative_deviation}.\"\n        )\n    self._max_absolute_deviation = max_absolute_deviation\n    self._max_relative_deviation = max_relative_deviation\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.numeric.NumericPercentile.percentage","title":"percentage  <code>instance-attribute</code>","text":"<pre><code>percentage = percentage\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row","title":"row","text":""},{"location":"api-documentation/#datajudge.constraints.row.Row","title":"Row","text":"<pre><code>Row(ref: DataReference, ref2: DataReference, max_missing_fraction_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code>, <code>ABC</code></p> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_missing_fraction_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._max_missing_fraction_getter = max_missing_fraction_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row.Row.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    if self._ref is None or self._ref2 is None:\n        raise ValueError()\n    self._max_missing_fraction = self._max_missing_fraction_getter(engine)\n    self._ref1_minus_ref2_sample, _ = db_access.get_row_difference_sample(\n        engine, self._ref, self._ref2\n    )\n    self._ref2_minus_ref1_sample, _ = db_access.get_row_difference_sample(\n        engine, self._ref2, self._ref\n    )\n    return super().test(engine)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row.RowEquality","title":"RowEquality","text":"<pre><code>RowEquality(ref: DataReference, ref2: DataReference, max_missing_fraction_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Row</code></p> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_missing_fraction_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._max_missing_fraction_getter = max_missing_fraction_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row.RowMatchingEquality","title":"RowMatchingEquality","text":"<pre><code>RowMatchingEquality(ref: DataReference, ref2: DataReference, matching_columns1: list[str], matching_columns2: list[str], comparison_columns1: list[str], comparison_columns2: list[str], max_missing_fraction_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Row</code></p> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    matching_columns1: list[str],\n    matching_columns2: list[str],\n    comparison_columns1: list[str],\n    comparison_columns2: list[str],\n    max_missing_fraction_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        max_missing_fraction_getter=max_missing_fraction_getter,\n        name=name,\n        cache_size=cache_size,\n    )\n    self._match_and_compare = _MatchAndCompare(\n        matching_columns1,\n        matching_columns2,\n        comparison_columns1,\n        comparison_columns2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row.RowMatchingEquality.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    if self._ref is None or self._ref2 is None:\n        raise ValueError()\n    missing_fraction, n_rows_match, selections = db_access.get_row_mismatch(\n        engine, self._ref, self._ref2, self._match_and_compare\n    )\n    self.factual_selections = selections\n    max_missing_fraction = self._max_missing_fraction_getter(engine)\n    result = missing_fraction &lt;= max_missing_fraction\n    if result:\n        return TestResult.success()\n    assertion_message = (\n        f\"{missing_fraction} &gt; \"\n        f\"{max_missing_fraction} of the rows differ \"\n        f\"on a match of {n_rows_match} rows between {self._ref} and \"\n        f\"{self._ref2}. \"\n        f\"{self._condition_string}\"\n        f\"{self._match_and_compare} \"\n    )\n    return TestResult.failure(assertion_message)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row.RowSubset","title":"RowSubset","text":"<pre><code>RowSubset(ref: DataReference, ref2: DataReference, max_missing_fraction_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Row</code></p> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_missing_fraction_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._max_missing_fraction_getter = max_missing_fraction_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.row.RowSuperset","title":"RowSuperset","text":"<pre><code>RowSuperset(ref: DataReference, ref2: DataReference, max_missing_fraction_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Row</code></p> Source code in <code>src/datajudge/constraints/row.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_missing_fraction_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._max_missing_fraction_getter = max_missing_fraction_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.stats","title":"stats","text":""},{"location":"api-documentation/#datajudge.constraints.stats.KolmogorovSmirnov2Sample","title":"KolmogorovSmirnov2Sample","text":"<pre><code>KolmogorovSmirnov2Sample(ref: DataReference, ref2: DataReference, significance_level: float = 0.05, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/stats.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    significance_level: float = 0.05,\n    name: str | None = None,\n    cache_size=None,\n):\n    self._significance_level = significance_level\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.stats.KolmogorovSmirnov2Sample.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/stats.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    if self._ref2 is None:\n        raise ValueError(\"KolmogorovSmirnov2Sample requires ref2.\")\n    (\n        d_statistic,\n        p_value,\n        n_samples,\n        m_samples,\n        selections,\n    ) = self._calculate_statistic(\n        engine,\n        self._ref,\n        self._ref2,\n    )\n    result = self._check_acceptance(\n        d_statistic, n_samples, m_samples, self._significance_level\n    )\n\n    assertion_text = (\n        f\"Null hypothesis (H0) for the 2-sample Kolmogorov-Smirnov test was rejected, i.e., \"\n        f\"the two samples ({self._ref} and {self._target_prefix}) \"\n        f\"do not originate from the same distribution. \"\n        f\"The test results are d={d_statistic}\"\n    )\n    if p_value is not None:\n        assertion_text += f\" and {p_value=}\"\n    assertion_text += \".\"\n\n    if selections:\n        queries = [\n            str(selection.compile(engine, compile_kwargs={\"literal_binds\": True}))\n            for selection in selections\n        ]\n\n    if not result:\n        return TestResult.failure(\n            assertion_text,\n            self.get_description(),\n            queries,\n        )\n\n    return TestResult.success()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques","title":"uniques","text":""},{"location":"api-documentation/#datajudge.constraints.uniques.CategoricalBoundConstraint","title":"CategoricalBoundConstraint","text":"<pre><code>CategoricalBoundConstraint(ref: DataReference, distribution: dict[_T, tuple[float, float]], default_bounds: tuple[float, float] = (0, 0), name: str | None = None, cache_size=None, max_relative_violations: float = 0, **kwargs)\n</code></pre> <p>               Bases: <code>Constraint</code></p> <p>Constraint that checks if the share of specific values in a column falls within predefined bounds.</p> <p>It compares the actual distribution of values in a <code>DataSource</code> column with a target distribution, supplied as a dictionary.</p> <p>Example use cases include testing for consistency in columns with expected categorical values or ensuring that the distribution of values in a column adheres to a certain criterion.</p> PARAMETER DESCRIPTION <code>ref</code> <p>A reference to the column in the data source.</p> <p> TYPE: <code>DataReference</code> </p> <code>distribution</code> <p>A dictionary with unique values as keys and tuples of minimum and maximum allowed shares as values.</p> <p> TYPE: <code>dict[_T, tuple[float, float]]</code> </p> <code>default_bounds</code> <p>A tuple specifying the minimum and maximum bounds for values not explicitly outlined in the target distribution dictionary.</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0, 0)</code> </p> <code>name</code> <p>An optional name for the constraint.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>max_relative_violations</code> <p>A tolerance threshold (0 to 1) for the proportion of elements in the data that can violate the bound constraints without triggering the constraint violation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    distribution: dict[_T, tuple[float, float]],\n    default_bounds: tuple[float, float] = (0, 0),\n    name: str | None = None,\n    cache_size=None,\n    max_relative_violations: float = 0,\n    **kwargs,\n):\n    self._default_bounds = default_bounds\n    self._max_relative_violations = max_relative_violations\n    super().__init__(\n        ref,\n        ref_value=distribution,\n        name=name,\n        cache_size=cache_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniques","title":"NUniques","text":"<pre><code>NUniques(ref: DataReference, *, ref2: DataReference | None = None, n_uniques: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code>, <code>ABC</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    n_uniques: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=n_uniques,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniquesEquality","title":"NUniquesEquality","text":"<pre><code>NUniquesEquality(ref: DataReference, *, ref2: DataReference | None = None, n_uniques: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NUniques</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    n_uniques: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=n_uniques,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniquesMaxGain","title":"NUniquesMaxGain","text":"<pre><code>NUniquesMaxGain(ref: DataReference, ref2: DataReference, max_relative_gain_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NUniques</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_relative_gain_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self._max_relative_gain_getter = max_relative_gain_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniquesMaxGain.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    self._max_relative_gain = self._max_relative_gain_getter(engine)\n    return super().test(engine)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniquesMaxLoss","title":"NUniquesMaxLoss","text":"<pre><code>NUniquesMaxLoss(ref: DataReference, ref2: DataReference, max_relative_loss_getter: _ToleranceGetter, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>NUniques</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    ref2: DataReference,\n    max_relative_loss_getter: _ToleranceGetter,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref2=ref2, name=name, cache_size=cache_size)\n    self.max_relative_loss_getter = max_relative_loss_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniquesMaxLoss.max_relative_loss_getter","title":"max_relative_loss_getter  <code>instance-attribute</code>","text":"<pre><code>max_relative_loss_getter = max_relative_loss_getter\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.NUniquesMaxLoss.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    self.max_relative_loss = self.max_relative_loss_getter(engine)\n    return super().test(engine)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.Uniques","title":"Uniques","text":"<pre><code>Uniques(ref: DataReference, name: str | None = None, cache_size=None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, *, ref2: DataReference | None = None, uniques: Collection | None = None, filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, max_relative_violations=0, compare_distinct=False)\n</code></pre> <p>               Bases: <code>Constraint</code>, <code>ABC</code></p> <p>Uniques is an abstract class for comparisons between unique values of a column and a reference.</p> <p>The <code>Uniques</code> constraint asserts if the values contained in a column of a <code>DataSource</code> are part of a reference set of expected values - either externally supplied through parameter <code>uniques</code> or obtained from another <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the add_null_absence_constraint helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as filternull_element, filternull_never, filternull_element_or_tuple_all, filternull_element_or_tuple_any. Passing <code>None</code> as the argument is equivalent to filternull_element but triggers a warning. The current default of filternull_element Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to filternull_element_or_tuple_all in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p>There are two ways to do some post processing of the data obtained from the database by providing a function to be executed. In general, no postprocessing is needed, but there are some cases where it's the only thing to do. For example, with text values that have some structure.</p> <p>One is <code>map_func</code>, it'll be executed over each obtained 'unique' value. This is a very local operation.</p> <p>If <code>map_func</code> is provided, it'll be executed over each obtained 'unique' value.</p> <p>The second one is <code>reduce_func</code> which will take the whole data retrieved and can perform global processing. If it is provided, it gets applied after the function given in <code>map_func</code> is finished. The output of this function has to be an iterable (eager or lazy) of the same type as the type of the values of the column (in their Python equivalent).</p> <p>Furthermore, the <code>max_relative_violations</code> parameter can be used to set a tolerance threshold for the proportion of elements in the data that can violate the constraint (default: 0). Setting this argument is currently not supported for <code>UniquesEquality</code>.</p> <p>For <code>UniquesSubset</code>, by default, the number of occurrences affects the computed fraction of violations. To disable this weighting, set <code>compare_distinct=True</code>. This argument does not have an effect on the test results for other <code>Uniques</code> constraints, or if <code>max_relative_violations</code> is 0.</p> <p>By default, the assertion messages make use of sets, thus, they may differ from run to run despite the exact same situation being present, and can have an arbitrary length. To enforce a reproducible, limited output via (e.g.) sorting and slicing, set <code>output_processors</code> to a callable or a list of callables. By default, only the first 100 elements are displayed (output_processor_limit).</p> <p>Each callable takes in two collections, and returns modified (e.g. sorted) versions of them. In most cases, the second argument is simply None, but for <code>UniquesSubset</code> it is the counts of each of the elements. The suggested functions are output_processor_sort and output_processor_limit - see their respective docstrings for details.</p> <p>One use is of this constraint is to test for consistency in columns with expected categorical values.</p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    name: str | None = None,\n    cache_size=None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    *,\n    ref2: DataReference | None = None,\n    uniques: Collection | None = None,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    max_relative_violations=0,\n    compare_distinct=False,\n):\n    ref_value: tuple[Collection, list] | None\n    ref_value = (uniques, []) if uniques else None\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=ref_value,\n        name=name,\n        cache_size=cache_size,\n        output_processors=output_processors,\n    )\n\n    if filter_func is None:\n        warnings.warn(\"Using deprecated default null filter function.\")\n        filter_func = filternull_element\n\n    self._filter_func = filter_func\n    self._local_func = map_func\n    self._global_func = reduce_func\n    self._max_relative_violations = max_relative_violations\n    self._compare_distinct = compare_distinct\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.UniquesEquality","title":"UniquesEquality","text":"<pre><code>UniquesEquality(args, name: str | None = None, cache_size=None, **kwargs)\n</code></pre> <p>               Bases: <code>Uniques</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(self, args, name: str | None = None, cache_size=None, **kwargs):\n    if kwargs.get(\"max_relative_violations\"):\n        raise RuntimeError(\n            \"max_relative_violations is not supported for UniquesEquality.\"\n        )\n    if kwargs.get(\"compare_distinct\"):\n        raise RuntimeError(\"compare_distinct is not supported for UniquesEquality.\")\n    super().__init__(args, name=name, **kwargs)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.UniquesSubset","title":"UniquesSubset","text":"<pre><code>UniquesSubset(ref: DataReference, name: str | None = None, cache_size=None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, *, ref2: DataReference | None = None, uniques: Collection | None = None, filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, max_relative_violations=0, compare_distinct=False)\n</code></pre> <p>               Bases: <code>Uniques</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    name: str | None = None,\n    cache_size=None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    *,\n    ref2: DataReference | None = None,\n    uniques: Collection | None = None,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    max_relative_violations=0,\n    compare_distinct=False,\n):\n    ref_value: tuple[Collection, list] | None\n    ref_value = (uniques, []) if uniques else None\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=ref_value,\n        name=name,\n        cache_size=cache_size,\n        output_processors=output_processors,\n    )\n\n    if filter_func is None:\n        warnings.warn(\"Using deprecated default null filter function.\")\n        filter_func = filternull_element\n\n    self._filter_func = filter_func\n    self._local_func = map_func\n    self._global_func = reduce_func\n    self._max_relative_violations = max_relative_violations\n    self._compare_distinct = compare_distinct\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.uniques.UniquesSuperset","title":"UniquesSuperset","text":"<pre><code>UniquesSuperset(args, name: str | None = None, cache_size=None, **kwargs)\n</code></pre> <p>               Bases: <code>Uniques</code></p> Source code in <code>src/datajudge/constraints/uniques.py</code> <pre><code>def __init__(self, args, name: str | None = None, cache_size=None, **kwargs):\n    if kwargs.get(\"compare_distinct\"):\n        raise RuntimeError(\"compare_distinct is not supported for UniquesSuperset.\")\n    super().__init__(args, name=name, **kwargs)\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.varchar","title":"varchar","text":""},{"location":"api-documentation/#datajudge.constraints.varchar.VarCharMaxLength","title":"VarCharMaxLength","text":"<pre><code>VarCharMaxLength(ref: DataReference, *, ref2: DataReference | None = None, max_length: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/varchar.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    *,\n    ref2: DataReference | None = None,\n    max_length: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=max_length,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.varchar.VarCharMinLength","title":"VarCharMinLength","text":"<pre><code>VarCharMinLength(ref, *, ref2: DataReference | None = None, min_length: int | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/varchar.py</code> <pre><code>def __init__(\n    self,\n    ref,\n    *,\n    ref2: DataReference | None = None,\n    min_length: int | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref2=ref2,\n        ref_value=min_length,\n        name=name,\n        cache_size=cache_size,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.varchar.VarCharRegex","title":"VarCharRegex","text":"<pre><code>VarCharRegex(ref: DataReference, regex: str, allow_none: bool = False, relative_tolerance: float = 0.0, aggregated: bool = True, n_counterexamples: int = 5, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/varchar.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    regex: str,\n    allow_none: bool = False,\n    relative_tolerance: float = 0.0,\n    aggregated: bool = True,\n    n_counterexamples: int = 5,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(ref, ref_value=regex, name=name, cache_size=cache_size)\n    self._allow_none = allow_none\n    self._relative_tolerance = relative_tolerance\n    self._aggregated = aggregated\n    self._n_counterexamples = n_counterexamples\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.varchar.VarCharRegex.test","title":"test","text":"<pre><code>test(engine: Engine) -&gt; TestResult\n</code></pre> Source code in <code>src/datajudge/constraints/varchar.py</code> <pre><code>def test(self, engine: sa.engine.Engine) -&gt; TestResult:\n    uniques_counter, selections = db_access.get_uniques(engine, self._ref)\n    self.factual_selections = selections\n    if not self._allow_none and uniques_counter.get(None):\n        return TestResult.failure(\n            \"The column contains a None value when it's not allowed. \"\n            \"To ignore None values, please use `allow_none=True` option.\"\n        )\n    elif None in uniques_counter:\n        uniques_counter.pop(None)\n\n    uniques_factual = list(uniques_counter.keys())\n    if not self._ref_value:\n        return TestResult.failure(\"No regex pattern given\")\n\n    pattern = re.compile(self._ref_value)\n    uniques_mismatching = {x for x in uniques_factual if not pattern.match(x)}\n\n    if self._aggregated:\n        n_violations = len(uniques_mismatching)\n        n_total = len(uniques_factual)\n    else:\n        n_violations = sum(uniques_counter[key] for key in uniques_mismatching)\n        n_total = sum(count for _, count in uniques_counter.items())\n\n    n_relative_violations = n_violations / n_total\n\n    if self._n_counterexamples == -1:\n        counterexamples = list(uniques_mismatching)\n    else:\n        counterexamples = list(\n            itertools.islice(uniques_mismatching, self._n_counterexamples)\n        )\n\n    counterexample_string = (\n        (f\"Some counterexamples consist of the following: {counterexamples}. \")\n        if counterexamples and len(counterexamples) &gt; 0\n        else \"\"\n    )\n\n    if n_relative_violations &gt; self._relative_tolerance:\n        assertion_text = (\n            f\"{self._ref} \"\n            f\"breaks regex '{self._ref_value}' in {n_relative_violations} &gt; \"\n            f\"{self._relative_tolerance} of the cases. \"\n            f\"In absolute terms, {n_violations} of the {n_total} samples violated the regex. \"\n            f\"{counterexample_string}{self._condition_string}\"\n        )\n        return TestResult.failure(assertion_text)\n    return TestResult.success()\n</code></pre>"},{"location":"api-documentation/#datajudge.constraints.varchar.VarCharRegexDb","title":"VarCharRegexDb","text":"<pre><code>VarCharRegexDb(ref: DataReference, regex: str, relative_tolerance: float = 0.0, aggregated: bool = True, n_counterexamples: int = 5, name: str | None = None, cache_size=None)\n</code></pre> <p>               Bases: <code>Constraint</code></p> Source code in <code>src/datajudge/constraints/varchar.py</code> <pre><code>def __init__(\n    self,\n    ref: DataReference,\n    regex: str,\n    relative_tolerance: float = 0.0,\n    aggregated: bool = True,\n    n_counterexamples: int = 5,\n    name: str | None = None,\n    cache_size=None,\n):\n    super().__init__(\n        ref,\n        ref_value=relative_tolerance,\n        name=name,\n        cache_size=cache_size,\n    )\n    self._regex = regex\n    self._aggregated = aggregated\n    self._n_counterexamples = n_counterexamples\n</code></pre>"},{"location":"api-documentation/#datajudge.data_source","title":"data_source","text":""},{"location":"api-documentation/#datajudge.data_source.DataSource","title":"DataSource","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api-documentation/#datajudge.data_source.ExpressionDataSource","title":"ExpressionDataSource","text":"<pre><code>ExpressionDataSource(expression: FromClause | Select, name: str)\n</code></pre> <p>               Bases: <code>DataSource</code></p> <p>A <code>DataSource</code> based on a sqlalchemy expression.</p> Source code in <code>src/datajudge/data_source.py</code> <pre><code>def __init__(\n    self,\n    expression: selectable.FromClause | selectable.Select,\n    name: str,\n):\n    self._expression = expression\n    self.name = name\n</code></pre>"},{"location":"api-documentation/#datajudge.data_source.ExpressionDataSource.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api-documentation/#datajudge.data_source.RawQueryDataSource","title":"RawQueryDataSource","text":"<pre><code>RawQueryDataSource(query_string: str, name: str, columns: list[str] | None = None)\n</code></pre> <p>               Bases: <code>DataSource</code></p> <p>A <code>DataSource</code> based on a SQL query as a string.</p> Source code in <code>src/datajudge/data_source.py</code> <pre><code>def __init__(self, query_string: str, name: str, columns: list[str] | None = None):\n    self._query_string = query_string\n    self.name = name\n    self._columns = columns\n    wrapped_query = f\"({query_string}) as t\"\n    if columns is not None and len(columns) &gt; 0:\n        subquery = (\n            sa.text(query_string)\n            .columns(*[sa.column(column_name) for column_name in columns])\n            .subquery()\n        )\n        self.clause = subquery\n    else:\n        wrapped_query = f\"({query_string}) as t\"\n        self.clause = sa.select(\"*\").select_from(sa.text(wrapped_query)).alias()\n</code></pre>"},{"location":"api-documentation/#datajudge.data_source.RawQueryDataSource.clause","title":"clause  <code>instance-attribute</code>","text":"<pre><code>clause = subquery\n</code></pre>"},{"location":"api-documentation/#datajudge.data_source.RawQueryDataSource.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api-documentation/#datajudge.data_source.TableDataSource","title":"TableDataSource","text":"<pre><code>TableDataSource(db_name: str, table_name: str, schema_name: str | None = None)\n</code></pre> <p>               Bases: <code>DataSource</code></p> <p>A <code>DataSource</code> based on a table.</p> Source code in <code>src/datajudge/data_source.py</code> <pre><code>def __init__(\n    self,\n    db_name: str,\n    table_name: str,\n    schema_name: str | None = None,\n):\n    self._db_name = db_name\n    self._table_name = table_name\n    self._schema_name = schema_name\n</code></pre>"},{"location":"api-documentation/#datajudge.formatter","title":"formatter","text":""},{"location":"api-documentation/#datajudge.formatter.AnsiColorFormatter","title":"AnsiColorFormatter","text":"<pre><code>AnsiColorFormatter()\n</code></pre> <p>               Bases: <code>Formatter</code></p> Source code in <code>src/datajudge/formatter.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    just_fix_windows_console()\n</code></pre>"},{"location":"api-documentation/#datajudge.formatter.Formatter","title":"Formatter","text":"<pre><code>Formatter()\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>src/datajudge/formatter.py</code> <pre><code>def __init__(self):\n    self._known_bb_pattern = re.compile(_STYLING_CODES)\n</code></pre>"},{"location":"api-documentation/#datajudge.formatter.Formatter.fmt_str","title":"fmt_str","text":"<pre><code>fmt_str(string: str) -&gt; str\n</code></pre> Source code in <code>src/datajudge/formatter.py</code> <pre><code>def fmt_str(self, string: str) -&gt; str:\n    # Replace codes with platform specific styling\n    string = self._known_bb_pattern.sub(\n        lambda m: self._apply_formatting(m.group(1), m.group(2)), string\n    )\n\n    return string\n</code></pre>"},{"location":"api-documentation/#datajudge.pytest_integration","title":"pytest_integration","text":""},{"location":"api-documentation/#datajudge.pytest_integration.collect_data_tests","title":"collect_data_tests","text":"<pre><code>collect_data_tests(requirements: Iterable[Requirement])\n</code></pre> <p>Make a Pytest test case that checks all <code>requirements</code>.</p> <p>Returns a function named <code>test_constraint</code> that is parametrized over all constraints in <code>requirements</code>. The function requires a <code>datajudge_engine</code> fixture that is a SQLAlchemy engine to be available.</p> Source code in <code>src/datajudge/pytest_integration.py</code> <pre><code>def collect_data_tests(requirements: Iterable[Requirement]):\n    \"\"\"Make a Pytest test case that checks all `requirements`.\n\n    Returns a function named `test_constraint` that is parametrized over all\n    constraints in `requirements`. The function requires a `datajudge_engine`\n    fixture that is a SQLAlchemy engine to be available.\n    \"\"\"\n    all_constraints = [\n        constraint for requirement in requirements for constraint in requirement\n    ]\n\n    @pytest.mark.parametrize(\n        \"constraint\", all_constraints, ids=Constraint.get_description\n    )\n    def test_constraint(constraint, datajudge_engine, pytestconfig):\n        # apply patches that fix sqlalchemy issues\n        formatter = get_formatter(pytestconfig)\n        apply_patches(datajudge_engine)\n        test_result = constraint.test(datajudge_engine)\n        assert test_result.outcome, test_result.formatted_failure_message(formatter)\n\n    return test_constraint\n</code></pre>"},{"location":"api-documentation/#datajudge.pytest_integration.get_formatter","title":"get_formatter","text":"<pre><code>get_formatter(pytestconfig)\n</code></pre> Source code in <code>src/datajudge/pytest_integration.py</code> <pre><code>def get_formatter(pytestconfig):\n    color = pytestconfig.getoption(\"color\")\n\n    if color == \"yes\" or color == \"auto\":\n        # before pytest-html &lt; 4.0.0\n        # styling in assertion messages was not formatted correctly\n        # so in this case we use the default formatter\n        # in pytest-html &gt;= 4.0.0 the styling gets stripped or\n        # translated to ANSI codes, depending if ansi2html is installed\n        if pytestconfig.getoption(\"htmlpath\", False):\n            try:\n                import pytest_html\n\n                if Version(pytest_html.__version__).major &gt;= 4:\n                    return AnsiColorFormatter()\n            finally:\n                return Formatter()\n\n        return AnsiColorFormatter()\n    else:\n        return Formatter()\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements","title":"requirements","text":""},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement","title":"BetweenRequirement","text":"<pre><code>BetweenRequirement(data_source: DataSource, data_source2: DataSource, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>               Bases: <code>Requirement</code></p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(\n    self,\n    data_source: DataSource,\n    data_source2: DataSource,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    self._data_source = data_source\n    self._data_source2 = data_source2\n    self._ref = DataReference(self._data_source)\n    self._ref2 = DataReference(self._data_source2)\n    self._date_column = date_column\n    self._date_column2 = date_column2\n    super().__init__()\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_column_subset_constraint","title":"add_column_subset_constraint","text":"<pre><code>add_column_subset_constraint(name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Columns of first table are subset of second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_subset_constraint(\n    self, name: str | None = None, cache_size=None\n) -&gt; None:\n    \"\"\"Columns of first table are subset of second table.\"\"\"\n    self._constraints.append(\n        column_constraints.ColumnSubset(\n            self._ref, ref2=self._ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_column_superset_constraint","title":"add_column_superset_constraint","text":"<pre><code>add_column_superset_constraint(name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Columns of first table are superset of columns of second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_superset_constraint(\n    self, name: str | None = None, cache_size=None\n) -&gt; None:\n    \"\"\"Columns of first table are superset of columns of second table.\"\"\"\n    self._constraints.append(\n        column_constraints.ColumnSuperset(\n            self._ref, ref2=self._ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_column_type_constraint","title":"add_column_type_constraint","text":"<pre><code>add_column_type_constraint(column1: str, column2: str, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check that the columns have the same type.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_type_constraint(\n    self,\n    column1: str,\n    column2: str,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check that the columns have the same type.\"\"\"\n    ref1 = DataReference(self._data_source, [column1])\n    ref2 = DataReference(self._data_source2, [column2])\n    self._constraints.append(\n        column_constraints.ColumnType(\n            ref1, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_date_max_constraint","title":"add_date_max_constraint","text":"<pre><code>add_date_max_constraint(column1: str, column2: str, use_upper_bound_reference: bool = True, column_type: str = 'date', condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Compare date max of first table to date max of second table.</p> <p>The used columns of both tables need to be of the same type.</p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_upper_bound_reference</code>, the max of the first table has to be smaller or equal to the max of the second table. If not <code>use_upper_bound_reference</code>, the max of the first table has to be greater or equal to the max of the second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_max_constraint(\n    self,\n    column1: str,\n    column2: str,\n    use_upper_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Compare date max of first table to date max of second table.\n\n    The used columns of both tables need to be of the same type.\n\n    For more information on ``column_type`` values, see ``add_column_type_constraint``.\n\n    If ``use_upper_bound_reference``, the max of the first table has to be\n    smaller or equal to the max of the second table.\n    If not ``use_upper_bound_reference``, the max of the first table has to\n    be greater or equal to the max of the second table.\n    \"\"\"\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        date_constraints.DateMax(\n            ref,\n            ref2=ref2,\n            use_upper_bound_reference=use_upper_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_date_min_constraint","title":"add_date_min_constraint","text":"<pre><code>add_date_min_constraint(column1: str, column2: str, use_lower_bound_reference: bool = True, column_type: str = 'date', condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Ensure date min of first table is greater or equal date min of second table.</p> <p>The used columns of both tables need to be of the same type.</p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_lower_bound_reference</code>, the min of the first table has to be greater or equal to the min of the second table. If not <code>use_upper_bound_reference</code>, the min of the first table has to be smaller or equal to the min of the second table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_min_constraint(\n    self,\n    column1: str,\n    column2: str,\n    use_lower_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Ensure date min of first table is greater or equal date min of second table.\n\n    The used columns of both tables need to be of the same type.\n\n    For more information on ``column_type`` values, see ``add_column_type_constraint``.\n\n    If ``use_lower_bound_reference``, the min of the first table has to be\n    greater or equal to the min of the second table.\n    If not ``use_upper_bound_reference``, the min of the first table has to\n    be smaller or equal to the min of the second table.\n    \"\"\"\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        date_constraints.DateMin(\n            ref,\n            ref2=ref2,\n            use_lower_bound_reference=use_lower_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_ks_2sample_constraint","title":"add_ks_2sample_constraint","text":"<pre><code>add_ks_2sample_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, significance_level: float = 0.05, cache_size=None) -&gt; None\n</code></pre> <p>Apply the so-called two-sample Kolmogorov-Smirnov test to the distributions of the two given columns.</p> <p>The constraint is fulfilled, when the resulting p-value of the test is higher than the significance level (default is 0.05, i.e., 5%). The significance_level must be a value between 0.0 and 1.0.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_ks_2sample_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    significance_level: float = 0.05,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"\n    Apply the so-called two-sample Kolmogorov-Smirnov test to the distributions of the two given columns.\n\n    The constraint is fulfilled, when the resulting p-value of the test is higher than the significance level\n    (default is 0.05, i.e., 5%).\n    The significance_level must be a value between 0.0 and 1.0.\n    \"\"\"\n    if not column1 or not column2:\n        raise ValueError(\n            \"Column names have to be given for this test's functionality.\"\n        )\n\n    if significance_level &lt;= 0.0 or significance_level &gt; 1.0:\n        raise ValueError(\n            \"The requested significance level has to be in ``(0.0, 1.0]``. Default is 0.05.\"\n        )\n\n    ref = DataReference(self._data_source, [column1], condition=condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition=condition2)\n    self._constraints.append(\n        stats_constraints.KolmogorovSmirnov2Sample(\n            ref,\n            ref2,\n            significance_level,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_max_null_fraction_constraint","title":"add_max_null_fraction_constraint","text":"<pre><code>add_max_null_fraction_constraint(column1: str, column2: str, max_relative_deviation: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the fraction of <code>NULL</code> values of one is at most that of the other.</p> <p>Given that <code>column2</code>'s underlying data has a fraction <code>q</code> of <code>NULL</code> values, the <code>max_relative_deviation</code> parameter allows <code>column1</code>'s underlying data to have a fraction <code>(1 + max_relative_deviation) * q</code> of <code>NULL</code> values.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_max_null_fraction_constraint(\n    self,\n    column1: str,\n    column2: str,\n    max_relative_deviation: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the fraction of ``NULL`` values of one is at most that of the other.\n\n    Given that ``column2``'s underlying data has a fraction ``q`` of ``NULL`` values, the\n    ``max_relative_deviation`` parameter allows ``column1``'s underlying data to have a\n    fraction ``(1 + max_relative_deviation) * q`` of ``NULL`` values.\n    \"\"\"  # noqa: D301\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        miscs_constraints.MaxNullFraction(\n            ref,\n            ref2=ref2,\n            max_relative_deviation=max_relative_deviation,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_rows_equality_constraint","title":"add_n_rows_equality_constraint","text":"<pre><code>add_n_rows_equality_constraint(condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_equality_constraint(\n    self,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsEquality(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_rows_max_gain_constraint","title":"add_n_rows_max_gain_constraint","text":"<pre><code>add_n_rows_max_gain_constraint(constant_max_relative_gain: float | None = None, date_range_gain_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of rows hasn't grown by more than expected.</p> <p>In particular, assert that</p> \\[n^{rows}_1 \\leq n^{rows}_2 \\cdot (1 + \\text{cmrg})\\] <p>See readme for more information on <code>constant_max_relative_gain</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_max_gain_constraint(\n    self,\n    constant_max_relative_gain: float | None = None,\n    date_range_gain_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of rows hasn't grown by more than expected.\n\n    In particular, assert that\n\n    $$n^{rows}_1 \\leq n^{rows}_2 \\cdot (1 + \\text{cmrg})$$\n\n    See readme for more information on ``constant_max_relative_gain``.\n    \"\"\"\n    max_relative_gain_getter = self._get_deviation_getter(\n        constant_max_relative_gain, date_range_gain_deviation\n    )\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsMaxGain(\n            ref,\n            ref2,\n            max_relative_gain_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_rows_max_loss_constraint","title":"add_n_rows_max_loss_constraint","text":"<pre><code>add_n_rows_max_loss_constraint(constant_max_relative_loss: float | None = None, date_range_loss_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of rows hasn't decreased too much.</p> <p>In particular, assert that</p> \\[n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 - \\text{cmrl})\\] <p>See readme for more information on <code>constant_max_relative_loss</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_max_loss_constraint(\n    self,\n    constant_max_relative_loss: float | None = None,\n    date_range_loss_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of rows hasn't decreased too much.\n\n    In particular, assert that\n\n    $$n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 - \\text{cmrl})$$\n\n    See readme for more information on ``constant_max_relative_loss``.\n    \"\"\"\n    max_relative_loss_getter = self._get_deviation_getter(\n        constant_max_relative_loss, date_range_loss_deviation\n    )\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsMaxLoss(\n            ref,\n            ref2,\n            max_relative_loss_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_rows_min_gain_constraint","title":"add_n_rows_min_gain_constraint","text":"<pre><code>add_n_rows_min_gain_constraint(constant_min_relative_gain: float | None = None, date_range_gain_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of rows hasn't grown less than expected.</p> <p>In particular, assert that</p> \\[n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 + \\text{cmrg})\\] <p>See readme for more information on <code>constant_min_relative_gain</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_min_gain_constraint(\n    self,\n    constant_min_relative_gain: float | None = None,\n    date_range_gain_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of rows hasn't grown less than expected.\n\n    In particular, assert that\n\n    $$n^{rows}_1 \\geq n^{rows}_2 \\cdot (1 + \\text{cmrg})$$\n\n    See readme for more information on ``constant_min_relative_gain``.\n    \"\"\"\n    min_relative_gain_getter = self._get_deviation_getter(\n        constant_min_relative_gain, date_range_gain_deviation\n    )\n    ref = DataReference(self._data_source, condition=condition1)\n    ref2 = DataReference(self._data_source2, condition=condition2)\n    self._constraints.append(\n        nrows_constraints.NRowsMinGain(\n            ref,\n            ref2,\n            min_relative_gain_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_uniques_equality_constraint","title":"add_n_uniques_equality_constraint","text":"<pre><code>add_n_uniques_equality_constraint(columns1: list[str] | None, columns2: list[str] | None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_equality_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.NUniquesEquality(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_uniques_max_gain_constraint","title":"add_n_uniques_max_gain_constraint","text":"<pre><code>add_n_uniques_max_gain_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_relative_gain: float | None = None, date_range_gain_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of uniques hasn't grown by too much.</p> <p>In particular, assert that</p> \\[n^{uniques}_1 \\leq n^{uniques}_2 \\cdot (1 - \\text{cmrg})\\] <p>The number of uniques in first table are defined based on <code>columns1</code>, the number of uniques in second table are defined based on <code>columns2</code>.</p> <p>See readme for more information on <code>constant_max_relative_gain</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_max_gain_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_relative_gain: float | None = None,\n    date_range_gain_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of uniques hasn't grown by too much.\n\n    In particular, assert that\n\n    $$n^{uniques}_1 \\leq n^{uniques}_2 \\cdot (1 - \\text{cmrg})$$\n\n    The number of uniques in first table are defined based on ``columns1``, the\n    number of uniques in second table are defined based on ``columns2``.\n\n    See readme for more information on ``constant_max_relative_gain``.\n    \"\"\"\n    max_relative_gain_getter = self._get_deviation_getter(\n        constant_max_relative_gain, date_range_gain_deviation\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.NUniquesMaxGain(\n            ref,\n            ref2,\n            max_relative_gain_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_n_uniques_max_loss_constraint","title":"add_n_uniques_max_loss_constraint","text":"<pre><code>add_n_uniques_max_loss_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_relative_loss: float | None = None, date_range_loss_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the number of uniques hasn't decreased too much.</p> <p>In particular, assert that</p> \\[n^{uniques}_1 \\geq n^{uniques}_2 \\cdot (1 - \\text{cmrl})\\] <p>The number of uniques in first table are defined based on <code>columns1</code>, the number of uniques in second table are defined based on <code>columns2</code>.</p> <p>See readme for more information on <code>constant_max_relative_loss</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_max_loss_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_relative_loss: float | None = None,\n    date_range_loss_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    r\"\"\"Assert that the number of uniques hasn't decreased too much.\n\n    In particular, assert that\n\n    $$n^{uniques}_1 \\geq n^{uniques}_2 \\cdot (1 - \\text{cmrl})$$\n\n    The number of uniques in first table are defined based on ``columns1``, the\n    number of uniques in second table are defined based on ``columns2``.\n\n    See readme for more information on ``constant_max_relative_loss``.\n    \"\"\"\n    max_relative_loss_getter = self._get_deviation_getter(\n        constant_max_relative_loss, date_range_loss_deviation\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.NUniquesMaxLoss(\n            ref,\n            ref2,\n            max_relative_loss_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_numeric_max_constraint","title":"add_numeric_max_constraint","text":"<pre><code>add_numeric_max_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_max_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericMax(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_numeric_mean_constraint","title":"add_numeric_mean_constraint","text":"<pre><code>add_numeric_mean_constraint(column1: str, column2: str, max_absolute_deviation: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_mean_constraint(\n    self,\n    column1: str,\n    column2: str,\n    max_absolute_deviation: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericMean(\n            ref,\n            max_absolute_deviation,\n            ref2=ref2,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_numeric_min_constraint","title":"add_numeric_min_constraint","text":"<pre><code>add_numeric_min_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_min_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericMin(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_numeric_percentile_constraint","title":"add_numeric_percentile_constraint","text":"<pre><code>add_numeric_percentile_constraint(column1: str, column2: str, percentage: float, max_absolute_deviation: float | None = None, max_relative_deviation: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the <code>percentage</code>-th percentile is approximately equal.</p> <p>The percentile is defined as the smallest value present in <code>column1</code> / <code>column2</code> for which <code>percentage</code> % of the values in <code>column1</code> / <code>column2</code> are less or equal. <code>NULL</code> values are ignored.</p> <p>Hence, if <code>percentage</code> is less than the inverse of the number of non-<code>NULL</code> rows, <code>None</code> is received as the <code>percentage</code>-th percentile.</p> <p><code>percentage</code> is expected to be provided in percent. The median, for example, would correspond to <code>percentage=50</code>.</p> <p>At least one of <code>max_absolute_deviation</code> and <code>max_relative_deviation</code> must be provided.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_percentile_constraint(\n    self,\n    column1: str,\n    column2: str,\n    percentage: float,\n    max_absolute_deviation: float | None = None,\n    max_relative_deviation: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the ``percentage``-th percentile is approximately equal.\n\n    The percentile is defined as the smallest value present in ``column1`` / ``column2``\n    for which ``percentage`` % of the values in ``column1`` / ``column2`` are\n    less or equal. ``NULL`` values are ignored.\n\n    Hence, if ``percentage`` is less than the inverse of the number of non-``NULL``\n    rows, ``None`` is received as the ``percentage``-th percentile.\n\n    ``percentage`` is expected to be provided in percent. The median, for example,\n    would correspond to ``percentage=50``.\n\n    At least one of ``max_absolute_deviation`` and ``max_relative_deviation`` must\n    be provided.\n    \"\"\"\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        numeric_constraints.NumericPercentile(\n            ref,\n            percentage=percentage,\n            max_absolute_deviation=max_absolute_deviation,\n            max_relative_deviation=max_relative_deviation,\n            ref2=ref2,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_row_equality_constraint","title":"add_row_equality_constraint","text":"<pre><code>add_row_equality_constraint(columns1: list[str] | None, columns2: list[str] | None, max_missing_fraction: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>At most <code>max_missing_fraction</code> of rows in T1 and T2 are absent in either.</p> <p>In other words</p> \\[\\frac{|T1 - T2| + |T2 - T1|}{|T1 \\cup T2|} \\leq \\text{mmf}\\] <p>Rows from T1 are indexed in <code>columns1</code>, rows from T2 are indexed in <code>columns2</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_equality_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    max_missing_fraction: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"At most ``max_missing_fraction`` of rows in T1 and T2 are absent in either.\n\n    In other words\n\n    $$\\\\frac{|T1 - T2| + |T2 - T1|}{|T1 \\\\cup T2|} \\\\leq \\\\text{mmf}$$\n\n    Rows from T1 are indexed in ``columns1``, rows from T2 are indexed in ``columns2``.\n    \"\"\"  # noqa: D301\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        row_constraints.RowEquality(\n            ref,\n            ref2,\n            lambda engine: max_missing_fraction,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_row_matching_equality_constraint","title":"add_row_matching_equality_constraint","text":"<pre><code>add_row_matching_equality_constraint(matching_columns1: list[str], matching_columns2: list[str], comparison_columns1: list[str], comparison_columns2: list[str], max_missing_fraction: float, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Match tables in matching_columns, compare for equality in comparison_columns.</p> <p>This constraint is similar to the nature of the <code>RowEquality</code> constraint. Just as the latter, this constraint divides the cardinality of an intersection by the cardinality of a union. The difference lies in how the set are created. While <code>RowEquality</code> considers all rows of both tables, indexed in columns, <code>RowMatchingEquality</code> considers only rows in both tables having values in <code>matching_columns</code> present in both tables. At most <code>max_missing_fraction</code> of such rows can be missing in the intersection.</p> <p>Alternatively, this can be thought of as counting mismatches in <code>comparison_columns</code> after performing an inner join on <code>matching_columns</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_matching_equality_constraint(\n    self,\n    matching_columns1: list[str],\n    matching_columns2: list[str],\n    comparison_columns1: list[str],\n    comparison_columns2: list[str],\n    max_missing_fraction: float,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Match tables in matching_columns, compare for equality in comparison_columns.\n\n    This constraint is similar to the nature of the ``RowEquality``\n    constraint. Just as the latter, this constraint divides the\n    cardinality of an intersection by the cardinality of a union.\n    The difference lies in how the set are created. While ``RowEquality``\n    considers all rows of both tables, indexed in columns,\n    ``RowMatchingEquality`` considers only rows in both tables having values\n    in ``matching_columns`` present in both tables. At most ``max_missing_fraction``\n    of such rows can be missing in the intersection.\n\n    Alternatively, this can be thought of as counting mismatches in\n    ``comparison_columns`` after performing an inner join on ``matching_columns``.\n    \"\"\"\n    ref = DataReference(\n        self._data_source, matching_columns1 + comparison_columns1, condition1\n    )\n    ref2 = DataReference(\n        self._data_source2, matching_columns2 + comparison_columns2, condition2\n    )\n    self._constraints.append(\n        row_constraints.RowMatchingEquality(\n            ref,\n            ref2,\n            matching_columns1,\n            matching_columns2,\n            comparison_columns1,\n            comparison_columns2,\n            lambda engine: max_missing_fraction,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_row_subset_constraint","title":"add_row_subset_constraint","text":"<pre><code>add_row_subset_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_missing_fraction: float | None, date_range_loss_fraction: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>At most <code>max_missing_fraction</code> of rows in T1 are not in T2.</p> <p>In other words, :math:<code>\\frac{|T1-T2|}{|T1|} \\leq</code> <code>max_missing_fraction</code>. Rows from T1 are indexed in columns1, rows from T2 are indexed in <code>columns2</code>.</p> <p>In particular, the operation <code>|T1-T2|</code> relies on a sql <code>EXCEPT</code> statement. In contrast to <code>EXCEPT ALL</code>, this should lead to a set subtraction instead of a multiset subtraction. In other words, duplicates in T1 are treated as single occurrences.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_subset_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_missing_fraction: float | None,\n    date_range_loss_fraction: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"At most ``max_missing_fraction`` of rows in T1 are not in T2.\n\n    In other words,\n    :math:`\\\\frac{|T1-T2|}{|T1|} \\\\leq` ``max_missing_fraction``.\n    Rows from T1 are indexed in columns1, rows from T2 are indexed in ``columns2``.\n\n    In particular, the operation ``|T1-T2|`` relies on a sql ``EXCEPT`` statement. In\n    contrast to ``EXCEPT ALL``, this should lead to a set subtraction instead of\n    a multiset subtraction. In other words, duplicates in T1 are treated as\n    single occurrences.\n    \"\"\"  # noqa: D301\n    max_missing_fraction_getter = self._get_deviation_getter(\n        constant_max_missing_fraction, date_range_loss_fraction\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        row_constraints.RowSubset(\n            ref,\n            ref2,\n            max_missing_fraction_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_row_superset_constraint","title":"add_row_superset_constraint","text":"<pre><code>add_row_superset_constraint(columns1: list[str] | None, columns2: list[str] | None, constant_max_missing_fraction: float, date_range_loss_fraction: float | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>At most <code>max_missing_fraction</code> of rows in T2 are not in T1.</p> <p>In other words, \\(\\frac{|T2-T1|}{|T2|} \\leq\\) <code>max_missing_fraction</code>. Rows from T1 are indexed in <code>columns1</code>, rows from T2 are indexed in <code>columns2</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_row_superset_constraint(\n    self,\n    columns1: list[str] | None,\n    columns2: list[str] | None,\n    constant_max_missing_fraction: float,\n    date_range_loss_fraction: float | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"At most ``max_missing_fraction`` of rows in T2 are not in T1.\n\n    In other words, $\\\\frac{|T2-T1|}{|T2|} \\\\leq$ ``max_missing_fraction``.\n    Rows from T1 are indexed in ``columns1``, rows from T2 are indexed in\n    ``columns2``.\n    \"\"\"  # noqa: D301\n    max_missing_fraction_getter = self._get_deviation_getter(\n        constant_max_missing_fraction, date_range_loss_fraction\n    )\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        row_constraints.RowSuperset(\n            ref,\n            ref2,\n            max_missing_fraction_getter,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_uniques_equality_constraint","title":"add_uniques_equality_constraint","text":"<pre><code>add_uniques_equality_constraint(columns1: list[str], columns2: list[str], filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check if the data's unique values in given columns are equal.</p> <p>The <code>UniquesEquality</code> constraint asserts if the values contained in a column of a <code>DataSource</code>'s columns, are strictly the ones of another <code>DataSource</code>'s columns.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly..</p> <p>See <code>Uniques</code> for further parameter details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_equality_constraint(\n    self,\n    columns1: list[str],\n    columns2: list[str],\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if the data's unique values in given columns are equal.\n\n    The ``UniquesEquality`` constraint asserts if the values contained in a column\n    of a ``DataSource``'s columns, are strictly the ones of another ``DataSource``'s\n    columns.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly..\n\n    See [`Uniques`][datajudge.constraints.uniques.Uniques] for further parameter details on ``map_func``,\n    ``reduce_func``, and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.UniquesEquality(\n            ref,\n            ref2=ref2,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_uniques_subset_constraint","title":"add_uniques_subset_constraint","text":"<pre><code>add_uniques_subset_constraint(columns1: list[str], columns2: list[str], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, compare_distinct: bool = False, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None) -&gt; None\n</code></pre> <p>Check if the given columns's unique values in are contained in reference data.</p> <p>The <code>UniquesSubset</code> constraint asserts if the values contained in given column of a <code>DataSource</code> are part of the unique values of given columns of another <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly. <code>max_relative_violations</code> indicates what fraction of rows of the given table may have values not included in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>By default, the number of occurrences affects the computed fraction of violations. To disable this weighting, set <code>compare_distinct=True</code>. This argument does not have an effect on the test results for other <code>Uniques</code> constraints, or if <code>max_relative_violations</code> is 0.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_subset_constraint(\n    self,\n    columns1: list[str],\n    columns2: list[str],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    compare_distinct: bool = False,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if the given columns's unique values in are contained in reference data.\n\n    The ``UniquesSubset`` constraint asserts if the values contained in given column of\n    a ``DataSource`` are part of the unique values of given columns of another\n    ``DataSource``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly.\n    ``max_relative_violations`` indicates what fraction of rows of the given table\n    may have values not included in the reference set of unique values. Please note\n    that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    By default, the number of occurrences affects the computed fraction of violations.\n    To disable this weighting, set ``compare_distinct=True``.\n    This argument does not have an effect on the test results for other [`Uniques`][datajudge.constraints.uniques.Uniques] constraints,\n    or if ``max_relative_violations`` is 0.\n\n    See [`Uniques`][datajudge.constraints.uniques.Uniques] for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.UniquesSubset(\n            ref,\n            ref2=ref2,\n            max_relative_violations=max_relative_violations,\n            compare_distinct=compare_distinct,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_uniques_superset_constraint","title":"add_uniques_superset_constraint","text":"<pre><code>add_uniques_superset_constraint(columns1: list[str], columns2: list[str], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None) -&gt; None\n</code></pre> <p>Check if unique values of columns are contained in the reference data.</p> <p>The <code>UniquesSuperset</code> constraint asserts that reference set of expected values, derived from the unique values in given columns of the reference <code>DataSource</code>, is contained in given columns of a <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly..</p> <p><code>max_relative_violations</code> indicates what fraction of unique values of the given <code>DataSource</code> are not represented in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>One use of this constraint is to test for consistency in columns with expected categorical values.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_superset_constraint(\n    self,\n    columns1: list[str],\n    columns2: list[str],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if unique values of columns are contained in the reference data.\n\n    The ``UniquesSuperset`` constraint asserts that reference set of expected values,\n    derived from the unique values in given columns of the reference ``DataSource``,\n    is contained in given columns of a ``DataSource``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly..\n\n    ``max_relative_violations`` indicates what fraction of unique values of the given\n    ``DataSource`` are not represented in the reference set of unique values. Please\n    note that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    One use of this constraint is to test for consistency in columns with expected\n    categorical values.\n\n    See [`Uniques`][datajudge.constraints.uniques.Uniques] for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns1, condition1)\n    ref2 = DataReference(self._data_source2, columns2, condition2)\n    self._constraints.append(\n        uniques_constraints.UniquesSuperset(\n            ref,\n            ref2=ref2,\n            max_relative_violations=max_relative_violations,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_varchar_max_length_constraint","title":"add_varchar_max_length_constraint","text":"<pre><code>add_varchar_max_length_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_max_length_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        varchar_constraints.VarCharMaxLength(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.add_varchar_min_length_constraint","title":"add_varchar_min_length_constraint","text":"<pre><code>add_varchar_min_length_constraint(column1: str, column2: str, condition1: Condition | None = None, condition2: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_min_length_constraint(\n    self,\n    column1: str,\n    column2: str,\n    condition1: Condition | None = None,\n    condition2: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    ref = DataReference(self._data_source, [column1], condition1)\n    ref2 = DataReference(self._data_source2, [column2], condition2)\n    self._constraints.append(\n        varchar_constraints.VarCharMinLength(\n            ref, ref2=ref2, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.from_expressions","title":"from_expressions  <code>classmethod</code>","text":"<pre><code>from_expressions(expression1, expression2, name1: str, name2: str, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>Create a <code>BetweenTableRequirement</code> based on sqlalchemy expressions.</p> <p>Any sqlalchemy object implementing the <code>alias</code> method can be passed as an argument for the <code>expression1</code> and <code>expression2</code> parameters. This could, e.g. be a <code>sqlalchemy.Table</code> object or the result of a <code>sqlalchemy.select</code> invocation.</p> <p><code>name1</code> and <code>name2</code> will be used to represent the expressions in error messages, respectively.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_expressions(\n    cls,\n    expression1,\n    expression2,\n    name1: str,\n    name2: str,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    \"\"\"Create a ``BetweenTableRequirement`` based on sqlalchemy expressions.\n\n    Any sqlalchemy object implementing the ``alias`` method can be passed as an\n    argument for the ``expression1`` and ``expression2`` parameters. This could,\n    e.g. be a ``sqlalchemy.Table`` object or the result of a ``sqlalchemy.select``\n    invocation.\n\n    ``name1`` and ``name2`` will be used to represent the expressions in error messages,\n    respectively.\n    \"\"\"\n    return cls(\n        data_source=ExpressionDataSource(expression1, name1),\n        data_source2=ExpressionDataSource(expression2, name2),\n        date_column=date_column,\n        date_column2=date_column2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.from_raw_queries","title":"from_raw_queries  <code>classmethod</code>","text":"<pre><code>from_raw_queries(query1: str, query2: str, name1: str, name2: str, columns1: list[str] | None = None, columns2: list[str] | None = None, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>Create a <code>BetweenRequirement</code> based on raw query strings.</p> <p>The <code>query1</code> and <code>query2</code> parameters can be passed any query string returning rows, e.g. <code>\"SELECT * FROM myschema.mytable LIMIT 1337\"</code> or <code>\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"</code>.</p> <p><code>name1</code> and <code>name2</code> will be used to represent the queries in error messages, respectively.</p> <p>If constraints rely on specific columns, these should be provided here via <code>columns1</code> and <code>columns2</code> respectively.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_raw_queries(\n    cls,\n    query1: str,\n    query2: str,\n    name1: str,\n    name2: str,\n    columns1: list[str] | None = None,\n    columns2: list[str] | None = None,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    \"\"\"Create a ``BetweenRequirement`` based on raw query strings.\n\n    The ``query1`` and ``query2`` parameters can be passed any query string returning\n    rows, e.g. ``\"SELECT * FROM myschema.mytable LIMIT 1337\"`` or\n    ``\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"``.\n\n    ``name1`` and ``name2`` will be used to represent the queries in error messages,\n    respectively.\n\n    If constraints rely on specific columns, these should be provided here via\n    ``columns1`` and ``columns2`` respectively.\n    \"\"\"\n    return cls(\n        data_source=RawQueryDataSource(query1, name1, columns=columns1),\n        data_source2=RawQueryDataSource(query2, name2, columns=columns2),\n        date_column=date_column,\n        date_column2=date_column2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.BetweenRequirement.from_tables","title":"from_tables  <code>classmethod</code>","text":"<pre><code>from_tables(db_name1: str, schema_name1: str, table_name1: str, db_name2: str, schema_name2: str, table_name2: str, date_column: str | None = None, date_column2: str | None = None)\n</code></pre> <p>Create a <code>BetweenRequirement</code> based on a table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_tables(\n    cls,\n    db_name1: str,\n    schema_name1: str,\n    table_name1: str,\n    db_name2: str,\n    schema_name2: str,\n    table_name2: str,\n    date_column: str | None = None,\n    date_column2: str | None = None,\n):\n    \"\"\"Create a ``BetweenRequirement`` based on a table.\"\"\"\n    return cls(\n        data_source=TableDataSource(\n            db_name=db_name1,\n            schema_name=schema_name1,\n            table_name=table_name1,\n        ),\n        data_source2=TableDataSource(\n            db_name=db_name2,\n            schema_name=schema_name2,\n            table_name=table_name2,\n        ),\n        date_column=date_column,\n        date_column2=date_column2,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.Requirement","title":"Requirement","text":"<pre><code>Requirement()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>MutableSequence</code></p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(self):\n    self._constraints: list[Constraint] = []\n    self._data_source: DataSource\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.Requirement.insert","title":"insert","text":"<pre><code>insert(index: int, value: Constraint) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def insert(self, index: int, value: Constraint) -&gt; None:\n    self._constraints.insert(index, value)\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.Requirement.test","title":"test","text":"<pre><code>test(engine) -&gt; list[TestResult]\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def test(self, engine) -&gt; list[TestResult]:\n    return [constraint.test(engine) for constraint in self]\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.TableQualifier","title":"TableQualifier","text":"<pre><code>TableQualifier(db_name: str, schema_name: str, table_name: str)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(self, db_name: str, schema_name: str, table_name: str):\n    self._db_name = db_name\n    self._schema_name = schema_name\n    self._table_name = table_name\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.TableQualifier.get_between_requirement","title":"get_between_requirement","text":"<pre><code>get_between_requirement(table_qualifier)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def get_between_requirement(self, table_qualifier):\n    return BetweenRequirement.from_tables(\n        db_name1=self._db_name,\n        schema_name1=self._schema_name,\n        table_name1=self._table_name,\n        db_name2=table_qualifier.db_name,\n        schema_name2=table_qualifier.schema_name,\n        table_name2=table_qualifier.table_name,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.TableQualifier.get_within_requirement","title":"get_within_requirement","text":"<pre><code>get_within_requirement()\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def get_within_requirement(self):\n    return WithinRequirement.from_table(\n        db_name=self._db_name,\n        schema_name=self._schema_name,\n        table_name=self._table_name,\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement","title":"WithinRequirement","text":"<pre><code>WithinRequirement(data_source: DataSource)\n</code></pre> <p>               Bases: <code>Requirement</code></p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def __init__(self, data_source: DataSource):\n    self._data_source = data_source\n    super().__init__()\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_categorical_bound_constraint","title":"add_categorical_bound_constraint","text":"<pre><code>add_categorical_bound_constraint(columns: list[str], distribution: dict[_T, tuple[float, float]], default_bounds: tuple[float, float] = (0, 0), max_relative_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check if the distribution of unique values in columns falls within the specified minimum and maximum bounds.</p> <p>The <code>CategoricalBoundConstraint</code> is added to ensure the distribution of unique values in the specified columns of a <code>DataSource</code> falls within the given minimum and maximum bounds defined in the <code>distribution</code> parameter.</p> PARAMETER DESCRIPTION <code>columns</code> <p>A list of column names from the <code>DataSource</code> to apply the constraint on.</p> <p> TYPE: <code>list[str]</code> </p> <code>distribution</code> <p>A dictionary where keys represent unique values and the corresponding tuple values represent the minimum and maximum allowed proportions of the respective unique value in the columns.</p> <p> TYPE: <code>dict[_T, tuple[float, float]]</code> </p> <code>default_bounds</code> <p>A tuple specifying the minimum and maximum allowed proportions for all elements not mentioned in the distribution. By default, it's set to (0, 0), which means all elements not present in <code>distribution</code> will cause a constraint failure.</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0, 0)</code> </p> <code>max_relative_violations</code> <p>A tolerance threshold (0 to 1) for the proportion of elements in the data that can violate the bound constraints without triggering the constraint violation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>condition</code> <p>An optional parameter to specify a <code>Condition</code> object to filter the data before applying the constraint.</p> <p> TYPE: <code>Condition | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>An optional parameter to provide a custom name for the constraint.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cache_size</code> <p>TODO</p> <p> DEFAULT: <code>None</code> </p> Example: <p>This method can be used to test for consistency in columns with expected categorical values or ensure that the distribution of values in a column adheres to a certain criterion.</p> <p>Usage:</p> <pre><code>requirement = WithinRequirement(data_source)\nrequirement.add_categorical_bound_constraint(\n    columns=['column_name'],\n    distribution={'A': (0.2, 0.3), 'B': (0.4, 0.6), 'C': (0.1, 0.2)},\n    max_relative_violations=0.05,\n    name='custom_name'\n)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_categorical_bound_constraint(\n    self,\n    columns: list[str],\n    distribution: dict[_T, tuple[float, float]],\n    default_bounds: tuple[float, float] = (0, 0),\n    max_relative_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check if the distribution of unique values in columns falls within the specified minimum and maximum bounds.\n\n    The ``CategoricalBoundConstraint`` is added to ensure the distribution of unique values\n    in the specified columns of a ``DataSource`` falls within the given minimum and maximum\n    bounds defined in the ``distribution`` parameter.\n\n    Parameters\n    ----------\n    columns:\n        A list of column names from the `DataSource` to apply the constraint on.\n    distribution:\n        A dictionary where keys represent unique values and the corresponding\n        tuple values represent the minimum and maximum allowed proportions of the respective\n        unique value in the columns.\n    default_bounds:\n        A tuple specifying the minimum and maximum allowed proportions for all\n        elements not mentioned in the distribution. By default, it's set to (0, 0), which means\n        all elements not present in `distribution` will cause a constraint failure.\n    max_relative_violations:\n        A tolerance threshold (0 to 1) for the proportion of elements in the data that can violate the\n        bound constraints without triggering the constraint violation.\n    condition:\n        An optional parameter to specify a `Condition` object to filter the data\n        before applying the constraint.\n    name:\n        An optional parameter to provide a custom name for the constraint.\n    cache_size:\n        TODO\n\n    Example:\n    -------\n    This method can be used to test for consistency in columns with expected categorical\n    values or ensure that the distribution of values in a column adheres to a certain\n    criterion.\n\n    Usage:\n\n    ```\n    requirement = WithinRequirement(data_source)\n    requirement.add_categorical_bound_constraint(\n        columns=['column_name'],\n        distribution={'A': (0.2, 0.3), 'B': (0.4, 0.6), 'C': (0.1, 0.2)},\n        max_relative_violations=0.05,\n        name='custom_name'\n    )\n    ```\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.CategoricalBoundConstraint(\n            ref,\n            distribution=distribution,\n            default_bounds=default_bounds,\n            max_relative_violations=max_relative_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_column_existence_constraint","title":"add_column_existence_constraint","text":"<pre><code>add_column_existence_constraint(columns: list[str], name: str | None = None, cache_size=None) -&gt; None\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_existence_constraint(\n    self, columns: list[str], name: str | None = None, cache_size=None\n) -&gt; None:\n    # Note that columns are not meant to be part of the reference.\n    ref = DataReference(self._data_source)\n    self._constraints.append(\n        column_constraints.ColumnExistence(ref, columns, cache_size=cache_size)\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_column_type_constraint","title":"add_column_type_constraint","text":"<pre><code>add_column_type_constraint(column: str, column_type: str | TypeEngine, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check if a column type matches the expected column_type.</p> <p>The <code>column_type</code> can be provided as a string (backend-specific type name), a backend-specific SQLAlchemy type, or a SQLAlchemy's generic type.</p> <p>If SQLAlchemy's generic types are used, the check is performed using <code>isinstance</code>, which means that the actual type can also be a subclass of the target type. For more information on SQLAlchemy's generic types, see https://docs.sqlalchemy.org/en/20/core/type_basics.html</p> PARAMETER DESCRIPTION <code>column</code> <p>The name of the column to which the constraint will be applied.</p> <p> TYPE: <code>str</code> </p> <code>column_type</code> <p>The expected type of the column. This can be a string, a backend-specific SQLAlchemy type, or a generic SQLAlchemy type.</p> <p> TYPE: <code>str | TypeEngine</code> </p> <code>name</code> <p>An optional name for the constraint. If not provided, a name will be generated automatically.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_column_type_constraint(\n    self,\n    column: str,\n    column_type: str | sa.types.TypeEngine,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"\n    Check if a column type matches the expected column_type.\n\n    The ``column_type`` can be provided as a string (backend-specific type name), a backend-specific SQLAlchemy type, or a SQLAlchemy's generic type.\n\n    If SQLAlchemy's generic types are used, the check is performed using ``isinstance``, which means that the actual type can also be a subclass of the target type.\n    For more information on SQLAlchemy's generic types, see https://docs.sqlalchemy.org/en/20/core/type_basics.html\n\n    Parameters\n    ----------\n    column : str\n        The name of the column to which the constraint will be applied.\n\n    column_type : str | sa.types.TypeEngine\n        The expected type of the column. This can be a string, a backend-specific SQLAlchemy type, or a generic SQLAlchemy type.\n\n    name : str | None\n        An optional name for the constraint. If not provided, a name will be generated automatically.\n    \"\"\"\n    ref = DataReference(self._data_source, [column])\n    self._constraints.append(\n        column_constraints.ColumnType(\n            ref,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_date_between_constraint","title":"add_date_between_constraint","text":"<pre><code>add_date_between_constraint(column: str, lower_bound: str, upper_bound: str, min_fraction: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Use string format: <code>lower_bound=\"'20121230'\"</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_between_constraint(\n    self,\n    column: str,\n    lower_bound: str,\n    upper_bound: str,\n    min_fraction: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Use string format: ``lower_bound=\"'20121230'\"``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        date_constraints.DateBetween(\n            ref,\n            min_fraction,\n            lower_bound,\n            upper_bound,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_date_max_constraint","title":"add_date_max_constraint","text":"<pre><code>add_date_max_constraint(column: str, max_value: str, use_upper_bound_reference: bool = True, column_type: str = 'date', condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Ensure all dates to be superior than <code>max_value</code>.</p> <p>Use string format: <code>max_value=\"'20121230'\"</code></p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_upper_bound_reference</code> is <code>True</code>, the maximum date in <code>column</code> has to be smaller or equal to <code>max_value</code>. Otherwise the maximum date in <code>column</code> has to be greater or equal to <code>max_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_max_constraint(\n    self,\n    column: str,\n    max_value: str,\n    use_upper_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Ensure all dates to be superior than ``max_value``.\n\n    Use string format: ``max_value=\"'20121230'\"``\n\n    For more information on ``column_type`` values, see [`add_column_type_constraint`][datajudge.requirements.WithinRequirement.add_column_type_constraint].\n\n    If ``use_upper_bound_reference`` is ``True``, the maximum date in ``column`` has to be smaller or\n    equal to ``max_value``. Otherwise the maximum date in ``column`` has to be greater or equal\n    to ``max_value``.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        date_constraints.DateMax(\n            ref,\n            max_value=max_value,\n            use_upper_bound_reference=use_upper_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_date_min_constraint","title":"add_date_min_constraint","text":"<pre><code>add_date_min_constraint(column: str, min_value: str, use_lower_bound_reference: bool = True, column_type: str = 'date', condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Ensure all dates to be superior than <code>min_value</code>.</p> <p>Use string format: <code>min_value=\"'20121230'\"</code>.</p> <p>For more information on <code>column_type</code> values, see <code>add_column_type_constraint</code>.</p> <p>If <code>use_lower_bound_reference</code>, the min of the first table has to be greater or equal to <code>min_value</code>. If not <code>use_upper_bound_reference</code>, the min of the first table has to be smaller or equal to <code>min_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_min_constraint(\n    self,\n    column: str,\n    min_value: str,\n    use_lower_bound_reference: bool = True,\n    column_type: str = \"date\",\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Ensure all dates to be superior than ``min_value``.\n\n    Use string format: ``min_value=\"'20121230'\"``.\n\n    For more information on ``column_type`` values, see ``add_column_type_constraint``.\n\n    If ``use_lower_bound_reference``, the min of the first table has to be\n    greater or equal to ``min_value``.\n    If not ``use_upper_bound_reference``, the min of the first table has to\n    be smaller or equal to ``min_value``.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        date_constraints.DateMin(\n            ref,\n            min_value=min_value,\n            use_lower_bound_reference=use_lower_bound_reference,\n            column_type=column_type,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_date_no_gap_constraint","title":"add_date_no_gap_constraint","text":"<pre><code>add_date_no_gap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Express that date range rows have no gap in-between them.</p> <p>The table under inspection must consist of at least one but up to many key columns, identifying an entity. Additionally, a <code>start_column</code> and an <code>end_column</code>, indicating start and end dates, should be provided.</p> <p>Neither of those columns should contain <code>NULL</code> values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p>Note that the value of <code>start_column</code> is expected to be included in each date range. By default, the value of <code>end_column</code> is expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several date ranges.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this gap property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key_values, at least one gap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_no_gap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Express that date range rows have no gap in-between them.\n\n    The table under inspection must consist of at least one but up to many key columns,\n    identifying an entity. Additionally, a ``start_column`` and an ``end_column``,\n    indicating start and end dates, should be provided.\n\n    Neither of those columns should contain ``NULL`` values. Also, it should hold that\n    for a given row, the value of ``end_column`` is strictly greater than the value of\n    ``start_column``.\n\n    Note that the value of ``start_column`` is expected to be included in each date range.\n    By default, the value of ``end_column`` is expected to be included as well - this can\n    however be changed by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often has\n    several rows. Thereby, a key will often come with several date ranges.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be\n    considered as composing the key.\n\n    In order to express a tolerance for some violations of this gap property, use the\n    ``max_relative_n_violations`` parameter. The latter expresses for what fraction\n    of all key_values, at least one gap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = (\n        ([start_column, end_column] + key_columns) if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        date_constraints.DateNoGap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            max_relative_n_violations=max_relative_n_violations,\n            legitimate_gap_size=1 if end_included else 0,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_date_no_overlap_2d_constraint","title":"add_date_no_overlap_2d_constraint","text":"<pre><code>add_date_no_overlap_2d_constraint(start_column1: str, end_column1: str, start_column2: str, end_column2: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Express that several date range rows do not overlap in two date dimensions.</p> <p>The table under inspection must consist of at least one but up to many key columns, identifying an entity. Per date dimension, a start column (<code>start_column1</code>, <code>start_column2</code>) and end (<code>end_column1</code>, <code>end_column2</code>) column should be provided in order to define date ranges.</p> <p>Date ranges in different date dimensions are expected to represent different kinds of dates. For example, let's say that a row in a table indicates an averag temperature forecast. <code>start_column1</code> and <code>end_column1</code> could the date span that was forecasted, e.g. the weather from next Saturday to next Sunday. <code>end_column1</code> and <code>end_column2</code> might indicate the timespan when this forceast was used, e.g. from the previous Monday to Wednesday.</p> <p>Neither of those columns should contain <code>NULL</code> values. Also, the value of <code>end_column_k</code> should be strictly greater than the value of <code>start_column_k</code>.</p> <p>Note that the values of <code>start_column1</code> and <code>start_column2</code> are expected to be included in each date range. By default, the values of <code>end_column1</code> and <code>end_column2</code> are expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in key_columns and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several date ranges.</p> <p>Often, you might want the date ranges for a given key not to overlap.</p> <p>If key_columns is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this non-overlapping property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key_values, at least one overlap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_no_overlap_2d_constraint(\n    self,\n    start_column1: str,\n    end_column1: str,\n    start_column2: str,\n    end_column2: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Express that several date range rows do not overlap in two date dimensions.\n\n    The table under inspection must consist of at least one but up to many key columns,\n    identifying an entity. Per date dimension, a start column (``start_column1``, ``start_column2``)\n    and end (``end_column1``, ``end_column2``) column should be provided in order to define\n    date ranges.\n\n    Date ranges in different date dimensions are expected to represent different kinds\n    of dates. For example, let's say that a row in a table indicates an averag temperature\n    forecast. ``start_column1`` and ``end_column1`` could the date span that was forecasted,\n    e.g. the weather from next Saturday to next Sunday. ``end_column1`` and ``end_column2``\n    might indicate the timespan when this forceast was used, e.g. from the\n    previous Monday to Wednesday.\n\n    Neither of those columns should contain ``NULL`` values. Also, the value of ``end_column_k``\n    should be strictly greater than the value of ``start_column_k``.\n\n    Note that the values of ``start_column1`` and ``start_column2`` are expected to be\n    included in each date range. By default, the values of ``end_column1`` and\n    ``end_column2`` are expected to be included as well - this can however be changed\n    by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in key_columns and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often has\n    several rows. Thereby, a key will often come with several date ranges.\n\n    Often, you might want the date ranges for a given key not to overlap.\n\n    If key_columns is ``None`` or ``[]``, all columns of the table will be considered as\n    composing the key.\n\n    In order to express a tolerance for some violations of this non-overlapping property,\n    use the ``max_relative_n_violations`` parameter. The latter expresses for what fraction\n    of all key_values, at least one overlap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = (\n        [start_column1, end_column1, start_column2, end_column2] + key_columns\n        if key_columns\n        else []\n    )\n    ref = DataReference(\n        self._data_source,\n        relevant_columns,\n        condition,\n    )\n    self._constraints.append(\n        date_constraints.DateNoOverlap2d(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column1, start_column2],\n            end_columns=[end_column1, end_column2],\n            end_included=end_included,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_date_no_overlap_constraint","title":"add_date_no_overlap_constraint","text":"<pre><code>add_date_no_overlap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Constraint expressing that several date range rows may not overlap.</p> <p>The <code>DataSource</code> under inspection must consist of at least one but up to many <code>key_columns</code>, identifying an entity, a <code>start_column</code> and an <code>end_column</code>.</p> <p>For a given row in this <code>DataSource</code>, <code>start_column</code> and <code>end_column</code> indicate a date range. Neither of those columns should contain NULL values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p>Note that the value of <code>start_column</code> is expected to be included in each date range. By default, the value of <code>end_column</code> is expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several date ranges.</p> <p>Often, you might want the date ranges for a given key not to overlap.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this non-overlapping property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key values, at least one overlap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_date_no_overlap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Constraint expressing that several date range rows may not overlap.\n\n    The [`DataSource`][datajudge.DataSource] under inspection must consist of at least one but up\n    to many ``key_columns``, identifying an entity, a ``start_column`` and an\n    ``end_column``.\n\n    For a given row in this [`DataSource`][datajudge.DataSource], ``start_column`` and ``end_column`` indicate a\n    date range. Neither of those columns should contain NULL values. Also, it\n    should hold that for a given row, the value of ``end_column`` is strictly greater\n    than the value of ``start_column``.\n\n    Note that the value of ``start_column`` is expected to be included in each date\n    range. By default, the value of ``end_column`` is expected to be included as well -\n    this can however be changed by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often\n    has several rows. Thereby, a key will often come with several date ranges.\n\n    Often, you might want the date ranges for a given key not to overlap.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be considered\n    as composing the key.\n\n    In order to express a tolerance for some violations of this non-overlapping\n    property, use the ``max_relative_n_violations`` parameter. The latter expresses for\n    what fraction of all key values, at least one overlap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = [start_column, end_column] + (\n        key_columns if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        date_constraints.DateNoOverlap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            end_included=end_included,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_functional_dependency_constraint","title":"add_functional_dependency_constraint","text":"<pre><code>add_functional_dependency_constraint(key_columns: list[str], value_columns: list[str], condition: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>Expresses a functional dependency, a constraint where the <code>value_columns</code> are uniquely determined by the <code>key_columns</code>.</p> <p>This means that for each unique combination of values in the <code>key_columns</code>, there is exactly one corresponding combination of values in the <code>value_columns</code>.</p> <p>The <code>add_unique_constraint</code> constraint is a special case of this constraint, where the <code>key_columns</code> are a primary key, and all other columns are included <code>value_columns</code>. This constraint allows for a more general definition of functional dependencies, where the <code>key_columns</code> are not necessarily a primary key.</p> <p>An additional configuration option (for details see the analogous parameter in for <code>Uniques</code>-constraints) on how the output is sorted and how many counterexamples are shown is available as <code>output_processors</code>.</p> <p>An additional configuration option (for details see the analogous parameter in for <code>Uniques</code>-constraints) on how the output is sorted and how many counterexamples are shown is available as <code>output_processors</code>.</p> <p>For more information on functional dependencies, see https://en.wikipedia.org/wiki/Functional_dependency.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_functional_dependency_constraint(\n    self,\n    key_columns: list[str],\n    value_columns: list[str],\n    condition: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    \"\"\"Expresses a functional dependency, a constraint where the `value_columns` are uniquely determined by the `key_columns`.\n\n    This means that for each unique combination of values in the `key_columns`, there is exactly one corresponding combination of values in the `value_columns`.\n\n    The ``add_unique_constraint`` constraint is a special case of this constraint, where the ``key_columns`` are a primary key,\n    and all other columns are included ``value_columns``.\n    This constraint allows for a more general definition of functional dependencies, where the ``key_columns`` are not necessarily a primary key.\n\n    An additional configuration option (for details see the analogous parameter in for ``Uniques``-constraints)\n    on how the output is sorted and how many counterexamples are shown is available as ``output_processors``.\n\n    An additional configuration option (for details see the analogous parameter in for ``Uniques``-constraints)\n    on how the output is sorted and how many counterexamples are shown is available as ``output_processors``.\n\n    For more information on functional dependencies, see https://en.wikipedia.org/wiki/Functional_dependency.\n    \"\"\"\n    relevant_columns = key_columns + value_columns\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        miscs_constraints.FunctionalDependency(\n            ref,\n            key_columns=key_columns,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_groupby_aggregation_constraint","title":"add_groupby_aggregation_constraint","text":"<pre><code>add_groupby_aggregation_constraint(columns: Sequence[str], aggregation_column: str, start_value: int, tolerance: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Check whether array aggregate corresponds to an integer range.</p> <p>The <code>DataSource</code> is grouped by <code>columns</code>. Sql's <code>array_agg</code> function is then applied to the <code>aggregate_column</code>.</p> <p>Since we expect <code>aggregate_column</code> to be a numeric column, this leads to a multiset of aggregated values. These values should correspond to the integers ranging from <code>start_value</code> to the cardinality of the multiset.</p> <p>In order to allow for slight deviations from this pattern, <code>tolerance</code> expresses the fraction of all grouped-by rows, which may be incomplete ranges.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_groupby_aggregation_constraint(\n    self,\n    columns: Sequence[str],\n    aggregation_column: str,\n    start_value: int,\n    tolerance: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Check whether array aggregate corresponds to an integer range.\n\n    The ``DataSource`` is grouped by ``columns``. Sql's ``array_agg`` function is then\n    applied to the ``aggregate_column``.\n\n    Since we expect ``aggregate_column`` to be a numeric column, this leads to\n    a multiset of aggregated values. These values should correspond to the integers\n    ranging from ``start_value`` to the cardinality of the multiset.\n\n    In order to allow for slight deviations from this pattern, ``tolerance`` expresses\n    the fraction of all grouped-by rows, which may be incomplete ranges.\n    \"\"\"\n    ref = DataReference(self._data_source, list(columns), condition)\n    self._constraints.append(\n        groupby_constraints.AggregateNumericRangeEquality(\n            ref,\n            aggregation_column=aggregation_column,\n            tolerance=tolerance,\n            start_value=start_value,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_max_null_fraction_constraint","title":"add_max_null_fraction_constraint","text":"<pre><code>add_max_null_fraction_constraint(column: str, max_null_fraction: float, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Assert that <code>column</code> has less than a certain fraction of <code>NULL</code> values.</p> <p><code>max_null_fraction</code> is expected to lie within [0, 1].</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_max_null_fraction_constraint(\n    self,\n    column: str,\n    max_null_fraction: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Assert that ``column`` has less than a certain fraction of ``NULL`` values.\n\n    ``max_null_fraction`` is expected to lie within [0, 1].\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        miscs_constraints.MaxNullFraction(\n            ref,\n            max_null_fraction=max_null_fraction,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_n_rows_equality_constraint","title":"add_n_rows_equality_constraint","text":"<pre><code>add_n_rows_equality_constraint(n_rows: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_equality_constraint(\n    self,\n    n_rows: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, None, condition)\n    self._constraints.append(\n        nrows_constraints.NRowsEquality(\n            ref, n_rows=n_rows, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_n_rows_max_constraint","title":"add_n_rows_max_constraint","text":"<pre><code>add_n_rows_max_constraint(n_rows_max: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_max_constraint(\n    self,\n    n_rows_max: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, None, condition)\n    self._constraints.append(\n        nrows_constraints.NRowsMax(\n            ref, n_rows=n_rows_max, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_n_rows_min_constraint","title":"add_n_rows_min_constraint","text":"<pre><code>add_n_rows_min_constraint(n_rows_min: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_rows_min_constraint(\n    self,\n    n_rows_min: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, None, condition)\n    self._constraints.append(\n        nrows_constraints.NRowsMin(\n            ref, n_rows=n_rows_min, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_n_uniques_equality_constraint","title":"add_n_uniques_equality_constraint","text":"<pre><code>add_n_uniques_equality_constraint(columns: list[str] | None, n_uniques: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_n_uniques_equality_constraint(\n    self,\n    columns: list[str] | None,\n    n_uniques: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.NUniquesEquality(\n            ref, n_uniques=n_uniques, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_null_absence_constraint","title":"add_null_absence_constraint","text":"<pre><code>add_null_absence_constraint(column: str, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_null_absence_constraint(\n    self,\n    column: str,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        miscs_constraints.MaxNullFraction(\n            ref, max_null_fraction=0, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_between_constraint","title":"add_numeric_between_constraint","text":"<pre><code>add_numeric_between_constraint(column: str, lower_bound: float, upper_bound: float, min_fraction: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the column's values lie between <code>lower_bound</code> and <code>upper_bound</code>.</p> <p>Note that both bounds are inclusive.</p> <p>Unless specified otherwise via the usage of a <code>condition</code>, <code>NULL</code> values will be considered in the denominator of <code>min_fraction</code>. <code>NULL</code> values will never be considered to lie in the interval [<code>lower_bound</code>, <code>upper_bound</code>].</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_between_constraint(\n    self,\n    column: str,\n    lower_bound: float,\n    upper_bound: float,\n    min_fraction: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the column's values lie between ``lower_bound`` and ``upper_bound``.\n\n    Note that both bounds are inclusive.\n\n    Unless specified otherwise via the usage of a ``condition``, ``NULL`` values will\n    be considered in the denominator of ``min_fraction``. ``NULL`` values will never be\n    considered to lie in the interval [``lower_bound``, ``upper_bound``].\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericBetween(\n            ref,\n            min_fraction,\n            lower_bound,\n            upper_bound,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_max_constraint","title":"add_numeric_max_constraint","text":"<pre><code>add_numeric_max_constraint(column: str, max_value: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>All values in <code>column</code> are less or equal <code>max_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_max_constraint(\n    self,\n    column: str,\n    max_value: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"All values in ``column`` are less or equal ``max_value``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericMax(\n            ref, max_value=max_value, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_mean_constraint","title":"add_numeric_mean_constraint","text":"<pre><code>add_numeric_mean_constraint(column: str, mean_value: float, max_absolute_deviation: float, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert the mean of the column <code>column</code> deviates at most <code>max_deviation</code> from <code>mean_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_mean_constraint(\n    self,\n    column: str,\n    mean_value: float,\n    max_absolute_deviation: float,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert the mean of the column ``column`` deviates at most ``max_deviation`` from ``mean_value``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericMean(\n            ref,\n            max_absolute_deviation,\n            mean_value=mean_value,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_min_constraint","title":"add_numeric_min_constraint","text":"<pre><code>add_numeric_min_constraint(column: str, min_value: float, condition: Condition | None = None, cache_size=None) -&gt; None\n</code></pre> <p>All values in <code>column</code> are greater or equal <code>min_value</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_min_constraint(\n    self,\n    column: str,\n    min_value: float,\n    condition: Condition | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"All values in ``column`` are greater or equal ``min_value``.\"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericMin(\n            ref, min_value=min_value, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_no_gap_constraint","title":"add_numeric_no_gap_constraint","text":"<pre><code>add_numeric_no_gap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, legitimate_gap_size: float = 0, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Express that numeric interval rows have no gaps larger than some max value in-between them.</p> <p>The table under inspection must consist of at least one but up to many key columns, identifying an entity. Additionally, a <code>start_column</code> and an <code>end_column</code>, indicating interval start and end values, should be provided.</p> <p>Neither of those columns should contain <code>NULL</code> values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p><code>legitimate_gap_size</code> is the maximum tollerated gap size between two intervals.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several intervals.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this gap property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key_values, at least one gap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_no_gap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    legitimate_gap_size: float = 0,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Express that numeric interval rows have no gaps larger than some max value in-between them.\n\n    The table under inspection must consist of at least one but up to many key columns,\n    identifying an entity. Additionally, a ``start_column`` and an ``end_column``,\n    indicating interval start and end values, should be provided.\n\n    Neither of those columns should contain ``NULL`` values. Also, it should hold that\n    for a given row, the value of ``end_column`` is strictly greater than the value of\n    ``start_column``.\n\n    ``legitimate_gap_size`` is the maximum tollerated gap size between two intervals.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often has\n    several rows. Thereby, a key will often come with several intervals.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be\n    considered as composing the key.\n\n    In order to express a tolerance for some violations of this gap property, use the\n    ``max_relative_n_violations`` parameter. The latter expresses for what fraction\n    of all key_values, at least one gap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = (\n        ([start_column, end_column] + key_columns) if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        numeric_constraints.NumericNoGap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            legitimate_gap_size=legitimate_gap_size,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_no_overlap_constraint","title":"add_numeric_no_overlap_constraint","text":"<pre><code>add_numeric_no_overlap_constraint(start_column: str, end_column: str, key_columns: list[str] | None = None, end_included: bool = True, max_relative_n_violations: float = 0, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Constraint expressing that several numeric interval rows may not overlap.</p> <p>The <code>DataSource</code> under inspection must consist of at least one but up to many <code>key_columns</code>, identifying an entity, a <code>start_column</code> and an <code>end_column</code>.</p> <p>For a given row in this <code>DataSource</code>, <code>start_column</code> and <code>end_column</code> indicate a numeric interval. Neither of those columns should contain NULL values. Also, it should hold that for a given row, the value of <code>end_column</code> is strictly greater than the value of <code>start_column</code>.</p> <p>Note that the value of <code>start_column</code> is expected to be included in each interval. By default, the value of <code>end_column</code> is expected to be included as well - this can however be changed by setting <code>end_included</code> to <code>False</code>.</p> <p>A 'key' is a fixed set of values in <code>key_columns</code> and represents an entity of interest. A priori, a key is not a primary key, i.e., a key can have and often has several rows. Thereby, a key will often come with several intervals.</p> <p>Often, you might want the intervals for a given key not to overlap.</p> <p>If <code>key_columns</code> is <code>None</code> or <code>[]</code>, all columns of the table will be considered as composing the key.</p> <p>In order to express a tolerance for some violations of this non-overlapping property, use the <code>max_relative_n_violations</code> parameter. The latter expresses for what fraction of all key values, at least one overlap may exist.</p> <p>For illustrative examples of this constraint, please refer to its test cases.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_no_overlap_constraint(\n    self,\n    start_column: str,\n    end_column: str,\n    key_columns: list[str] | None = None,\n    end_included: bool = True,\n    max_relative_n_violations: float = 0,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Constraint expressing that several numeric interval rows may not overlap.\n\n    The ``DataSource`` under inspection must consist of at least one but up\n    to many ``key_columns``, identifying an entity, a ``start_column`` and an\n    ``end_column``.\n\n    For a given row in this ``DataSource``, ``start_column`` and ``end_column`` indicate a\n    numeric interval. Neither of those columns should contain NULL values. Also, it\n    should hold that for a given row, the value of ``end_column`` is strictly greater\n    than the value of ``start_column``.\n\n    Note that the value of ``start_column`` is expected to be included in each interval.\n    By default, the value of ``end_column`` is expected to be included as well -\n    this can however be changed by setting ``end_included`` to ``False``.\n\n    A 'key' is a fixed set of values in ``key_columns`` and represents an entity of\n    interest. A priori, a key is not a primary key, i.e., a key can have and often\n    has several rows. Thereby, a key will often come with several intervals.\n\n    Often, you might want the intervals for a given key not to overlap.\n\n    If ``key_columns`` is ``None`` or ``[]``, all columns of the table will be considered\n    as composing the key.\n\n    In order to express a tolerance for some violations of this non-overlapping\n    property, use the ``max_relative_n_violations`` parameter. The latter expresses for\n    what fraction of all key values, at least one overlap may exist.\n\n    For illustrative examples of this constraint, please refer to its test cases.\n    \"\"\"\n    relevant_columns = [start_column, end_column] + (\n        key_columns if key_columns else []\n    )\n    ref = DataReference(self._data_source, relevant_columns, condition)\n    self._constraints.append(\n        numeric_constraints.NumericNoOverlap(\n            ref,\n            key_columns=key_columns,\n            start_columns=[start_column],\n            end_columns=[end_column],\n            end_included=end_included,\n            max_relative_n_violations=max_relative_n_violations,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_numeric_percentile_constraint","title":"add_numeric_percentile_constraint","text":"<pre><code>add_numeric_percentile_constraint(column: str, percentage: float, expected_percentile: float, max_absolute_deviation: float | None = None, max_relative_deviation: float | None = None, condition: Condition | None = None, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Assert that the <code>percentage</code>-th percentile is approximately <code>expected_percentile</code>.</p> <p>The percentile is defined as the smallest value present in <code>column</code> for which <code>percentage</code> % of the values in <code>column</code> are less or equal. <code>NULL</code> values are ignored.</p> <p>Hence, if <code>percentage</code> is less than the inverse of the number of non-<code>NULL</code> rows, <code>None</code> is received as the <code>percentage</code> -th percentile.</p> <p><code>percentage</code> is expected to be provided in percent. The median, for example, would correspond to <code>percentage=50</code>.</p> <p>At least one of <code>max_absolute_deviation</code> and <code>max_relative_deviation</code> must be provided.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_numeric_percentile_constraint(\n    self,\n    column: str,\n    percentage: float,\n    expected_percentile: float,\n    max_absolute_deviation: float | None = None,\n    max_relative_deviation: float | None = None,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Assert that the ``percentage``-th percentile is approximately ``expected_percentile``.\n\n    The percentile is defined as the smallest value present in ``column`` for which\n    ``percentage`` % of the values in ``column`` are less or equal. ``NULL`` values\n    are ignored.\n\n    Hence, if ``percentage`` is less than the inverse of the number of non-``NULL`` rows,\n    ``None`` is received as the ``percentage`` -th percentile.\n\n    ``percentage`` is expected to be provided in percent. The median, for example, would\n    correspond to ``percentage=50``.\n\n    At least one of ``max_absolute_deviation`` and ``max_relative_deviation`` must\n    be provided.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        numeric_constraints.NumericPercentile(\n            ref,\n            percentage=percentage,\n            expected_percentile=expected_percentile,\n            max_absolute_deviation=max_absolute_deviation,\n            max_relative_deviation=max_relative_deviation,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_primary_key_definition_constraint","title":"add_primary_key_definition_constraint","text":"<pre><code>add_primary_key_definition_constraint(primary_keys: list[str], name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Check that the primary key constraints in the database are exactly equal to the given column names.</p> <p>Note that this doesn't actually check that the primary key values are unique across the table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_primary_key_definition_constraint(\n    self,\n    primary_keys: list[str],\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Check that the primary key constraints in the database are exactly equal to the given column names.\n\n    Note that this doesn't actually check that the primary key values are unique across the table.\n    \"\"\"\n    ref = DataReference(self._data_source)\n    self._constraints.append(\n        miscs_constraints.PrimaryKeyDefinition(\n            ref, primary_keys, name=name, cache_size=cache_size\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_uniqueness_constraint","title":"add_uniqueness_constraint","text":"<pre><code>add_uniqueness_constraint(columns: list[str] | None = None, max_duplicate_fraction: float = 0, condition: Condition | None = None, max_absolute_n_duplicates: int = 0, infer_pk_columns: bool = False, name: str | None = None, cache_size=None) -&gt; None\n</code></pre> <p>Columns should uniquely identify row.</p> <p>Given a list of columns <code>columns</code>, validate the condition of a primary key, i.e. uniqueness of tuples in said columns. This constraint has a tolerance for inconsistencies, expressed via <code>max_duplicate_fraction</code>. The latter suggests that the number of uniques from said columns is larger or equal to <code>1 - max_duplicate_fraction</code> times the number of rows.</p> <p>If <code>infer_pk_columns</code> is <code>True</code>, <code>columns</code> will be retrieved from the primary keys. If <code>columns</code> is <code>None</code> and <code>infer_pk_column</code> is <code>False</code>, the fallback is validating that all rows in a table are unique.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniqueness_constraint(\n    self,\n    columns: list[str] | None = None,\n    max_duplicate_fraction: float = 0,\n    condition: Condition | None = None,\n    max_absolute_n_duplicates: int = 0,\n    infer_pk_columns: bool = False,\n    name: str | None = None,\n    cache_size=None,\n) -&gt; None:\n    \"\"\"Columns should uniquely identify row.\n\n    Given a list of columns ``columns``, validate the condition of a primary key, i.e.\n    uniqueness of tuples in said columns. This constraint has a tolerance\n    for inconsistencies, expressed via ``max_duplicate_fraction``. The latter\n    suggests that the number of uniques from said columns is larger or equal\n    to ``1 - max_duplicate_fraction`` times the number of rows.\n\n    If ``infer_pk_columns`` is ``True``, ``columns`` will be retrieved from the primary keys.\n    If ``columns`` is ``None`` and ``infer_pk_column`` is ``False``, the fallback is\n    validating that all rows in a table are unique.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        miscs_constraints.Uniqueness(\n            ref,\n            max_duplicate_fraction=max_duplicate_fraction,\n            max_absolute_n_duplicates=max_absolute_n_duplicates,\n            infer_pk_columns=infer_pk_columns,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_uniques_equality_constraint","title":"add_uniques_equality_constraint","text":"<pre><code>add_uniques_equality_constraint(columns: list[str], uniques: Collection[_T], filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> <p>Check if the data's unique values are equal to a given set of values.</p> <p>The <code>UniquesEquality</code> constraint asserts if the values contained in a column of a <code>DataSource</code> are strictly the ones of a reference set of expected values, specified via the <code>uniques</code> parameter.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>.</p> <p>By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p>See the <code>Uniques</code> class for further parameter details on <code>map_func</code> and <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_equality_constraint(\n    self,\n    columns: list[str],\n    uniques: Collection[_T],\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    \"\"\"Check if the data's unique values are equal to a given set of values.\n\n    The `UniquesEquality` constraint asserts if the values contained in a column\n    of a `DataSource` are strictly the ones of a reference set of expected values,\n    specified via the `uniques` parameter.\n\n    Null values in the columns `columns` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for `WithinRequirement`.\n\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom `filter_func` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing `None` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set `filter_func` explicitly.\n\n    See the `Uniques` class for further parameter details on `map_func` and\n    `reduce_func`, and `output_processors`.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.UniquesEquality(\n            ref,\n            uniques=uniques,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_uniques_subset_constraint","title":"add_uniques_subset_constraint","text":"<pre><code>add_uniques_subset_constraint(columns: list[str], uniques: Collection[_T], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, compare_distinct: bool = False, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>Check if the data's unique values are contained in a given set of values.</p> <p>The <code>UniquesSubset</code> constraint asserts if the values contained in a column of a <code>DataSource</code> are part of a reference set of expected values, specified via <code>uniques</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>. By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> Cause (possibly often unintended) changes in behavior when the users adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p><code>max_relative_violations</code> indicates what fraction of rows of the given table may have values not included in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>By default, the number of occurrences affects the computed fraction of violations. To disable this weighting, set <code>compare_distinct=True</code>. This argument does not have an effect on the test results for other <code>Uniques</code> constraints, or if <code>max_relative_violations</code> is 0.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_subset_constraint(\n    self,\n    columns: list[str],\n    uniques: Collection[_T],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    compare_distinct: bool = False,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    \"\"\"Check if the data's unique values are contained in a given set of values.\n\n    The ``UniquesSubset`` constraint asserts if the values contained in a column of\n    a ``DataSource`` are part of a reference set of expected values, specified via\n    ``uniques``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    Cause (possibly often unintended) changes in behavior when the users adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly.\n\n\n    ``max_relative_violations`` indicates what fraction of rows of the given table\n    may have values not included in the reference set of unique values. Please note\n    that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    By default, the number of occurrences affects the computed fraction of violations.\n    To disable this weighting, set `compare_distinct=True`.\n    This argument does not have an effect on the test results for other `Uniques` constraints,\n    or if `max_relative_violations` is 0.\n\n    See ``Uniques`` for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.UniquesSubset(\n            ref,\n            uniques=uniques,\n            max_relative_violations=max_relative_violations,\n            filter_func=filter_func,\n            compare_distinct=compare_distinct,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_uniques_superset_constraint","title":"add_uniques_superset_constraint","text":"<pre><code>add_uniques_superset_constraint(columns: list[str], uniques: Collection[_T], max_relative_violations: float = 0, filter_func: Callable[[list[_T]], list[_T]] | None = None, map_func: Callable[[_T], _T] | None = None, reduce_func: Callable[[Collection], Collection] | None = None, condition: Condition | None = None, name: str | None = None, output_processors: OutputProcessor | list[OutputProcessor] | None = output_processor_limit, cache_size=None)\n</code></pre> <p>Check if unique values of columns are contained in the reference data.</p> <p>The <code>UniquesSuperset</code> constraint asserts that reference set of expected values, specified via <code>uniques</code>, is contained in given columns of a <code>DataSource</code>.</p> <p>Null values in the columns <code>columns</code> are ignored. To assert the non-existence of them use the <code>add_null_absence_constraint</code> helper method for <code>WithinRequirement</code>.</p> <p>By default, the null filtering does not trigger if multiple columns are fetched at once. It can be configured in more detail by supplying a custom <code>filter_func</code> function. Some exemplary implementations are available as <code>filternull_element</code>, <code>filternull_never</code>, <code>filternull_element_or_tuple_all</code>, <code>filternull_element_or_tuple_any</code>. Passing <code>None</code> as the argument is equivalent to <code>filternull_element</code> but triggers a warning. The current default of <code>filternull_element</code> will cause (possibly often unintended) changes in behavior when the user adds a second column (filtering no longer can trigger at all). The default will be changed to <code>filternull_element_or_tuple_all</code> in future versions. To silence the warning, set <code>filter_func</code> explicitly.</p> <p><code>max_relative_violations</code> indicates what fraction of unique values of the given <code>DataSource</code> are not represented in the reference set of unique values. Please note that <code>UniquesSubset</code> and <code>UniquesSuperset</code> are not symmetrical in this regard.</p> <p>One use of this constraint is to test for consistency in columns with expected categorical values.</p> <p>See <code>Uniques</code> for further details on <code>map_func</code>, <code>reduce_func</code>, and <code>output_processors</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_uniques_superset_constraint(\n    self,\n    columns: list[str],\n    uniques: Collection[_T],\n    max_relative_violations: float = 0,\n    filter_func: Callable[[list[_T]], list[_T]] | None = None,\n    map_func: Callable[[_T], _T] | None = None,\n    reduce_func: Callable[[Collection], Collection] | None = None,\n    condition: Condition | None = None,\n    name: str | None = None,\n    output_processors: OutputProcessor\n    | list[OutputProcessor]\n    | None = output_processor_limit,\n    cache_size=None,\n):\n    \"\"\"Check if unique values of columns are contained in the reference data.\n\n    The ``UniquesSuperset`` constraint asserts that reference set of expected values,\n    specified via ``uniques``, is contained in given columns of a ``DataSource``.\n\n    Null values in the columns ``columns`` are ignored. To assert the non-existence of them use\n    the [`add_null_absence_constraint`][datajudge.requirements.WithinRequirement.add_null_absence_constraint] helper method\n    for ``WithinRequirement``.\n\n    By default, the null filtering does not trigger if multiple columns are fetched at once.\n    It can be configured in more detail by supplying a custom ``filter_func`` function.\n    Some exemplary implementations are available as [`filternull_element`][datajudge.utils.filternull_element],\n    [`filternull_never`][datajudge.utils.filternull_never], [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all],\n    [`filternull_element_or_tuple_any`][datajudge.utils.filternull_element_or_tuple_any].\n    Passing ``None`` as the argument is equivalent to [`filternull_element`][datajudge.utils.filternull_element] but triggers a warning.\n    The current default of [`filternull_element`][datajudge.utils.filternull_element]\n    will cause (possibly often unintended) changes in behavior when the user adds a second column\n    (filtering no longer can trigger at all).\n    The default will be changed to [`filternull_element_or_tuple_all`][datajudge.utils.filternull_element_or_tuple_all] in future versions.\n    To silence the warning, set ``filter_func`` explicitly.\n\n    ``max_relative_violations`` indicates what fraction of unique values of the given\n    ``DataSource`` are not represented in the reference set of unique values. Please\n    note that ``UniquesSubset`` and ``UniquesSuperset`` are not symmetrical in this regard.\n\n    One use of this constraint is to test for consistency in columns with expected\n    categorical values.\n\n    See ``Uniques`` for further details on ``map_func``, ``reduce_func``,\n    and ``output_processors``.\n    \"\"\"\n    ref = DataReference(self._data_source, columns, condition)\n    self._constraints.append(\n        uniques_constraints.UniquesSuperset(\n            ref,\n            uniques=uniques,\n            max_relative_violations=max_relative_violations,\n            filter_func=filter_func,\n            map_func=map_func,\n            reduce_func=reduce_func,\n            output_processors=output_processors,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_varchar_max_length_constraint","title":"add_varchar_max_length_constraint","text":"<pre><code>add_varchar_max_length_constraint(column: str, max_length: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_max_length_constraint(\n    self,\n    column: str,\n    max_length: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharMaxLength(\n            ref,\n            max_length=max_length,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_varchar_min_length_constraint","title":"add_varchar_min_length_constraint","text":"<pre><code>add_varchar_min_length_constraint(column: str, min_length: int, condition: Condition | None = None, name: str | None = None, cache_size=None)\n</code></pre> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_min_length_constraint(\n    self,\n    column: str,\n    min_length: int,\n    condition: Condition | None = None,\n    name: str | None = None,\n    cache_size=None,\n):\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharMinLength(\n            ref,\n            min_length=min_length,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_varchar_regex_constraint","title":"add_varchar_regex_constraint","text":"<pre><code>add_varchar_regex_constraint(column: str, regex: str, condition: Condition | None = None, name: str | None = None, allow_none: bool = False, relative_tolerance: float = 0.0, aggregated: bool = True, n_counterexamples: int = 5, cache_size=None)\n</code></pre> <p>Assesses whether the values in a column match a given regular expression pattern.</p> <p>The option <code>allow_none</code> can be used in cases where the column is defined as nullable and contains null values.</p> <p>How the tolerance factor is calculated can be controlled with the <code>aggregated</code> flag. When <code>True</code>, the tolerance is calculated using unique values. If not, the tolerance is calculated using all the instances of the data.</p> <p><code>n_counterexamples</code> defines how many counterexamples are displayed in an assertion text. If all counterexamples are meant to be shown, provide <code>-1</code> as an argument.</p> <p>When using this method, the regex matching will take place in memory. If instead, you would like the matching to take place in database which is typically faster and substantially more memory-saving, please consider using <code>add_varchar_regex_constraint_db</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_regex_constraint(\n    self,\n    column: str,\n    regex: str,\n    condition: Condition | None = None,\n    name: str | None = None,\n    allow_none: bool = False,\n    relative_tolerance: float = 0.0,\n    aggregated: bool = True,\n    n_counterexamples: int = 5,\n    cache_size=None,\n):\n    \"\"\"Assesses whether the values in a column match a given regular expression pattern.\n\n    The option ``allow_none`` can be used in cases where the column is defined as\n    nullable and contains null values.\n\n    How the tolerance factor is calculated can be controlled with the ``aggregated``\n    flag. When ``True``, the tolerance is calculated using unique values. If not, the\n    tolerance is calculated using all the instances of the data.\n\n    ``n_counterexamples`` defines how many counterexamples are displayed in an\n    assertion text. If all counterexamples are meant to be shown, provide ``-1`` as\n    an argument.\n\n    When using this method, the regex matching will take place in memory. If instead,\n    you would like the matching to take place in database which is typically faster and\n    substantially more memory-saving, please consider using\n    ``add_varchar_regex_constraint_db``.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharRegex(\n            ref,\n            regex,\n            allow_none=allow_none,\n            relative_tolerance=relative_tolerance,\n            aggregated=aggregated,\n            n_counterexamples=n_counterexamples,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.add_varchar_regex_constraint_db","title":"add_varchar_regex_constraint_db","text":"<pre><code>add_varchar_regex_constraint_db(column: str, regex: str, condition: Condition | None = None, name: str | None = None, relative_tolerance: float = 0.0, aggregated: bool = True, n_counterexamples: int = 5, cache_size=None)\n</code></pre> <p>Assesses whether the values in a column match a given regular expression pattern.</p> <p>How the tolerance factor is calculated can be controlled with the <code>aggregated</code> flag. When <code>True</code>, the tolerance is calculated using unique values. If not, the tolerance is calculated using all the instances of the data.</p> <p><code>n_counterexamples</code> defines how many counterexamples are displayed in an assertion text. If all counterexamples are meant to be shown, provide <code>-1</code> as an argument.</p> <p>When using this method, the regex matching will take place in database, which is only supported for Postgres, Sqllite and Snowflake. Note that for this feature is only for Snowflake when using sqlalchemy-snowflake &gt;= 1.4.0. As an alternative, <code>add_varchar_regex_constraint</code> performs the regex matching in memory. This is typically slower and more expensive in terms of memory but available on all supported database mamangement systems.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>def add_varchar_regex_constraint_db(\n    self,\n    column: str,\n    regex: str,\n    condition: Condition | None = None,\n    name: str | None = None,\n    relative_tolerance: float = 0.0,\n    aggregated: bool = True,\n    n_counterexamples: int = 5,\n    cache_size=None,\n):\n    \"\"\"Assesses whether the values in a column match a given regular expression pattern.\n\n    How the tolerance factor is calculated can be controlled with the ``aggregated``\n    flag. When ``True``, the tolerance is calculated using unique values. If not, the\n    tolerance is calculated using all the instances of the data.\n\n    ``n_counterexamples`` defines how many counterexamples are displayed in an\n    assertion text. If all counterexamples are meant to be shown, provide ``-1`` as\n    an argument.\n\n    When using this method, the regex matching will take place in database, which is\n    only supported for Postgres, Sqllite and Snowflake. Note that for this\n    feature is only for Snowflake when using sqlalchemy-snowflake &gt;= 1.4.0. As an\n    alternative, ``add_varchar_regex_constraint`` performs the regex matching in memory.\n    This is typically slower and more expensive in terms of memory but available\n    on all supported database mamangement systems.\n    \"\"\"\n    ref = DataReference(self._data_source, [column], condition)\n    self._constraints.append(\n        varchar_constraints.VarCharRegexDb(\n            ref,\n            regex=regex,\n            relative_tolerance=relative_tolerance,\n            aggregated=aggregated,\n            n_counterexamples=n_counterexamples,\n            name=name,\n            cache_size=cache_size,\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.from_expression","title":"from_expression  <code>classmethod</code>","text":"<pre><code>from_expression(expression: FromClause, name: str)\n</code></pre> <p>Create a <code>WithinRequirement</code> based on a sqlalchemy expression.</p> <p>Any sqlalchemy object implementing the <code>alias</code> method can be passed as an argument for the <code>expression</code> parameter. This could, e.g. be an <code>sqlalchemy.Table</code> object or the result of a <code>sqlalchemy.select</code> call.</p> <p>The <code>name</code> will be used to represent this expression in error messages.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_expression(cls, expression: selectable.FromClause, name: str):\n    \"\"\"Create a ``WithinRequirement`` based on a sqlalchemy expression.\n\n    Any sqlalchemy object implementing the ``alias`` method can be passed as an\n    argument for the ``expression`` parameter. This could, e.g. be an\n    ``sqlalchemy.Table`` object or the result of a ``sqlalchemy.select`` call.\n\n    The ``name`` will be used to represent this expression in error messages.\n    \"\"\"\n    return cls(data_source=ExpressionDataSource(expression, name))\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.from_raw_query","title":"from_raw_query  <code>classmethod</code>","text":"<pre><code>from_raw_query(query: str, name: str, columns: list[str] | None = None)\n</code></pre> <p>Create a <code>WithinRequirement</code> based on a raw query string.</p> <p>The <code>query</code> parameter can be passed any query string returning rows, e.g. <code>\"SELECT * FROM myschema.mytable LIMIT 1337\"</code> or <code>\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"</code>.</p> <p>The <code>name</code> will be used to represent this query in error messages.</p> <p>If constraints rely on specific columns, these should be provided here via <code>columns</code>, e.g. <code>[\"id\", \"name\"]</code>.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_raw_query(cls, query: str, name: str, columns: list[str] | None = None):\n    \"\"\"Create a ``WithinRequirement`` based on a raw query string.\n\n    The ``query`` parameter can be passed any query string returning rows, e.g.\n    ``\"SELECT * FROM myschema.mytable LIMIT 1337\"`` or\n    ``\"SELECT id, name FROM table1 UNION SELECT id, name FROM table2\"``.\n\n    The ``name`` will be used to represent this query in error messages.\n\n    If constraints rely on specific columns, these should be provided here via\n    ``columns``, e.g. ``[\"id\", \"name\"]``.\n    \"\"\"\n    return cls(data_source=RawQueryDataSource(query, name, columns=columns))\n</code></pre>"},{"location":"api-documentation/#datajudge.requirements.WithinRequirement.from_table","title":"from_table  <code>classmethod</code>","text":"<pre><code>from_table(db_name: str, schema_name: str, table_name: str)\n</code></pre> <p>Create a <code>WithinRequirement</code> based on a table.</p> Source code in <code>src/datajudge/requirements.py</code> <pre><code>@classmethod\ndef from_table(cls, db_name: str, schema_name: str, table_name: str):\n    \"\"\"Create a `WithinRequirement` based on a table.\"\"\"\n    return cls(\n        data_source=TableDataSource(\n            db_name=db_name, schema_name=schema_name, table_name=table_name\n        )\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.utils","title":"utils","text":""},{"location":"api-documentation/#datajudge.utils.OutputProcessor","title":"OutputProcessor","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"api-documentation/#datajudge.utils.filternull_element","title":"filternull_element","text":"<pre><code>filternull_element(values: list) -&gt; list\n</code></pre> Source code in <code>src/datajudge/utils.py</code> <pre><code>def filternull_element(values: list) -&gt; list:\n    return [value for value in values if value is not None]\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.filternull_element_or_tuple_all","title":"filternull_element_or_tuple_all","text":"<pre><code>filternull_element_or_tuple_all(values: list) -&gt; list\n</code></pre> Source code in <code>src/datajudge/utils.py</code> <pre><code>def filternull_element_or_tuple_all(values: list) -&gt; list:\n    return [\n        value\n        for value in values\n        if value is not None\n        and not (isinstance(value, tuple) and all(x is None for x in value))\n    ]\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.filternull_element_or_tuple_any","title":"filternull_element_or_tuple_any","text":"<pre><code>filternull_element_or_tuple_any(values: list) -&gt; list\n</code></pre> Source code in <code>src/datajudge/utils.py</code> <pre><code>def filternull_element_or_tuple_any(values: list) -&gt; list:\n    return [\n        value\n        for value in values\n        if value is not None\n        and not (isinstance(value, tuple) and any(x is None for x in value))\n    ]\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.filternull_never","title":"filternull_never","text":"<pre><code>filternull_never(values: list) -&gt; list\n</code></pre> Source code in <code>src/datajudge/utils.py</code> <pre><code>def filternull_never(values: list) -&gt; list:\n    return values\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.format_difference","title":"format_difference","text":"<pre><code>format_difference(n1: float | int, n2: float | int, decimal_separator: bool = True) -&gt; tuple[str, str]\n</code></pre> <p>Format and highlight how two numbers differ.</p> <p>Given two numbers, n1 and n2, return a tuple of two strings, each representing one of the input numbers with the differing part highlighted. Highlighting is done using BBCode-like tags, which are replaced by the formatter.</p> <p>Examples:</p> <pre><code>123, 123.0\n-&gt; 123, 123[numDiff].0[/numDiff]\n122593859432, 122593859432347\n-&gt; 122593859432, 122593859432[numDiff]347[/numDiff]\n</code></pre> <p>Args: - n1: The first number to compare. - n2: The second number to compare. - decimal_separator: Whether to separate the decimal part of the numbers with commas.</p> RETURNS DESCRIPTION <code>- A tuple of two strings, each representing one of the input numbers with the differing part highlighted.</code> Source code in <code>src/datajudge/utils.py</code> <pre><code>def format_difference(\n    n1: float | int, n2: float | int, decimal_separator: bool = True\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Format and highlight how two numbers differ.\n\n    Given two numbers, n1 and n2, return a tuple of two strings,\n    each representing one of the input numbers with the differing part highlighted.\n    Highlighting is done using BBCode-like tags, which are replaced by the formatter.\n\n    Examples\n    --------\n        123, 123.0\n        -&gt; 123, 123[numDiff].0[/numDiff]\n        122593859432, 122593859432347\n        -&gt; 122593859432, 122593859432[numDiff]347[/numDiff]\n\n    Args:\n    - n1: The first number to compare.\n    - n2: The second number to compare.\n    - decimal_separator: Whether to separate the decimal part of the numbers with commas.\n\n    Returns\n    -------\n    - A tuple of two strings, each representing one of the input numbers with the differing part highlighted.\n    \"\"\"\n    if decimal_separator:\n        s1, s2 = f\"{n1:,}\", f\"{n2:,}\"\n    else:\n        s1, s2 = str(n1), str(n2)\n\n    min_len = min(len(s1), len(s2))\n    diff_idx = next(\n        (i for i in range(min_len) if s1[i] != s2[i]),\n        min_len,\n    )\n\n    return (\n        f\"{s1[:diff_idx]}{_fmt_diff_part(s1, diff_idx)}\",\n        f\"{s2[:diff_idx]}{_fmt_diff_part(s2, diff_idx)}\",\n    )\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.output_processor_limit","title":"output_processor_limit","text":"<pre><code>output_processor_limit(collection: Collection, counts: Collection | None = None, limit: int = 100) -&gt; tuple[Collection, Collection | None]\n</code></pre> <p>Limits the collection to the first <code>limit</code> elements.</p> <p>If the list was shortened, will add a <code>limit+1</code>-th string element, informing the user of the truncation.</p> <p>The default limit of <code>100</code> can be adjusted using <code>functools.partial</code>.</p> Source code in <code>src/datajudge/utils.py</code> <pre><code>def output_processor_limit(\n    collection: Collection, counts: Collection | None = None, limit: int = 100\n) -&gt; tuple[Collection, Collection | None]:\n    \"\"\"\n    Limits the collection to the first `limit` elements.\n\n    If the list was shortened, will add a `limit+1`-th string element,\n    informing the user of the truncation.\n\n    The default limit of ``100`` can be adjusted using `functools.partial`.\n    \"\"\"\n    collection = list(collection)\n\n    ret_collection = collection[:limit]\n    ret_counts = None if counts is None else list(counts)[:limit]\n    if len(collection) &gt; limit:\n        ret_collection.append(\n            f\"&lt;SHORTENED OUTPUT, displaying the first {limit} / {len(collection)} elements above&gt;\"\n        )\n        if ret_counts is not None:\n            ret_counts.append(\n                f\"&lt;SHORTENED OUTPUT, displaying the first {limit} / {len(collection)} counts above&gt;\"\n            )\n\n    return ret_collection, ret_counts\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.output_processor_sort","title":"output_processor_sort","text":"<pre><code>output_processor_sort(collection: Collection, counts: Collection | None = None) -&gt; tuple[Collection, Collection | None]\n</code></pre> <p>Sorts a collection of tuple elements in descending order of their counts.</p> <p>If ties exist, the ascending order of the elements themselves is used.</p> <p>If the first element is not instanceof tuple, each element will be transparently packaged into a 1-tuple for processing; this process is not visible to the caller.</p> <p>Handles <code>None</code> values as described in <code>sort_tuple_none_aware</code>.</p> Source code in <code>src/datajudge/utils.py</code> <pre><code>def output_processor_sort(\n    collection: Collection, counts: Collection | None = None\n) -&gt; tuple[Collection, Collection | None]:\n    \"\"\"\n    Sorts a collection of tuple elements in descending order of their counts.\n\n    If ties exist, the ascending order of the elements themselves is used.\n\n    If the first element is not instanceof tuple,\n    each element will be transparently packaged into a 1-tuple for processing;\n    this process is not visible to the caller.\n\n    Handles ``None`` values as described in ``sort_tuple_none_aware``.\n    \"\"\"\n    collection = list(collection)\n    if not isinstance(collection[0], tuple):\n        # package into a 1 tuple and pass into the method again\n        packaged_list = [(elem,) for elem in collection]\n        res_main, res_counts = output_processor_sort(packaged_list, counts)\n        return [elem[0] for elem in res_main], res_counts\n\n    if counts is None:\n        return sort_tuple_none_aware(collection), counts\n\n    if len(collection) != len(counts):\n        raise ValueError(\"collection and counts must have the same length\")\n\n    if len(collection) &lt;= 1:\n        return collection, counts  # empty or 1 element lists are always sorted\n\n    lst = sort_tuple_none_aware(\n        [(-count, *elem) for count, elem in zip(counts, collection)]\n    )\n    return [elem[1:] for elem in lst], [-elem[0] for elem in lst]\n</code></pre>"},{"location":"api-documentation/#datajudge.utils.sort_tuple_none_aware","title":"sort_tuple_none_aware","text":"<pre><code>sort_tuple_none_aware(collection: Collection[tuple], ascending=True) -&gt; Collection[tuple]\n</code></pre> <p>Stable sort of a collection of tuples.</p> <p>Each tuple in the collection must have the same length, since they are treated as rows in a table, with <code>elem[0]</code> being the first column, <code>elem[1]</code> the second, etc. for each <code>elem</code> in <code>collection</code>. For sorting, <code>None</code> is considered the same as the default value of the respective column's type.</p> <p>ints and floats <code>int()</code> and <code>float()</code> yield <code>0</code> and <code>0.0</code> respectively; for strings, <code>str()</code> yields <code>''</code>. The constructor is determined by calling <code>type</code> on the first non-<code>None</code> element of the respective column.</p> <p>Validates that all elements in collection are tuples and that all tuples have the same length.</p> Source code in <code>src/datajudge/utils.py</code> <pre><code>def sort_tuple_none_aware(\n    collection: Collection[tuple], ascending=True\n) -&gt; Collection[tuple]:\n    \"\"\"\n    Stable sort of a collection of tuples.\n\n    Each tuple in the collection must have the same length,\n    since they are treated as rows in a table,\n    with ``elem[0]`` being the first column,\n    ``elem[1]`` the second, etc. for each ``elem`` in ``collection``.\n    For sorting, ``None`` is considered the same as the default value of the respective column's type.\n\n    ints and floats ``int()`` and ``float()`` yield ``0`` and ``0.0`` respectively; for strings, ``str()`` yields ``''``.\n    The constructor is determined by calling ``type`` on the first non-``None`` element of the respective column.\n\n    Validates that all elements in collection are tuples and that all tuples have the same length.\n    \"\"\"\n    lst = list(collection)\n\n    if len(lst) &lt;= 1:\n        return lst  # empty or 1 element lists are always sorted\n\n    if not all(isinstance(elem, tuple) and len(elem) == len(lst[0]) for elem in lst):\n        raise ValueError(\"all elements must be tuples and have the same length\")\n\n    dtypes_each_tupleelement: list[type | None] = [None] * len(lst[0])\n    for dtypeidx in range(len(dtypes_each_tupleelement)):\n        for elem in lst:\n            if elem[dtypeidx] is not None:\n                dtypes_each_tupleelement[dtypeidx] = type(elem[dtypeidx])\n                break\n        else:\n            # if all entries are None, just use a constant int() == 0\n            dtypes_each_tupleelement[dtypeidx] = int\n\n    def replace_None_with_default(elem):  # noqa: N802\n        return tuple(\n            ((dtype() if dtype else None) if subelem is None else subelem)\n            for dtype, subelem in zip(dtypes_each_tupleelement, elem)\n        )\n\n    return sorted(\n        lst, key=lambda elem: replace_None_with_default(elem), reverse=not ascending\n    )\n</code></pre>"},{"location":"development/","title":"Development","text":"<p><code>datajudge</code> development relies on pixi. In order to work on <code>datajudge</code>, you can create a development environment as follows:</p> <pre><code>git clone https://github.com/Quantco/datajudge\ncd datajudge\npixi run postinstall\n</code></pre> <p>Unit tests can be run by executing</p> <pre><code>pixi run test\n</code></pre> <p>Integration tests are run against a specific backend at a time. As of now, we provide helper scripts to spin up either a Postgres or MSSQL backend.</p> <p>To run integration tests against Postgres, first start a docker container with a Postgres database:</p> <pre><code>./start_postgres.sh\n</code></pre> <p>Then, you can run tests against the database you just started with one of the Postgres-specific pixi environments, e.g.:</p> <pre><code>pixi run -e postgres-py312 test\n</code></pre> <p>Analogously, for MSSQL, run</p> <pre><code>./start_mssql.sh\n</code></pre> <p>and</p> <pre><code>pixi run -e mssql-py312 test\n</code></pre> <p>or</p> <pre><code>pixi run -e mssql-py312 test_freetds\n</code></pre> <p>depending on the driver you'd like to use.</p> <p>Please note that running tests against Snowflake and BigQuery requires authentication to be set up properly.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#glossary","title":"Glossary","text":"<ul> <li> <p>A <code>DataSource</code> represents a way to retrieve data from database. Typically, this corresponds to a table in the database. Yet, it could also be a more elaborate object. See the section on 'Alternative <code>DataSource</code> s' for more detail.</p> </li> <li> <p>A <code>Constraint</code> captures a concrete expectation between either two <code>DataSource</code> s or a single <code>DataSource</code> and a reference value.</p> </li> <li> <p>A <code>Requirement</code> captures all <code>Constraint</code> s between two given <code>DataSource</code> s or all <code>Constraint</code> s within a single <code>DataSource</code>. If a <code>Requirement</code> refers links to two <code>DataSource</code> s, it is a <code>BetweenRequirement</code>. If a <code>Requirement</code> merely refers to a single <code>DataSource</code>, it is a <code>WithinRequirement</code>.</p> </li> <li> <p>Conceptually, a 'specification' captures all <code>Requirement</code> s against a database. In practice that means it is usually a separate python file which:</p> <ul> <li>gathers all relevant <code>Requirement</code> s</li> <li>turns these <code>Requirement</code> s' <code>Constraint</code> s into individual tests</li> <li>can be 'tested' by pytest</li> </ul> </li> </ul>"},{"location":"getting-started/#creating-a-specification","title":"Creating a specification","text":"<p>In order to get going, you might want to use the following snippet in a new python file. This file will represent a specification.</p> <pre><code>import pytest\nimport sqlalchemy as sa\nfrom datajudge.pytest_integration import collect_data_tests\n\n\n@pytest.fixture(scope=\"module\")\ndef datajudge_engine():\n    # TODO: Adapt connection string to database at hand.\n    return sa.create_engine(\"your_connection_string\")\n\n\n# TODO: Insert Requirement objects to list.\nrequirements = []\n\ntest_constraints = collect_data_tests(requirements)\n</code></pre> <p>This file will eventually lead as an input to pytest. More on that in the section Testing a specification.</p> <p>In case you haven't worked with sqlalchemy engines before, you might need to install drivers to connect to your database. You might want to install snowflake-sqlalchemy when using Snowflake, pyscopg when using Postgres and platform-specific drivers (Windows, Linux, macOS) when using MSSQL.</p>"},{"location":"getting-started/#specifying-constraints","title":"Specifying Constraints","text":"<p>In order to discover possible <code>Constraint</code> s, please investigate the <code>_add_*_constraint</code> methods for <code>BetweenRequirement</code> and <code>WithinRequirement</code> respectively.</p> <p>These methods are meant to be mostly self-documenting through the usage of expressive parameters.</p> <p>Note that most <code>Constraint</code> s will allow for at least one <code>Condition</code>. A <code>Condition</code> can be thought of as a conditional event in probability theory or a filter/clause in a database query. Please consult the doc string of <code>Condition</code> for greater detail. For examples, please see <code>tests/unit/test_condition.py</code>.</p> <p>Many <code>Constraint</code> s have optional <code>columns</code> parameters. If no argument is given, all available columns will be used.</p>"},{"location":"getting-started/#defining-limitations-of-change","title":"Defining limitations of change","text":"<p><code>BetweenRequirement</code> s allow for <code>Constraint</code> s expressing the limitation of a loss or gain. For example, the <code>NRowsMinGain</code> <code>Constraint</code> expresses by how much the number of rows must at least grow from the first <code>DataSource</code> to the second. In the example of <code>NRowsMinGain</code> , this growth limitation is expressed relative to the number of rows of the first <code>DataSource</code>.</p> <p>Generally, such relative limitations can be defined in two ways:</p> <ul> <li> <p>manually, based on domain knowledge (e.g. 'at least 5% growth')</p> </li> <li> <p>automatically, based on date ranges</p> </li> </ul> <p>The former would translate to</p> <pre><code>#rows_table_2 &gt; (1 + min_relative_gain) * #rows_table_1\n</code></pre> <p>while the latter would translate to</p> <pre><code>date_growth := (max_date_table_2 - min_date_table_2) / (max_date_table_1 - min_date_table_1)\n#rows_table_2 &gt; (1 + date_growth) * #rows_table_1\n</code></pre> <p>In the latter case a date column must be passed during the instantiation of the <code>BetweenRequirement</code>. Moreover, the <code>date_range_*</code> must be passed in the respective <code>add_*_constraint</code> method. When using date ranges as an indicator of change, the <code>constant_max_*</code> argument can safely be ignored. Additionally, an additional buffer to the date growth can be added with help of the <code>date_range_gain_deviation</code> parameter:</p> <pre><code>date_growth := (max_date_table_2 - min_date_table_2) / (max_date_table_1 - min_date_table_1)\n#rows_table_2 &gt; (1 + date_growth + date_range_gain_deviation) + * #rows_table_1\n</code></pre> <p>This example revolving around <code>NRowsMinGain</code> generalizes to many <code>Constraint</code> s concerned with growth, gain, loss or shrinkage limitations.</p>"},{"location":"getting-started/#testing-a-specification","title":"Testing a specification","text":"<p>In order to test whether the <code>Constraint</code> s expressed in a specification hold true, you can simply run</p> <pre><code>pytest your_specification.py\n</code></pre> <p>This will produce results directly in your terminal. If you prefer to additionally generate a report, you can run</p> <pre><code>pytest your_specification.py --html=your_report.html\n</code></pre> <p>As the testing relies on pytest, all of <code>pytest</code>'s features can be used. More on this in the article on testing.</p>"},{"location":"getting-started/#test-information","title":"Test information","text":"<p>When calling a <code>Constraint</code>'s <code>test</code> method, a <code>TestResult</code> is returned. The latter comes with a <code>logging_message</code> field. This field comprises information about the test failure, the constraint at hand as well as the underlying database queries.</p> <p>Depending on the use case at hand, it might make sense to rely on this information for logging or data investigation purposes. Again, more on this in the article on testing.</p>"},{"location":"getting-started/#assertion-message-styling","title":"Assertion Message Styling","text":"<p>Constraints can use styling to increase the readability of their assertion messages. The styling can be set independently of the platform and converted to e.g. ANSI color codes for command line output or CSS color tags for HTML reports. The styling tags describe use cases and not concrete colors, so formatters can use arbitrary color palettes, and these are not fixed by the constraint.</p> <p>The following table lists all the supported codes, along with their descriptions and examples of how they can be used:</p>"},{"location":"getting-started/#supported-styling-codes","title":"Supported styling codes","text":"Code Description Example <code>numMatch</code> Indicates the part of a number that matches the expected value. <code>[numMatch]3.141[/numMatch]</code> <code>numDiff</code> Indicates the part of a number that differs. <code>[numDiff]6[/numDiff]</code>"},{"location":"getting-started/#alternative-datasources","title":"Alternative DataSources","text":"<p>A <code>Requirement</code> is instantiated with either one or two fixed <code>DataSource</code>s.</p> <p>While the most typical example of a <code>DataSource</code> would be a table in a database, <code>datajudge</code> allows for other <code>DataSource</code>s as well. These are often derived from primitive tables of a database.</p> <code>DataSource</code> explanation <code>WithinRequirement</code> constructor <code>BetweenRequirement</code> constructor <code>TableDataSource</code> represents a table in a database <code>WithinRequirement.from_table</code> <code>BetweenRequirement.from_tables</code> <code>ExpressionDataSource</code> represents the result of a <code>sqlalchemy</code> expression <code>WithinRequirement.from_expression</code> <code>BetweenRequirement.from_expressions</code> <code>RawQueryDataSource</code> represents the result of a SQL query expressed via a string <code>WithinRequirement.from_raw_query</code> <code>BetweenRequirement.from_raw_queries</code> <p>Typically, a user does not need to instantiate a corresponding <code>DataSource</code> themselves. Rather, this is taken care of by using the appropriate constructor for <code>WithinRequirement</code> or <code>BetweenRequirement</code>.</p> <p>Note that in principle, several tables can be combined to make up for a single <code>DataSource</code>. Yet, most of the time when trying to compare two tables, it is more convenient to create a <code>BetweenRequirement</code> and use the <code>from_tables</code> constructor.</p>"},{"location":"getting-started/#column-capitalization","title":"Column capitalization","text":"<p>Different database management systems handle the capitalization of entities, such as column names, differently. For the time being:</p> <ul> <li>Mssql: <code>datajudge</code> expects column name capitalization as is seen in database, either lowercase or uppercase.</li> <li>Postgres: <code>datajudge</code> expects lowercase column names.</li> <li>Snowflake: <code>datajudge</code> will lowercase independently of the capitalization provided.</li> </ul> <p>The Snowflake behavior is due to an upstream bug in <code>snowflake-sqlalchemy</code>.</p> <p>This behavior is subject to change.</p>"},{"location":"installation/","title":"Installation","text":"<p>To install, execute</p> <pre><code>$ pip install datajudge\n# or\n$ uv add datajudge\n</code></pre> <p>or to install it into a conda environment</p> <pre><code>$ pixi add datajudge\n# or\n$ conda install datajudge -c conda-forge\n</code></pre>"},{"location":"motivation/","title":"Motivation","text":"<p>Ensuring data quality is of great importance for many use cases. <code>datajudge</code> seeks to make this convenient.</p> <p><code>datajudge</code> allows for the expression of expectations held against data stored in databases. In particular, it allows for comparing different <code>DataSource</code>s. Yet, it also comes with functionalities to compare data from a single <code>DataSource</code> to fixed reference values derived from explicit domain knowledge.</p> <p>Not trying to reinvent the wheel, <code>datajudge</code> relies on <code>pytest</code> to execute the data expectations.</p>"},{"location":"motivation/#comparisons-between-datasources","title":"Comparisons between DataSources","text":"<p>The data generating process can be obscure for a variety of reasons. In such scenarios one might ask the questions of</p> <ul> <li>Has the data 'changed' over time?</li> <li>Was the transformation of the data successful?</li> </ul> <p>In both cases one might want to compare different data -- either from different points in time or from different transformation steps -- to each other.</p>"},{"location":"motivation/#why-not-great-expectations","title":"Why not Great Expectations?","text":"<p>The major selling point is to be able to conveniently express expectations between different <code>DataSource</code>s. Great Expectations, in contrast, focuses on expectations against a single <code>DataSource</code>.</p> <p>Moreover, some users have pointed out the following advantages:</p> <ul> <li>lots of 'query writing' is taken care of by having tailored <code>Constraint</code>s</li> <li>easier and faster onboarding</li> <li>assertion messages with counterexamples and other context information, speeding up the data debugging process</li> </ul>"},{"location":"testing/","title":"Testing","text":"<p>While <code>datajudge</code> allows to express expectations via specifications, <code>Requirement</code>s and <code>Constraint</code>s, the execution of tests is delegated to pytest. As a consequence, one may use any functionalities that pytest has to offer. Here, we want to illustrate some of these advanced functionalities that might turn out useful.</p> <p>Yet, it should be noted that for most intents and purposes, using <code>datajudge</code> 's helper function <code>collect_data_tests</code> is a good starting point. It should work out of the box and hides some complexity. For exemplary applications see, the companies example or the twitch example.</p> <p>Throughout this article we will not rely on <code>collect_data_tests</code>. Instead we will more explicitly create a mechanism turning a List of <code>Requirement</code> objects into something that can be tested by pytest manually. Importantly, we want every <code>Constraint</code> of every <code>Requirement</code> to be tested independently of each other. For instance, we would not like one failing test to halt all others.</p> <p>Many of these approaches rely on adapting pytest's <code>conftest.py</code>. If you are not familiar with this concept, you might want to read up on it in the pytest docs.</p>"},{"location":"testing/#subselection","title":"Subselection","text":"<p>Most often one might want to run all tests defined by a specification.</p> <p>Yet, for example after believing to have fixed a data problem, one might simply want to test whether a single test, which had previously been failing, succeeds at last.</p> <p>Another example for when one would like to test a subset of tests is if the data at hand is not available in its entirety. Rather, it could be that one would like to run a subset of the test suite against a subsample of the typical dataset.</p> <p>In this section, we present two approaches to do a subselection of tests.</p>"},{"location":"testing/#ex-post-subselecting-generated-tests","title":"Ex-post: subselecting generated tests","text":"<p>Instead of merely running <code>$ pytest specification.py</code> one may add pytests's <code>-k</code> flag and specify the <code>Constraint</code> (s) one cares about.</p> <p>Importantly, every <code>Constraint</code> object can be identified via a name. If one wants to figure out how this string is built, please refer to the implementation of :meth:<code>~datajudge.constraints.base.Constraint.get_description</code>. Otherwise, one could also just run all of the tests once and investigate the resulting test report to find the relevant names.</p> <p>When only caring about the <code>UniquesEquality</code> constraint in our :doc:<code>twitch example &lt;examples/example_twitch&gt;</code>. one might for instance use the following prefix the filter for it:</p> <pre><code>pytest twitch_specification.py -k \"UniquesEquality::public.twitch_v1\"\n</code></pre>"},{"location":"testing/#ex-ante-defining-categories-of-tests","title":"Ex-ante: Defining categories of tests","text":"<p>Another option to subselect a certain set of tests is by use of pytest markers. The following is one way of using markers in conjunction with <code>datajudge</code>.</p> <p>In this particular illustration we'll allow for two markers:</p> <ul> <li><code>basic</code>: indicating that only truly fundamental tests should be run</li> <li><code>all</code>: indicating that any available test should be run</li> </ul> <p>For that matter we'll add a bit of pytest magic to the respective <code>conftest.py</code>.</p> conftest.py<pre><code>def pytest_generate_tests(metafunc):\n    if \"basic_constraint\" in metafunc.fixturenames:\n        metafunc.parametrize(\n            \"basic_constraint\",\n            # Find these functions in specification.py.\n            metafunc.module.get_basic_constraints(),\n            ids=metafunc.module.idfn,\n        )\n    if \"constraint\" in metafunc.fixturenames:\n        metafunc.parametrize(\n            \"constraint\",\n            # Find these functions in specification.py.\n            metafunc.module.get_all_constraints(),\n            ids=metafunc.module.idfn,\n        )\n</code></pre> <p>Moreover, we'll have to register these markers in pytest's <code>[tool.pytest.ini_options]</code> in <code>pyproject.toml</code>. You can read more about these files here.</p> pyproject.toml<pre><code>[tool.pytest.ini_options]\naddopts = \"--strict-markers\"\nmarkers = [\n  \"basic: basic specification\",\n  \"all: entire specification\",\n]\n</code></pre> <p>Once that is taken care of, one can adapt one's specification as follows:</p> specification.py<pre><code>def get_basic_requirements() -&gt; List[Requirement]:\n    # Create relevant Requirement objects and respective Constraints.\n    # ...\n    return requirements\n\ndef get_advanced_requirements() -&gt; List[Requirement]:\n    # Create relevant Requirement objects and respective Constraints.\n    # ...\n    return requirements\n\ndef get_basic_constraints() -&gt; List[Constraint]:\n    return [constraint for requirement in get_basic_requirements() for constraint in requirement]\n\ndef get_all_constraints() -&gt; List[Constraint]:\n    all_requirements = get_basic_requirements() + get_advanced_requirements()\n    return [constraint for requirement in all_requirements for constraint in requirement]\n\n# Function used in conftest.py.\n# Given a constraint, returns an identifier used to refer to it as a test.\ndef idfn(constraint):\n    return constraint.get_description()\n\n@pytest.mark.basic\ndef test_basic_constraint(basic_constraint: Constraint, datajudge_engine):\n    test_result = basic_constraint.test(datajudge_engine)\n    assert test_result.outcome, test_result.failure_message\n\n@pytest.mark.all\ndef test_all_constraint(constraint: Constraint, datajudge_engine):\n    test_result = constraint.test(datajudge_engine)\n    assert test_result.outcome, test_result.failure_message\n</code></pre> <p>Once these changes are taken care of, one may run</p> <pre><code>pytest specification.py -m basic\n</code></pre> <p>to only test the basic <code>Requirement</code> s or</p> <pre><code>pytest specification.py -m all\n</code></pre> <p>to test all <code>Requirement</code>s.</p>"},{"location":"testing/#using-parameters-in-a-specification","title":"Using parameters in a specification","text":"<p>A given specification might rely on identifiers such as database names or table names. Moreover it might be that, e.g. when iterating from one version of the data to another, these names change.</p> <p>In other words, it could be that the logic should remain unchanged while pointers to data might change. Therefore, one might just as well consider those pointers or identifiers as parameters of the specification.</p> <p>For the sake of concreteness, we will assume here that we wish frame two identifiers as parameters:</p> <ul> <li><code>new_db</code>: the name of the 'new database'</li> <li><code>old_db</code>: the name of the 'old database'</li> </ul> <p>In light of that we will again adapt pytest's <code>conftest.py</code>:</p> conftest.py<pre><code>def pytest_addoption(parser):\n    parser.addoption(\"--new_db\", action=\"store\", help=\"name of the new database\")\n    parser.addoption(\"--old_db\", action=\"store\", help=\"name of the old database\")\n\n\ndef pytest_generate_tests(metafunc):\n    params = {\n        \"db_name_new\": metafunc.config.option.new_db,\n        \"db_name_old\": metafunc.config.option.old_db,\n    }\n    metafunc.parametrize(\n        \"constraint\",\n        metafunc.module.get_constraints(params),\n        ids=metafunc.module.idfn,\n    )\n</code></pre> <p>Now, we can make the creation of our <code>Requirement</code>s and <code>Constraint</code>s dependent on these parameters:</p> application.py<pre><code>def get_requirements(params):\n    between_requirement = BetweenRequirement.from_tables(\n        db_name1=params[\"old_db\"],\n        db_name2=params[\"new_db\"],\n        # ...\n    )\n    # ...\n    return requirements\n\ndef get_constraints(params):\n    return [\n        constraint for requirement in get_requirements(params) for constraint in requirement\n    ]\n\ndef idfn(constraint):\n    return constraint.get_description()\n\ndef test_constraint(constraint, datajudge_engine):\n    test_result = constraint.test(datajudge_engine)\n    assert test_result.outcome, test_result.failure_message\n</code></pre> <p>Once the specification is defined to be dependent on such parameters, they can simply be passed via CLI:</p> <pre><code>pytest specification.py --new_db=db_v1 --old_db=db_v2\n</code></pre>"},{"location":"testing/#html-reports","title":"Html reports","text":"<p>By default, running <code>pytest</code> tests will output test results to one's respective shell. Alternatively, one might want to generate an html report summarizing and expanding on all test results. This can be advantageous for</p> <ul> <li>Sharing test results with colleagues</li> <li>Archiving and tracking test results over time</li> <li>Make underlying sql queries conveniently accessible</li> </ul> <p>Concretely, such an html report can be generated by pytest-html. Once installed, using it is as simple as appending <code>--html=myreport.html</code> to the pytest call.</p> <p>In our twitch example, this generates this html report.</p>"},{"location":"testing/#retrieving-queries","title":"Retrieving queries","text":"<p>Usually we not only care about knowing whether there is a problem with the data at hand and what it is. Rather, we would also like to fix it as fast and conveniently as possible.</p> <p>For that matter, <code>datajudge</code> makes the queries it uses to assert testing predicates available via the :class:<code>datajudge.constraints.base.TestResult</code> class. Hence, if a test is failing, the user can jumpstart the investigation of the problem by reusing and potentially adapting the underlying queries.</p> <p>Instead of simply running <code>assert constraint.test(engine).outcome</code>, one may add the <code>TestResult</code>'s <code>logging_message</code> to e.g. a <code>logger</code> or add it to pytest <code>extra</code>:</p> <pre><code>from pytest_html import extras\n\ndef test_constraint(constraint: Constraint, engine, extra):\n    test_result = constraint.test(engine)\n    message = test_result.logging_message\n\n    if not test_result.outcome:\n      # Send to logger.\n      logger.info(message)\n      # Add to html report.\n      extra.append(\n        extras.extra(\n          content=message,\n          format_type=\"text\",\n          name=\"failing_query\",\n          mime_type=\"text/plain\",\n          extension=\"sql\",\n        )\n      )\n\n    assert test_result.outcome\n</code></pre> <p>Such a <code>logging_message</code> - with ready to execute sql queries - can look as follows:</p> <pre><code>/*\nFailure message:\ntempdb.public.twitch_v1's column(s) 'language' doesn't have the\nelement(s) '{'Sw3d1zh'}' when compared with the reference values.\n*/\n\n  --Factual queries:\n  SELECT anon_1.language, count(*) AS count_1\nFROM (SELECT public.twitch_v1.language AS language\nFROM public.twitch_v1) AS anon_1 GROUP BY anon_1.language\n\n-- Target queries:\n  SELECT anon_1.language, count(*) AS count_1\nFROM (SELECT public.twitch_v2.language AS language\nFROM public.twitch_v2) AS anon_1 GROUP BY anon_1.language\n</code></pre> <p>If using a mechanism - as previously outlined - to forward these messages to an html report, this can look as follows:</p> <p> </p>"},{"location":"examples/company-data/","title":"Company data","text":"<p>To get started, we will create a sample database using sqlite that contains a list of companies.</p> <p>The table \"companies_archive\" contains three entries:</p> <p>companies_archive</p> id name num_employees 1 QuantCo 90 2 Google 140,000 3 BMW 110,000 <p>While \"companies\" contains an additional entry:</p> <p>companies</p> id name num_employees 1 QuantCo 100 2 Google 150,000 3 BMW 120,000 4 Apple 145,000 <pre><code>import sqlalchemy as sa\n\neng = sa.create_engine('sqlite:///example.db')\n\nwith eng.connect() as con:\n    con.execute(\"CREATE TABLE companies (id INTEGER PRIMARY KEY, name TEXT, num_employees INTEGER)\")\n    con.execute(\"INSERT INTO companies (name, num_employees) VALUES ('QuantCo', 100), ('Google', 150000), ('BMW', 120000), ('Apple', 145000)\")\n    con.execute(\"CREATE TABLE companies_archive (id INTEGER PRIMARY KEY, name TEXT, num_employees INTEGER)\")\n    con.execute(\"INSERT INTO companies_archive (name, num_employees) VALUES ('QuantCo', 90), ('Google', 140000), ('BMW', 110000)\")\n</code></pre> <p>As an example, we will run 4 tests on this table:</p> <ol> <li>Does the table \"companies\" contain a column named \"name\"?</li> <li>Does the table \"companies\" contain at least 1 entry with the name \"QuantCo\"?</li> <li>Does the column \"num_employees\" of the \"companies\" table have all positive values?</li> <li>Does the column \"name\" of the table \"companies\" contain at least all the values of    the corresponding column in \"companies_archive\"?</li> </ol> <pre><code>import pytest\nimport sqlalchemy as sa\n\nfrom datajudge import (\n    Condition,\n    WithinRequirement,\n    BetweenRequirement,\n)\nfrom datajudge.pytest_integration import collect_data_tests\n\n\n# We create a Requirement, within a table. This object will contain\n# all the constraints we want to test on the specified table.\n# To test another table or test the same table against another table,\n# we would create another Requirement object.\ncompanies_req = WithinRequirement.from_table(\n    db_name=\"example\", schema_name=None, table_name=\"companies\"\n)\n\n# Constraint 1: Does the table \"companies\" contain a column named \"name\"?\ncompanies_req.add_column_existence_constraint(columns=[\"name\"])\n\n# Constraint 2: Does the table \"companies\" contain at least 1 entry with the name \"QuantCo\"?\ncondition = Condition(raw_string=\"name = 'QuantCo'\")\ncompanies_req.add_n_rows_min_constraint(n_rows_min=1, condition=condition)\n\n# Constraint 3: Does the column \"num_employees\" of the \"companies\" table have all\n# positive values?\ncompanies_req.add_numeric_min_constraint(column=\"num_employees\", min_value=1)\n\n# We create a new Requirement, this time between different tables.\n# Concretely, we intent to test constraints between the table \"companies\"\n# and the table \"companies_archive\".\ncompanies_between_req = BetweenRequirement.from_tables(\n    db_name1=\"example\",\n    schema_name1=None,\n    table_name1=\"companies\",\n    db_name2=\"example\",\n    schema_name2=None,\n    table_name2=\"companies_archive\",\n)\n\n# Constraint 4: Does the column \"name\" of the table \"companies\" contain at least all\n# the values of the corresponding column in \"companies_archive\"?\ncompanies_between_req.add_row_superset_constraint(\n    columns1=['name'], columns2=['name'], constant_max_missing_fraction=0\n)\n\n# collect_data_tests expects a pytest fixture with the name\n# \"datajudge_engine\" that is a SQLAlchemy engine\n\n@pytest.fixture()\ndef datajudge_engine():\n    return sa.create_engine(\"sqlite:///example.db\")\n\n# We gather our distinct Requirements in a list.\nrequirements = [companies_req, companies_between_req]\n\n# \"collect_data_tests\" takes all requirements and turns their respective\n# Constraints into individual tests. pytest will be able to pick\n# up these tests.\ntest_constraint = collect_data_tests(requirements)\n</code></pre> <p>Saving this file as <code>specification.py</code> and running <code>$ pytest specification.py</code> will verify that all constraints are satisfied. The output you see in the terminal should be similar to this:</p> <pre><code>=================================== test session starts ===================================\n...\ncollected 4 items\n\nspecification.py::test_constraint[ColumnExistence::companies] PASSED                [ 25%]\nspecification.py::test_constraint[NRowsMin::companies] PASSED                       [ 50%]\nspecification.py::test_constraint[NumericMin::companies] PASSED                     [ 75%]\nspecification.py::test_constraint[RowSuperset::companies|companies_archive] PASSED  [100%]\n\n==================================== 4 passed in 0.31s ====================================\n</code></pre> <p>You can also use a formatted html report using the <code>--html=report.html</code> flag.</p>"},{"location":"examples/dates/","title":"Dates","text":"<p>This example concerns itself with expressing <code>Constraint</code>s against data revolving around dates. While date <code>Constraint</code>s between tables exist, we will only illustrate <code>Constraint</code>s on a single table and reference values here. As a consequence, we will only use <code>WithinRequirement</code>, as opposed to <code>BetweenRequirement</code>.</p> <p>Concretely, we will assume a table containing prices for a given product of id 1. Importantly, these prices are valid for a certain date range only. More precisely, we assume that the price for a product - identified via the <code>preduct_id</code> column - is indicated in the <code>price</code> column, the date from which it is valid - the date itself included - in <code>date_from</code> and the the until when it is valid - the date itself included - in the <code>date_to</code> column.</p> <p>Such a table might look as follows:</p> <p>prices</p> product_id price date_from date_to 1 13.99 22/01/01 22/01/10 1 14.5 22/01/11 22/01/17 1 13.37 22/01/16 22/01/31 <p>Given this table, we would like to ensure - for the sake of illustrational purposes - that 6 constraints are satisfied:</p> <ol> <li>All values from column <code>date_from</code> should be in January 2022.</li> <li>All values from column <code>date_to</code> should be in January 2022.</li> <li>The minimum value in column <code>date_from</code> should be the first of January 2022.</li> <li>The maximum value in column <code>date_to</code> should be the 31st of January 2022.</li> <li>There is no gap between <code>date_from</code> and <code>date_to</code>. In other words, every date    of January has to be assigned to at least one row for a given product.</li> <li>There is no overlap between <code>date_from</code> and <code>date_to</code>. In other words, every    date of January has to be assigned to at most one row for a given product.</li> </ol> <p>Assuming that such a table exists in database, we can write a specification against it.</p> <pre><code>import pytest\nimport sqlalchemy as sa\n\nfrom datajudge import WithinRequirement\nfrom datajudge.pytest_integration import collect_data_tests\n\n# We create a Requirement, within a table. This object will contain\n# all the constraints we want to test on the specified table.\n# To test another table or test the same table against another table,\n# we would create another Requirement object.\nprices_req = WithinRequirement.from_table(\n    db_name=\"example\", schema_name=None, table_name=\"prices\"\n)\n\n# Constraint 1:\n# All values from column date_from should be in January 2022.\nprices_req.add_date_between_constraint(\n    column=\"date_from\",\n    lower_bound=\"'20220101'\",\n    upper_bound=\"'20220131'\",\n    # We don't tolerate any violations of the constraint:\n    min_fraction=1,\n)\n\n# Constraint 2:\n# All values from column date_to should be in January 2022.\nprices_req.add_date_between_constraint(\n    column=\"date_to\",\n    lower_bound=\"'20220101'\",\n    upper_bound=\"'20220131'\",\n    # We don't tolerate any violations of the constraint:\n    min_fraction=1,\n)\n\n# Constraint 3:\n# The minimum value in column date_from should be the first of January 2022.\n\n# Ensure that the minimum is smaller or equal the reference value min_value.\nprices_req.add_date_min_constraint(column=\"date_from\", min_value=\"'20220101'\")\n# Ensure that the minimum is greater or equal the reference value min_value.\nprices_req.add_date_min_constraint(\n    column=\"date_from\",\n    min_value=\"'20220101'\",\n    use_upper_bound_reference=True,\n)\n\n# Constraint 4:\n# The maximum value in column date_to should be the 31st of January 2022.\n\n# Ensure that the maximum is greater or equal the reference value max_value.\nprices_req.add_date_max_constraint(column=\"date_to\", max_value=\"'20220131'\")\n# Ensure that the maximum is smaller or equal the reference value max_value.\nprices_req.add_date_max_constraint(\n    column=\"date_to\",\n    max_value=\"'20220131'\",\n    use_upper_bound_reference=True,\n)\n\n# Constraint 5:\n# There is no gap between date_from and date_to. In other words, every date\n# of January has to be assigned to at least one row for a given product.\nprices_req.add_date_no_gap_constraint(\n    start_column=\"date_from\",\n    end_column=\"date_to\",\n    # We don't want a gap of price date ranges for a given product.\n    # For different products, we allow arbitrary date gaps.\n    key_columns=[\"product_id\"],\n    # As indicated in prose, date_from and date_to are included in ranges.\n    end_included=True,\n    # Again, we don't expect any violations of our constraint.\n    max_relative_violations=0,\n)\n\n# Constraint 6:\n# There is no overlap between date_from and date_to. In other words, every\n# of January has to be assigned to at most one row for a given product.\nprinces_req.add_date_no_overlap_constraint(\n    start_column=\"date_from\",\n    end_column=\"date_to\",\n    # We want no overlap of price date ranges for a given product.\n    # For different products, we allow arbitrary date overlaps.\n    key_columns=[\"product_id\"],\n    # As indicated in prose, date_from and date_to are included in ranges.\n    end_included=True,\n    # Again, we don't expect any violations of our constraint.\n    max_relative_violations=0,\n)\n\n@pytest.fixture()\ndef datajudge_engine():\n# TODO: Insert actual connection string\n    return sa.create_engine(\"your_db://\")\n\n# We gather our single Requirement in a list.\nrequirements = [prices_req]\n\n# \"collect_data_tests\" takes all requirements and turns their respective\n# Constraints into individual tests. pytest will be able to pick\n# up these tests.\ntest_constraint = collect_data_tests(requirements)\n</code></pre> <p>Please note that the <code>DateNoOverlap</code> and <code>DateNoGap</code> constraints also exist in a slightly different form: <code>DateNoOverlap2d</code> and <code>DateNoGap2d</code>. As the names suggest, these can operate in 'two date dimensions'.</p> <p>For example, let's assume a table with four date columns, representing two ranges in distinct dimensions, respectively:</p> <ul> <li><code>date_from</code>: Date from when a price is valid</li> <li><code>date_to</code>: Date until when a price is valid</li> <li><code>date_definition_from</code>: Date when a price definition was inserted</li> <li><code>date_definition_to</code>: Date until when a price definition was used</li> </ul> <p>Analogously to the unidimensional scenario illustrated here, one might care for certain constraints in two dimensions.</p>"},{"location":"examples/exploration/","title":"Exploration","text":"<p>While datajudge seeks to tackle the use case of expressing and evaluating tests against data, its fairly generic inner workings allow for using it in a rather explorative workflow as well.</p> <p>Let's first clarify terminology by exemplifying both scenarios. A person wishing to test data might ask the question</p> <p>Has the number of rows not grown too much from version 1 of the table to version 2 of the table?</p> <p>whereas a person wishing to explore the data might ask the question</p> <p>By how much has the number of rows grown from version 1 to version 2 of the table?</p> <p>Put differently, a test typically revolves around a binary outcome while an exploration usually doesn't.</p> <p>In the following we will attempt to illustrate possible usages of datajudge for exploration by looking at three simple examples.</p> <p>These examples rely on some insight about how most datajudge <code>Constraint</code> s work under the hood. Importantly, <code>Constraint</code> s typically come with</p> <ul> <li>a <code>retrieve</code> method: this method fetches relevant data from database, given a   <code>DataReference</code></li> <li>a <code>get_factual_value</code> method: this is typically a wrapper around <code>retrieve</code> for the   first <code>DataReference</code> of the given <code>Requirement</code> / <code>Constraint</code></li> <li>a <code>get_target_value</code> method: this is either a wrapper around <code>retrieve</code> for the   second <code>DataReference</code> in the case of a <code>BetweenRequirement</code> or an echoing of the   <code>Constraint</code> s key reference value in the case of a <code>WithinRequirement</code></li> </ul> <p>Moreover, as is the case when using datajudge for testing purposes, these approaches rely on a sqlalchemy engine. The latter is the gateway to the database at hand.</p>"},{"location":"examples/exploration/#example-1-comparing-numbers-of-rows","title":"Example 1: Comparing numbers of rows","text":"<p>Assume we have two tables in the same database called <code>table1</code> and <code>table2</code>. Now we would like to compare their numbers of rows. Naturally, we would like to retrieve the respective numbers of rows before we can compare them. For this purpose we create a <code>BetweenTableRequirement</code> referring to both tables and add a <code>NRowsEquality</code> <code>Constraint</code> onto it.</p> <pre><code>import sqlalchemy as sa\nfrom datajudge import BetweenRequirement\n\nengine = sa.create_engine(your_connection_string)\nreq = BetweenRequirement.from_tables(\n    db_name,\n    schema_name,\n    \"table1\",\n    db_name,\n    schema_name,\n    \"table2\",\n)\nreq.add_n_rows_equality_constraint()\nn_rows1 = req[0].get_factual_value(engine)\nn_rows2 = req[0].get_target_value(engine)\n</code></pre> <p>Note that here, we access the first (and only) <code>Constraint</code> that has been added to the <code>BetweenRequirement</code> by writing <code>req[0]</code>. <code>Requirements</code> are are sequences of <code>Constraint</code> s, after all.</p> <p>Once the numbers of rows are retrieved, we can compare them as we wish. For instance, we could compute the absolute and relative growth (or loss) of numbers of rows from <code>table1</code> to <code>table2</code>:</p> <pre><code>absolute_change = abs(n_rows2 - n_rows1)\nrelative_change = (absolute_change) / n_rows1 if n_rows1 != 0 else None\n</code></pre> <p>Importantly, many datajudge staples, such as <code>Condition</code> s can be used, too. We shall see this in our next example.</p>"},{"location":"examples/exploration/#example-2-investigating-unique-values","title":"Example 2: Investigating unique values","text":"<p>In this example we will suppose that there is a table called <code>table</code> consisting of several columns. Two of its columns are supposed to be called <code>col_int</code> and <code>col_varchar</code>. We are now interested in the unique values in these two columns combined. Put differently, we are wondering:</p> <p>Which unique pairs of values in <code>col_int</code> and <code>col_varchar</code> have we encountered?</p> <p>To add to the mix, we will moreover only be interested in tuples in which <code>col_int</code> has a value of larger than 10.</p> <p>As before, we will start off by creating a <code>Requirement</code>. Since we are only dealing with a single table this time, we will create a <code>WithinRequirement</code>.</p> <pre><code>import sqlalchemy as sa\nfrom datajudge import WithinRequirement, Condition\n\nengine = sa.create_engine(your_connection_string)\n\nreq = requirements.WithinRequirement.from_table(\n    db_name,\n    schema_name,\n    \"table\",\n)\n\ncondition = Condition(raw_string=\"col_int &gt;= 10\")\n\nreq.add_uniques_equality_constraint(\n    columns=[\"col_int\", \"col_varchar\"],\n    uniques=[], # This is really just a placeholder.\n    condition=condition,\n)\nuniques = req[0].get_factual_value(engine)\n</code></pre> <p>If one was to investigate this <code>uniques</code> variable further, one could, e.g. see the following:</p> <pre><code>([(10, 'hi10'), (11, 'hi11'), (12, 'hi12'), (13, 'hi13'), (14, 'hi14'), (15, 'hi15'), (16, 'hi16'), (17, 'hi17'), (18, 'hi18'), (19, 'hi19')], [1, 100, 12, 1, 7, 8, 1, 1, 1337, 1])\n</code></pre> <p>This becomes easier to parse when inspecting the underlying <code>retrieve</code> method of the <code>UniquesEquality</code> <code>Constraint</code>: the first value of the tuple corresponds to the list of unique pairs in columns <code>col_int</code> and <code>col_varchar</code>. The second value of the tuple are the respective counts thereof.</p> <p>Moreoever, one could manually customize the underlying SQL query. In order to do so, one can use the fact that <code>retrieve</code> methods typically return an actual result or value as well as the sqlalchemy selections that led to said result or value. We can use these selections and compile them to a standard, textual SQL query:</p> <pre><code>values, selections = req[0].retrieve(engine, constraint.ref)\nprint(str(selections[0].compile(engine, compile_kwargs={\"literal_binds\": True}))\n</code></pre> <p>In the case from above, this would return the following query:</p> <pre><code>SELECT\n    anon_1.col_int,\nanon_1.col_varchar,\ncount(*) AS count_1\nFROM\n    (SELECT\n    tempdb.dbo.table.col_int AS col_int,\n    tempdb.dbo.table.col_varchar AS col_varchar\n    FROM\n    tempdb.dbo.table WITH (NOLOCK)\n    WHERE col_int &gt;= 10) AS anon_1\nGROUP BY anon_1.col_int, anon_1.col_varchar\n</code></pre>"},{"location":"examples/exploration/#example-3-comparing-column-structure","title":"Example 3: Comparing column structure","text":"<p>While we often care about value tuples of given columns, i.e. rows, it can also provide meaningful insights to compare the column structure of two tables. In particular, we might want to compare whether columns of one table are a subset or superset of another table. Moreover, for columns present in both tables, we'd like to learn about their respective types.</p> <p>In order to illustrate such an example, we will again assume that there are two tables called <code>table1</code> and <code>table2</code>, irrespective of prior examples.</p> <p>We can now create a <code>BetweenRequirement</code> for these two tables and use the <code>ColumnSubset</code> <code>Constraint</code>. As before, we will rely on the <code>get_factual_value</code> method to retrieve the values of interest for the first table passed to the <code>BetweenRequirement</code> and the <code>get_target_value</code> method for the second table passed to the <code>BetweenRequirement</code>.</p> <pre><code>import sqlalchemy as sa\nfrom datajudge import BetweenRequirement\n\nengine = sa.create_engine(your_connection_string)\n\nreq = BetweenRequirement.from_tables(\n    db_name,\n    schema_name,\n    \"table1\",\n    db_name,\n    schema_name,\n    \"table2\",\n)\n\nreq.add_column_subset_constraint()\n\ncolumns1 = req[0].get_factual_value(engine)\ncolumns2 = req[0].get_target_value(engine)\n\nprint(f\"Columns present in both: {set(columns1) &amp; set(columns2)}\")\nprint(f\"Columns present in only table1: {set(columns1) - set(columns2)}\")\nprint(f\"Columns present in only table2: {set(columns2) - set(columns1)}\")\n</code></pre> <p>This could, for instance result in the following printout:</p> <pre><code>Columns present in both: {'col_varchar', 'col_int'}\nColumns present in only table1: set()\nColumns present in only table2: {'col_date'}\n</code></pre> <p>Now, we can investigate the types of the columns present in both tables:</p> <pre><code>for column in set(columns1) &amp; set(columns2):\n    req.add_column_type_constraint(column1=column, column2=column)\ntype1 = req[0].get_factual_value(engine)\ntype2 = req[0].get_target_value(engine)\nprint(f\"Column '{column}' has type '{type1}' in table1 and type '{type2}' in table2.\")\n</code></pre> <p>Depending on the underlying database management system and data, the output of this could for instance be:</p> <pre><code>Column 'col_varchar' has type 'varchar' in table1 and type 'varchar' in table2.\nColumn 'col_int' has type 'integer' in table1 and type 'integer' in table2.\n</code></pre>"},{"location":"examples/twitch/","title":"Dumps of Twitch data","text":"<p>This example is based on data capturing statistics and properties of popular Twitch channels. The setup is such that we have two data sets 'of the same kind' but from different points in time.</p> <p>In other words, a 'version' of the data set represents a temporal notion. For example, version 1 might stem from end of March and version 2 from end of April. Moreover, we will assume that the first, version 1, has been vetted and approved with the help of manual investigation and domain knowledge. The second data set, version 2, has just been made available. We would like to use it but can't be sure of its validity just yet. As a consequence we would like to assess the quality of the data in version 2.</p> <p>In order to have a database Postgres instance to begin with, it might be useful to use our script, spinning up a dockerized Postgres database:</p> <pre><code>./start_postgres.sh\n</code></pre> <p>The original data set can be found on kaggle. For the sake of this tutorial, we slightly process it and provide two versions of it. One can either recreate this by executing this processing script oneself on the original data or download our processed files ( version 1 and version 2) right away.</p> <p>Once both version of the data exist, they can be uploaded to the tabase. We provide an uploading script creating and populating one table per version of the data in a Postgres database. It resembles the following:</p> <pre><code>address = os.environ.get(\"DB_ADDR\", \"localhost\")\nconnection_string = f\"postgresql://datajudge:datajudge@{address}:5432/datajudge\"\nengine = sa.create_engine(connection_string)\ndf_v2.to_sql(\"twitch_v2\", engine, schema=\"public\", if_exists=\"replace\")\ndf_v1.to_sql(\"twitch_v1\", engine, schema=\"public\", if_exists=\"replace\")\n</code></pre> <p>Once the tables are stored in a database, we can actually write a <code>datajudge</code> specification against them. But first, we'll have a look at what the data roughly looks like by investigating a random sample of four rows:</p> <p>A sample of the data</p> channel watch_time stream_time peak_viewers average_viewers followers followers_gained views_gained partnered mature language xQcOW 6196161750 215250 222720 27716 3246298 1734810 93036735 True False English summit1g 6091677300 211845 310998 25610 5310163 1374810 89705964 True False English Gaules 5644590915 515280 387315 10976 1767635 1023779 102611607 True True Portuguese ESL_CSGO 3970318140 517740 300575 7714 3944850 703986 106546942 True False English <p>Note that we expect both version 1 and version 2 to follow this structure. Due to them being assembled at different points in time, merely their rows shows differ.</p> <p>Now let's write an actual specification, expressing our expectations against the data. First, we need to make sure a connection to the database can be established at test execution time. How this is done exactly depends on how you set up your database. When using our default setup with running, this would look as follows:</p> <pre><code>import os\nimport pytest\nimport sqlalchemy as sa\n\n\n@pytest.fixture(scope=\"module\")\ndef datajudge_engine():\n    address = os.environ.get(\"DB_ADDR\", \"localhost\")\n    connection_string = f\"postgresql://datajudge:datajudge@{address}:5432/datajudge\"\n    return sa.create_engine(connection_string)\n</code></pre> <p>Once a way to connect to the database is defined, we want to declare our data sources and express expectations against them. In this example, we have two tables in the same database - one table per version of the Twitch data.</p> <p>Yet, let's start with a straightforward example only using version 2. We want to use our domain knowledge that constrains the values of the <code>language</code> column only to contain letters and have a length strictly larger than 0.</p> <pre><code>from datajudge import WithinRequirement\n\n\n# Postgres' default database.\ndb_name = \"tempdb\"\n# Postgres' default schema.\nschema_name = \"public\"\n\nwithin_requirement = WithinRequirement.from_table(\n    table_name=\"twitch_v2\",\n    schema_name=schema_name,\ndb_name=db_name,\n)\nwithin_requirement.add_varchar_regex_constraint(\ncolumn=\"language\",\nregex=\"^[a-zA-Z]+$\",\n)\n</code></pre> <p>Done! Now onto comparisons between the table representing the approved version 1 of the data and the to be assessed version 2 of the data.</p> <pre><code>from datajudge import BetweenRequirement, Condition\n\nbetween_requirement_version = BetweenRequirement.from_tables(\n    db_name1=db_name,\n    db_name2=db_name,\n    schema_name1=schema_name,\n    schema_name2=schema_name,\n    table_name1=\"twitch_v1\",\n    table_name2=\"twitch_v2\",\n)\nbetween_requirement_version.add_column_subset_constraint()\nbetween_requirement_version.add_column_superset_constraint()\ncolumns = [\"channel\", \"partnered\", \"mature\"]\nbetween_requirement_version.add_row_subset_constraint(\ncolumns 1=columns, columns2=columns, constant_max_missing_fraction=0\n)\nbetween_requirement_version.add_row_matching_equality_constraint(\n    matching_columns1=[\"channel\"],\n    matching_columns2=[\"channel\"],\n    comparison_columns1=[\"language\"],\n    comparison_columns2=[\"language\"],\n    max_missing_fraction=0,\n)\n\nbetween_requirement_version.add_ks_2sample_constraint(\n    column1=\"average_viewers\",\n    column2=\"average_viewers\",\n    significance_level=0.05,\n)\nbetween_requirement_version.add_uniques_equality_constraint(\n    columns1=[\"language\"],\n    columns2=[\"language\"],\n)\n</code></pre> <p>Now having compared the 'same kind of data' between version 1 and version 2, we may as well compare 'different kind of data' within version 2, as a means of a sanity check. This sanity check consists of checking whether the mean <code>average_viewer</code> value of mature channels should deviate at most 10% from the overall mean.</p> <pre><code>between_requirement_columns = BetweenRequirement.from_tables(\n    db_name1=db_name,\n    db_name2=db_name,\n    schema_name1=schema_name,\n    schema_name2=schema_name,\n    table_name1=\"twitch_v2\",\n    table_name2=\"twitch_v2\",\n)\n\nbetween_requirement_columns.add_numeric_mean_constraint(\n    column1=\"average_viewers\",\n    column2=\"average_viewers\",\n    condition1=None,\n    condition2=Condition(raw_string=\"mature IS TRUE\"),\n    max_absolute_deviation=0.1,\n)\n</code></pre> <p>Lastly, we need to collect all of our requirements in a list and make sure <code>pytest</code> can find them by calling <code>collect_data_tests</code>.</p> <pre><code>from datajudge.pytest_integration import collect_data_tests\nrequirements = [\n    within_requirement,\n    between_requirement_version,\n    between_requirement_columns,\n]\ntest_func = collect_data_tests(requirements)\n</code></pre> <p>If we then test these expectations against the data by running</p> <pre><code>pytest specification.py`` -- where ``specification.py\n</code></pre> <p>contains all of the code outlined before (you can find it here) -- we see that the new version of the data is not quite on par with what we'd expect:</p> <pre><code>$ pytest twitch_specification.py\n================================== test session starts ===================================\nplatform darwin -- Python 3.10.5, pytest-7.1.2, pluggy-1.0.0\nrootdir: /Users/kevin/Code/datajudge/docs/source/examples\nplugins: html-3.1.1, cov-3.0.0, metadata-2.0.2\ncollected 8 items\n\ntwitch_specification.py F.....FF                                                   [100%]\n\n======================================== FAILURES ========================================\n____________________ test_func[VarCharRegex::tempdb.public.twitch_v2] ____________________\n\nconstraint = &lt;datajudge.constraints.varchar.VarCharRegex object at 0x10855da20&gt;\ndatajudge_engine = Engine(postgresql://datajudge:***@localhost:5432/datajudge)\n\n@pytest.mark.parametrize(\n    \"constraint\", all_constraints, ids=Constraint.get_description\n)\ndef test_constraint(constraint, datajudge_engine):\n    test_result = constraint.test(datajudge_engine)\n&gt;       assert test_result.outcome, test_result.failure_message\nE       AssertionError: tempdb.public.twitch_v2's column(s) 'language' breaks regex\n        '^[a-zA-Z]+$' in 0.045454545454545456 &gt; 0.0 of the cases. In absolute terms, 1\n    of the 22 samples violated the regex. Some counterexamples consist of the\n    following: ['Sw3d1zh'].\n\n../../../src/datajudge/pytest_integration.py:25: AssertionError\n____________ test_func[UniquesEquality::public.twitch_v1 | public.twitch_v2] _____________\n\nconstraint = &lt;datajudge.constraints.uniques.UniquesEquality object at 0x10855d270&gt;\ndatajudge_engine = Engine(postgresql://datajudge:***@localhost:5432/datajudge)\n\n@pytest.mark.parametrize(\n    \"constraint\", all_constraints, ids=Constraint.get_description\n)\ndef test_constraint(constraint, datajudge_engine):\n    test_result = constraint.test(datajudge_engine)\n&gt;       assert test_result.outcome, test_result.failure_message\nE       AssertionError: tempdb.public.twitch_v1's column(s) 'language' doesn't have\n        the element(s) '{'Sw3d1zh'}' when compared with the reference values.\n\n../../../src/datajudge/pytest_integration.py:25: AssertionError\n______________ test_func[NumericMean::public.twitch_v2 | public.twitch_v2] _______________\n\nconstraint = &lt;datajudge.constraints.numeric.NumericMean object at 0x1084e1810&gt;\ndatajudge_engine = Engine(postgresql://datajudge:***@localhost:5432/datajudge)\n\n@pytest.mark.parametrize(\n    \"constraint\", all_constraints, ids=Constraint.get_description\n)\ndef test_constraint(constraint, datajudge_engine):\n    test_result = constraint.test(datajudge_engine)\n&gt;       assert test_result.outcome, test_result.failure_message\nE       AssertionError: tempdb.public.twitch_v2's column(s) 'average_viewers' has\n        mean 4734.9780000000000000, deviating more than 0.1 from\n    tempdb.public.twitch_v2's column(s) 'average_viewers''s\n    3599.9826086956521739. Condition on second table: WHERE mature IS TRUE\n\n../../../src/datajudge/pytest_integration.py:25: AssertionError\n================================ short test summary info =================================\nFAILED twitch_specification.py::test_func[VarCharRegex::tempdb.public.twitch_v2] - Asse...\nFAILED twitch_specification.py::test_func[UniquesEquality::public.twitch_v1 | public.twitch_v2]\nFAILED twitch_specification.py::test_func[NumericMean::public.twitch_v2 | public.twitch_v2]\n============================== 3 failed, 5 passed in 1.52s ===============================\n</code></pre> <p>Alternatively, you can also look at these test results in this html report generated by pytest-html.</p> <p>Hence we see that we might not want to blindly trust version 2 of the data as is. Rather, we might need to investigate what is wrong with the data, what this has been caused by and how to fix it.</p> <p>Concretely, what exactly do we learn from the error messages?</p> <ul> <li>The column <code>language</code> now has a row with value <code>'Sw3d1zh'</code>. This break two of our   constraints. The <code>VarCharRegex</code> constraint compared the columns' values to a regular   expression. The <code>UniquesEquality</code> constraint expected the unique values of the   <code>language</code> column to not have changed between version 1 and version 2.</li> <li>The mean value of <code>average_viewers</code> of <code>mature</code> channels is substantially - more   than our 10% tolerance - lower than the global mean.</li> </ul>"}]}